{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "from scipy import stats, signal\n",
    "import scipy.io as sio\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import KFold, LeaveOneOut\n",
    "from sklearn import preprocessing, model_selection, metrics\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_vu_all_files(filedirectory, numfiles, delaybool):\n",
    "# Get data from .mat files\n",
    "# Assumes input data are in the format of time (rows) x channels (columns)\n",
    "    if len(numfiles) == 0:\n",
    "        numfiles_start = 0\n",
    "        numfiles_end = len(os.listdir(filedirectory)) # Use all files in directory\n",
    "        print('Loading {} processed data file(s) ...'.format(numfiles_end - numfiles_start + 1))     \n",
    "    else:\n",
    "        numfiles_start = numfiles[0]\n",
    "        numfiles_end = numfiles[1]\n",
    "        print('Loading {} processed data file(s) ...'.format(numfiles_end - numfiles_start + 1))      \n",
    "\n",
    "    filedict = {}\n",
    "    allindtrigsdict = {}\n",
    "    \n",
    "    for findex, filename in enumerate(sorted(os.listdir(filedirectory))):\n",
    "        if numfiles_start <= fileindex < numfiles_end:\n",
    "            print('=== {} ==='.format(filename))\n",
    "            mat_contents = sio.loadmat(filedirectory + '\\\\' + filename)\n",
    "            daqdata = mat_contents['python_input']\n",
    "            \n",
    "            if fileindex == numfiles_start:\n",
    "                chanlabels = mat_contents['chanlabels'].flatten()\n",
    "                allcolheaders = []\n",
    "                for i in np.arange(len(chanlabels)):\n",
    "                    allcolheaders.append(chanlabels[i][0])\n",
    "                        \n",
    "            # Get post-processed channel data\n",
    "            chans = daqdata[:,:31]\n",
    "            locomode = daqdata[:,31]\n",
    "            \n",
    "            if delaybool == True: # 3 frame (~90 ms) delay\n",
    "                use_trig = daqdata[:,33]\n",
    "                # Define valid triggers (5-digit)\n",
    "                hc_sm_trigs = [24210, 28210, 54510, 64610, 34310, 44410, 24510, 54210, 24610, 64210, 24410, 44210]\n",
    "                mst_sm_trigs = [21220, 51520, 61620, 31110, 41420, 41260]\n",
    "                to_sm_trigs = [22230, 26230, 52530, 62630, 41430, 42430, 22530, 52230, 22630, 62230, 22330, 21330, 41230, 42230]\n",
    "                msw_sm_trigs = [23240, 53540, 63640, 33390, 43440, 39340, 33240, 39280]\n",
    "                shc_sm_trigs = [13110, 13410]\n",
    "                sto_sm_trigs = [11130, 11330]\n",
    "            else: # No delay\n",
    "                use_trig = daqdata[:,32]\n",
    "                # Define valid triggers (4-digit)\n",
    "                hc_sm_trigs = [2421, 2821, 5451, 6461, 3431, 4441, 2451, 5421, 2461, 6421, 2441, 4421]\n",
    "                mst_sm_trigs = [2122, 5152, 6162, 3111, 4142, 4126]\n",
    "                to_sm_trigs = [2223, 2623, 5253, 6263, 4143, 4243, 2253, 5223, 2263, 6223, 2233, 2133, 4123, 4223]\n",
    "                msw_sm_trigs = [2324, 5354, 6364, 3339, 4344, 3934, 3324, 3928]\n",
    "                shc_sm_trigs = [1311, 1341]\n",
    "                sto_sm_trigs = [1113, 1133]\n",
    "\n",
    "            trig_diff = np.diff(use_trig)\n",
    "            trig_ind = np.where(trig_diff > 0)[0] + 1\n",
    "            \n",
    "            hc_ind, mst_ind, to_ind, msw_ind, shc_ind, sto_ind = [], [], [], [], [], []\n",
    "            hc_trig, mst_trig, to_trig, msw_trig, shc_trig, sto_trig = [], [], [], [], [], []\n",
    "            \n",
    "            for i in np.arange(len(trig_ind)):\n",
    "                if use_trig[trig_ind[i]] in hc_sm_trigs:\n",
    "                    hc_ind.append(int(trig_ind[i]))\n",
    "                    hc_trig.append(int(use_trig[trig_ind[i]]))\n",
    "                elif use_trig[trig_ind[i]] in mst_sm_trigs:\n",
    "                    mst_ind.append(int(trig_ind[i]))\n",
    "                    mst_trig.append(int(use_trig[trig_ind[i]]))\n",
    "                elif use_trig[trig_ind[i]] in to_sm_trigs:\n",
    "                    to_ind.append(int(trig_ind[i]))\n",
    "                    to_trig.append(int(use_trig[trig_ind[i]]))\n",
    "                elif use_trig[trig_ind[i]] in msw_sm_trigs:\n",
    "                    msw_ind.append(int(trig_ind[i]))\n",
    "                    msw_trig.append(int(use_trig[trig_ind[i]]))\n",
    "                elif use_trig[trig_ind[i]] in shc_sm_trigs:\n",
    "                    shc_ind.append(int(trig_ind[i]))\n",
    "                    shc_trig.append(int(use_trig[trig_ind[i]]))\n",
    "                elif use_trig[trig_ind[i]] in sto_sm_trigs:\n",
    "                    sto_ind.append(int(trig_ind[i]))\n",
    "                    sto_trig.append(int(use_trig[trig_ind[i]]))\n",
    "            \n",
    "            hc_ind, hc_trig = np.array(hc_ind), np.array(hc_trig)\n",
    "            mst_ind, mst_trig = np.array(mst_ind), np.array(mst_trig)\n",
    "            to_ind, to_trig = np.array(to_ind), np.array(to_trig)\n",
    "            msw_ind, msw_trig = np.array(msw_ind), np.array(msw_trig)\n",
    "            shc_ind, shc_trig = np.array(shc_ind), np.array(shc_trig)\n",
    "            sto_ind, sto_trig = np.array(sto_ind), np.array(sto_trig)\n",
    "            \n",
    "            # Save the post-processed data and indices/triggers into dictionaries\n",
    "            filedict[filename] = [chans,locomode,daqdata[:,30],daqdata[:,31]]\n",
    "            allindtrigsdict[filename] = {'HC': np.vstack((hc_ind,hc_trig)).T, \n",
    "                                         'MST': np.vstack((mst_ind,mst_trig)).T, \n",
    "                                         'TO': np.vstack((to_ind,to_trig)).T, \n",
    "                                         'MSW': np.vstack((msw_ind,msw_trig)).T,\n",
    "                                         'SHC': np.vstack((shc_ind,shc_trig)).T,\n",
    "                                         'STO': np.vstack((sto_ind,sto_trig)).T}\n",
    "            \n",
    "    # filedict has the filename as the key and the 2D DAQ data and targets as entries\n",
    "    # allcolheaders has the channel names\n",
    "    \n",
    "    print('Finished!')\n",
    "    \n",
    "    return filedict, allindtrigsdict, allcolheaders\n",
    "\n",
    "\n",
    "def unpack_files(filedict, allindtrigsdict, filekeys, arginput, allcolheaders):    \n",
    "    # Unpack the list of arguments (arginput)\n",
    "    # Windowing parameters\n",
    "    TRAIN_SIZE = arginput[0] # Sliding window length\n",
    "    PREST_TRAIN_SIZE = arginput[1] # Window length to extract before HC\n",
    "    PREMST_TRAIN_SIZE = arginput[2] # Window length to extract before MST\n",
    "    PRESW_TRAIN_SIZE = arginput[3] # Window length to extract before TO\n",
    "    PREMSW_TRAIN_SIZE = arginput[4] # Window length to extract before MSW\n",
    "    PRESHC_TRAIN_SIZE = arginput[5] # Window length to extract before SHC\n",
    "    PRESTO_TRAIN_SIZE = arginput[6] # Window length to extract before STO\n",
    "    PRED_SIZE = arginput[7] # Length of window (after the training window) to get the ground truth label\n",
    "    STEP_SIZE = arginput[8] # Sliding window increment\n",
    "    DS_FACTOR = arginput[9] # Down-sampling factor\n",
    "    \n",
    "    # Jitter parameters\n",
    "    PRE_AUG = arginput[10] # Number of ms before the gait event to end training window\n",
    "    POST_AUG = arginput[11] # Number of ms after the gait event to end training window\n",
    "    NUM_AUG = arginput[12] # Number of augmented windows to extract (symmetric about the gait event)\n",
    "    \n",
    "    # Train/test split parameters\n",
    "    FOR_TEST = arginput[13]\n",
    "    TOTAL_FILES = len(filekeys)\n",
    "    if len(FOR_TEST) > 1 or type(FOR_TEST[0]) is str:\n",
    "        print('Generating train/test data from {} specified file(s):'.format(len(FOR_TEST)))\n",
    "        if NUM_AUG > 0:\n",
    "            print('Data augmentation x{} ...'.format(NUM_AUG))\n",
    "        else:\n",
    "            print('No data augmentation...')\n",
    "        TEST_FILE_INDS = []\n",
    "        for testfileind, testfile in enumerate(FOR_TEST):\n",
    "            TEST_FILE_INDS.append(filekeys.index(testfile))\n",
    "        TEST_FILE_INDS = np.array(TEST_FILE_INDS)\n",
    "        TRAIN_FILE_INDS = np.setdiff1d(np.arange(TOTAL_FILES),TEST_FILE_INDS)\n",
    "    else:\n",
    "        print('Randomly select train/test data based on testing data proportion ({}%):'.format(int(100*FOR_TEST[0])))\n",
    "        if NUM_AUG > 0:\n",
    "            print('Data augmentation x{} ...'.format(NUM_AUG))\n",
    "        else:\n",
    "            print('No data augmentation...')\n",
    "        TOTAL_FOLDS = int(1/(FOR_TEST[0]))\n",
    "        SPLIT_FILES = np.array_split(np.arange(TOTAL_FILES),TOTAL_FOLDS)\n",
    "        TEST_FILE_INDS = SPLIT_FILES[np.random.randint(0,TOTAL_FOLDS-1)]\n",
    "        TRAIN_FILE_INDS = np.setdiff1d(np.arange(TOTAL_FILES),TEST_FILE_INDS)\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    # Channels to use\n",
    "    CHAN_EMG = arginput[14]\n",
    "    \n",
    "    # Printing parameters\n",
    "    PRINT_OUT = arginput[15]\n",
    "    PRINT_SUMMARY = arginput[16]\n",
    "    \n",
    "    alldict = {}\n",
    "    alldict['Combined'] = [[],[],[]]\n",
    "    alldict['Combined File Index'] = []\n",
    "        \n",
    "    for gaitevent in ['HC', 'MST', 'TO', 'MSW', 'SHC', 'STO']:\n",
    "        alldict['Combined ' + gaitevent + ' Windows'] = []\n",
    "        alldict['Combined ' + gaitevent + ' Features'] = []\n",
    "        alldict['Combined ' + gaitevent + ' Triggers'] = [] \n",
    "        alldict['Combined ' + gaitevent + ' Augmented'] = []\n",
    "        alldict['Combined ' + gaitevent + ' File Index'] = []\n",
    "        \n",
    "    for fkeyind, fkey in enumerate(filekeys):\n",
    "        print('Preparing file {}: {} ...'.format(fkeyind+1,fkey))\n",
    "\n",
    "        data = filedict[fkey][0]\n",
    "        locomode = filedict[fkey][1]\n",
    "        indtrigs = allindtrigsdict[fkey]\n",
    "\n",
    "        data, X, X_feats, Y, featnames = slide_windows(data, DS_FACTOR, locomode, TRAIN_SIZE, PRED_SIZE, STEP_SIZE, CHAN_MECH, allcolheaders)\n",
    "        alleventdict = event_windows(indtrigs, data, DS_FACTOR, PREST_TRAIN_SIZE, PREMST_TRAIN_SIZE, PRESW_TRAIN_SIZE, PREMSW_TRAIN_SIZE, PRESHC_TRAIN_SIZE, PRESTO_TRAIN_SIZE, PRE_AUG, POST_AUG, NUM_AUG, CHAN_MECH, allcolheaders)\n",
    "        \n",
    "        if PRINT_OUT:\n",
    "            print('The original data dimensions are: {}'.format(data.shape))\n",
    "            print('DNN input dimensions: {}.'.format(X.shape))\n",
    "            print('FEAT input dimensions: {}.'.format(X_feats_smooth.shape))\n",
    "            print('Output dimensions: {}.'.format(Y.shape))\n",
    "            print() \n",
    "\n",
    "        alleventfeats = {'HC': alleventdict['HC'][1], \n",
    "                         'MST': alleventdict['MST'][1], \n",
    "                         'TO': alleventdict['TO'][1], \n",
    "                         'MSW': alleventdict['MSW'][1],\n",
    "                         'SHC': alleventdict['SHC'][1],\n",
    "                         'STO': alleventdict['STO'][1]}\n",
    "\n",
    "        # Save the sliding windows, their extracted features, ground truth labels, and file indices\n",
    "        alldict['Combined'][0].append(X)\n",
    "        alldict['Combined'][1].append(X_feats)\n",
    "        alldict['Combined'][2].append(Y)\n",
    "        alldict['Combined File Index'].append(np.tile(fkeyind,len(Y)))\n",
    "\n",
    "        for gaitevent in ['HC', 'MST', 'TO', 'MSW', 'SHC', 'STO']:\n",
    "            # Save the windows extracted before each gait event\n",
    "            alldict['Combined ' + gaitevent + ' Windows'].append(alleventdict[gaitevent][0])\n",
    "            # Save the features extracted from each gait event\n",
    "            alldict['Combined ' + gaitevent + ' Features'].append(alleventfeats[gaitevent])\n",
    "            # Save the trigger associated with each gait event\n",
    "            alldict['Combined ' + gaitevent + ' Triggers'].append(alleventdict[gaitevent][2])\n",
    "            # Save file index associated with each gait event\n",
    "            alldict['Combined ' + gaitevent + ' File Index'].append(np.tile(fkeyind,len(alleventdict[gaitevent][2])))\n",
    "            # Save binary of whether each event is original or augmented\n",
    "            alldict['Combined ' + gaitevent + ' Augmented'].append(alleventdict[gaitevent][3])\n",
    "    print()\n",
    "    \n",
    "    # Convert from list to array\n",
    "    alldict['Combined'][0] = np.concatenate(alldict['Combined'][0])\n",
    "    alldict['Combined'][1] = np.concatenate(alldict['Combined'][1])\n",
    "    alldict['Combined'][2] = np.concatenate(alldict['Combined'][2])\n",
    "    alldict['Combined File Index'] = np.concatenate(alldict['Combined File Index'])\n",
    "    \n",
    "    for gaitevent in ['HC', 'MST', 'TO', 'MSW', 'SHC', 'STO']:\n",
    "        # Remove empty lists\n",
    "        alldict['Combined ' + gaitevent + ' Windows'] = [windows for windows in alldict['Combined ' + gaitevent + ' Windows'] if len(windows) > 0]\n",
    "        alldict['Combined ' + gaitevent + ' Features'] = [features for features in alldict['Combined ' + gaitevent + ' Features'] if len(features) > 0]\n",
    "        alldict['Combined ' + gaitevent + ' Triggers'] = [triggers for triggers in alldict['Combined ' + gaitevent + ' Triggers'] if len(triggers) > 0]\n",
    "        alldict['Combined ' + gaitevent + ' File Index'] = [fileinds for fileinds in alldict['Combined ' + gaitevent + ' File Index'] if len(fileinds) > 0]\n",
    "        alldict['Combined ' + gaitevent + ' Augmented'] = [augmented for augmented in alldict['Combined ' + gaitevent + ' Augmented'] if len(augmented) > 0]\n",
    "        \n",
    "        alldict['Combined ' + gaitevent + ' Windows'] = np.concatenate(alldict['Combined ' + gaitevent + ' Windows'])\n",
    "        alldict['Combined ' + gaitevent + ' Features'] = np.concatenate(alldict['Combined ' + gaitevent + ' Features'])\n",
    "        alldict['Combined ' + gaitevent + ' Triggers'] = np.concatenate(alldict['Combined ' + gaitevent + ' Triggers'])\n",
    "        alldict['Combined ' + gaitevent + ' File Index'] = np.concatenate(alldict['Combined ' + gaitevent + ' File Index'])\n",
    "        alldict['Combined ' + gaitevent + ' Augmented'] = np.concatenate(alldict['Combined ' + gaitevent + ' Augmented'])\n",
    "        \n",
    "    if PRINT_SUMMARY:\n",
    "        print('Aggregated input dimensions: {}'.format(alldict['Combined'][0].shape))\n",
    "        print('Aggregated feature dimensions: {}'.format(alldict['Combined'][1].shape))\n",
    "        print('Aggregated target dimensions: {}'.format(alldict['Combined'][2].shape))\n",
    "        print()\n",
    "        print('Aggregated HC feature dimensions: {}'.format(alldict['Combined HC Features'].shape))\n",
    "        print('Aggregated MST feature dimensions: {}'.format(alldict['Combined MST Features'].shape))\n",
    "        print('Aggregated TO feature dimensions: {}'.format(alldict['Combined TO Features'].shape))\n",
    "        print('Aggregated MSW feature dimensions: {}'.format(alldict['Combined MSW Features'].shape))\n",
    "        print('Aggregated SHC feature dimensions: {}'.format(alldict['Combined SHC Features'].shape))\n",
    "        print('Aggregated STO feature dimensions: {}'.format(alldict['Combined STO Features'].shape))\n",
    "        print()\n",
    "        \n",
    "    return alldict, featnames, TRAIN_FILE_INDS, TEST_FILE_INDS\n",
    "\n",
    "\n",
    "def slide_windows(data, DS_FACTOR, target, train, predict, step, chanmech, allcolheaders):\n",
    "# Get data from sliding windows from the beginning to end of the file \n",
    "# train = size of training window\n",
    "# predict = size of forecasting window\n",
    "# step = window increment\n",
    "# scale = normalize data (boolean)\n",
    "\n",
    "    mechheaders = [allcolheaders[i] for i in chanmech]\n",
    "        \n",
    "    X, Y = [], []\n",
    "    allfeats = []\n",
    "    \n",
    "    # Form sliding windows from start to end of the data file\n",
    "    for i in range(0, len(data), step):\n",
    "        if i > train and i < (len(data) - predict):\n",
    "            x_i = data[i-train:i]\n",
    "            y_i = stats.mode(target[i:i+predict])[0]\n",
    "    \n",
    "            feats, featsnames = feats_from_window(x_i,chanmech,mechheaders)\n",
    "        \n",
    "            allfeats.append(feats)\n",
    "    \n",
    "            X.append(x_i[::DS_FACTOR])\n",
    "            Y.append(y_i)\n",
    "\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "    \n",
    "    return data, X, allfeats, Y, featsnames\n",
    "\n",
    "\n",
    "def event_windows(indtrigs, data, DS_FACTOR, pre_stance, pre_midstance, pre_swing, pre_midswing, pre_shc, pre_sto, aug_pre, aug_post, aug_windows_per_event, chanmech, allcolheaders):\n",
    "# Get data from windows near gait events specified by indtrigs\n",
    "# Try different sized windows (pre_stance before stance and pre_swing before swing)\n",
    "# Try data augmentation (get aug_windows_per_event extra windows beginning aug_pre before to aug_post after the gait event)\n",
    "# scale = normalize data (boolean)\n",
    "\n",
    "    mechheaders = [allcolheaders[i] for i in chanmech]\n",
    "\n",
    "    eventkeys = list(indtrigs.keys())\n",
    "    \n",
    "    # Specify data augmentation\n",
    "    if aug_windows_per_event > 0:\n",
    "        aug_start = np.linspace(-aug_pre,aug_post,aug_windows_per_event+1).astype(int)\n",
    "        aug_start = aug_start[np.absolute(aug_start) > 0]\n",
    "        to_aug = True\n",
    "    else:\n",
    "        to_aug = False\n",
    "        \n",
    "    alleventdict = {}\n",
    "        \n",
    "    for eventkeyind, eventkey in enumerate(eventkeys):\n",
    "        eventwin_list, allfeats, trig_list, aug_bin = [], [], [], []\n",
    "        if eventkey == 'HC':\n",
    "            train_window = pre_stance\n",
    "        elif eventkey == 'MST':\n",
    "            train_window = pre_midstance\n",
    "        elif eventkey == 'TO':\n",
    "            train_window = pre_swing\n",
    "        elif eventkey == 'MSW':\n",
    "            train_window = pre_midswing\n",
    "        elif eventkey == 'SHC':\n",
    "            train_window = pre_shc\n",
    "        elif eventkey == 'STO':\n",
    "            train_window = pre_sto\n",
    "        \n",
    "        windows = 0\n",
    "        \n",
    "        inds = indtrigs[eventkey][:,0]\n",
    "        trigs = indtrigs[eventkey][:,1]\n",
    "        \n",
    "        # Remove triggers occurring less than 300 ms into DAQ file\n",
    "        keepinds = [inds > train_window]\n",
    "        inds = inds[keepinds]\n",
    "        trigs = trigs[keepinds] \n",
    "        \n",
    "        if len(inds) > 0:\n",
    "            for i in range(inds.shape[0]): # Iterates over indices\n",
    "                windows += (1+aug_windows_per_event)\n",
    "                x_i = data[(inds[i]-train_window):inds[i],:]\n",
    "                y_i = trigs[i]\n",
    "\n",
    "                feats, featslabels = feats_from_window(x_i[-300:],chanmech,mechheaders)\n",
    "\n",
    "                allfeats.append(feats)\n",
    "\n",
    "                eventwin_list.append(x_i[::DS_FACTOR])\n",
    "                trig_list.append(y_i)\n",
    "                aug_bin.append(False)\n",
    "\n",
    "                # Add augmented (i.e. shifted) windows\n",
    "                if to_aug:\n",
    "                    for shift in aug_start:\n",
    "                        x_i = data[(inds[i]+shift-train_window):(inds[i]+shift),:]\n",
    "                        y_i = trigs[i]\n",
    "\n",
    "                        feats, featslabels = feats_from_window(x_i[-300:],chanmech,mechheaders)\n",
    "\n",
    "                        allfeats.append(feats)                   \n",
    "\n",
    "                        eventwin_list.append(x_i[::DS_FACTOR])\n",
    "                        trig_list.append(y_i)\n",
    "                        aug_bin.append(True)\n",
    "\n",
    "            # Convert from list of arrays to 3D array        \n",
    "            alleventdict[eventkey] = [np.array(eventwin_list)] # Raw data\n",
    "            alleventdict[eventkey].append(np.array(allfeats)) # Extracted features\n",
    "            alleventdict[eventkey].append(np.array(trig_list)) # Target\n",
    "            alleventdict[eventkey].append(aug_bin)\n",
    "        else:\n",
    "            print('No ' + eventkey + ' events...')\n",
    "            alleventdict[eventkey] = [[],[],[],[]]\n",
    "            \n",
    "    # alleventdict has the gait event (HC, TO) as the key and contains the 3d array of the extracted windows (300 ms for feature extraction and pre_stance, pre_swing) and targets\n",
    "    return alleventdict\n",
    "\n",
    "\n",
    "def feats_from_window(window,chanmech,mechheaders):\n",
    "    if len(chanmech) > 0:\n",
    "        mechfeats, mechfeatsnames = getmechfeats(window,chanmech,mechheaders)\n",
    "    else:\n",
    "        mechfeats = []\n",
    "\n",
    "    feats = mechfeats.T    \n",
    "    featslabels = mechfeatsnames\n",
    "    \n",
    "    return feats, featslabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def butter_BP(lowcut, highcut, fs, order):\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    b, a = signal.butter(order, [low, high], btype='bandpass')\n",
    "    return b, a\n",
    "\n",
    "\n",
    "def butter_LP(lowcut, fs, order):\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    b, a = signal.butter(order, low, btype='lowpass')\n",
    "    return b, a\n",
    "\n",
    "\n",
    "def butter_HP(highcut, fs, order):\n",
    "    nyq = 0.5 * fs\n",
    "    high = highcut / nyq\n",
    "    b, a = signal.butter(order, high, btype='highpass')\n",
    "    return b, a\n",
    "\n",
    "    \n",
    "def getmechfeats(X,chanmech,mechheaders):    \n",
    "    X_mech = X[:,chanmech]\n",
    "\n",
    "    X_min = np.min(X_mech,axis=0)\n",
    "    X_max = np.max(X_mech,axis=0)\n",
    "    X_mean = np.mean(X_mech,axis=0)\n",
    "    X_std = np.std(X_mech,axis=0)\n",
    "    X_init = X_mech[0]\n",
    "    X_final = X_mech[-1]\n",
    "\n",
    "    mechfeats = np.array([X_min,X_max,X_init,X_final,X_mean,X_std]).flatten()\n",
    "    mechfeatsnames = []\n",
    "    \n",
    "    featsnames = [' Min',' Max',' Initial',' Final',' Mean',' SD']\n",
    "    \n",
    "    for names in featsnames:\n",
    "        for mechchan in mechheaders:\n",
    "            mechfeatsnames.append(mechchan + names)\n",
    "\n",
    "    return mechfeats, mechfeatsnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unpack_trig(triggers):\n",
    "# Get the first and third digits of the four-digit trigger in order to make mode-specific classifiers\n",
    "\n",
    "    trigstr = triggers.astype(int).astype(str)\n",
    "    leavemode = np.array([int(trig[0]) for trig in trigstr])\n",
    "    entermode = np.array([int(trig[2]) for trig in trigstr])\n",
    "    steptype = np.array([int(leavemode[i] == entermode[i]) for i in range(len(leavemode))])\n",
    "    \n",
    "    return leavemode, entermode, steptype\n",
    "    \n",
    "    \n",
    "def split_by_leavemode(win_data, feat_data, leavemode, useleave, entermode, steptype, fileind, aug_bin):\n",
    "# Organize data into dictionary with two-layer keys: gait event (RHC, RTO, LHC, LTO) and mode (LW, RA, RD, SA, SD)\n",
    "# DS_FACTOR = down-sampling factor (may improve NN performance by reducing dimensionality of raw data)\n",
    "# fileind keeps track of which file each gait event came from (in order to do leave-one-circuit-out cross validation)\n",
    "    data_ms = {}\n",
    "        \n",
    "#     modecode = {'1': 'St', '2': 'LW', '3': 'SA', '4': 'SD', '5': 'RA', '6': 'RD'}\n",
    "    \n",
    "    lmode_inds = [ind for ind in range(len(leavemode)) if leavemode[ind] in useleave]\n",
    "    win_data_inds = win_data[lmode_inds]        \n",
    "    feat_data_inds = feat_data[lmode_inds]\n",
    "    target_inds = entermode[lmode_inds]\n",
    "    file_inds = fileind[lmode_inds]\n",
    "    aug_bin_inds = aug_bin[lmode_inds]\n",
    "    type_bin_inds = steptype[lmode_inds]\n",
    "    \n",
    "    # Merge RA targets with LW targets\n",
    "#     target_inds[target_inds == 5] = 2\n",
    "\n",
    "    data_ms = [win_data_inds, feat_data_inds, target_inds, file_inds, aug_bin_inds, type_bin_inds]\n",
    "    \n",
    "    return data_ms \n",
    " \n",
    "\n",
    "def make_modespec(alldict,USE_CHAN,USE_FEAT):\n",
    "# Make modespec_dict to organize all events/modes\n",
    "    \n",
    "    hc_leave, hc_enter, hc_type = unpack_trig(alldict['Combined HC Triggers'])\n",
    "    mst_leave, mst_enter, mst_type = unpack_trig(alldict['Combined MST Triggers'])\n",
    "    to_leave, to_enter, to_type = unpack_trig(alldict['Combined TO Triggers'])\n",
    "    msw_leave, msw_enter, msw_type = unpack_trig(alldict['Combined MSW Triggers'])\n",
    "    shc_leave, shc_enter, shc_type = unpack_trig(alldict['Combined SHC Triggers'])\n",
    "    sto_leave, sto_enter, sto_type = unpack_trig(alldict['Combined STO Triggers'])\n",
    "    \n",
    "#     hc_lw_ms = split_by_leavemode(alldict['Combined HC Windows'][:,:,USE_CHAN], alldict['Combined HC Features'][:,USE_FEAT], hc_leave, [2,5], hc_enter, hc_type, alldict['Combined HC File Index'], alldict['Combined HC Augmented'])\n",
    "    hc_lw_ms = split_by_leavemode(alldict['Combined HC Windows'][:,:,USE_CHAN], alldict['Combined HC Features'][:,USE_FEAT], hc_leave, [2], hc_enter, hc_type, alldict['Combined HC File Index'], alldict['Combined HC Augmented'])\n",
    "    hc_ra_ms = split_by_leavemode(alldict['Combined HC Windows'][:,:,USE_CHAN], alldict['Combined HC Features'][:,USE_FEAT], hc_leave, [5], hc_enter, hc_type, alldict['Combined HC File Index'], alldict['Combined HC Augmented'])\n",
    "    hc_rd_ms = split_by_leavemode(alldict['Combined HC Windows'][:,:,USE_CHAN], alldict['Combined HC Features'][:,USE_FEAT], hc_leave, [6], hc_enter, hc_type, alldict['Combined HC File Index'], alldict['Combined HC Augmented'])\n",
    "    hc_sd_ms = split_by_leavemode(alldict['Combined HC Windows'][:,:,USE_CHAN], alldict['Combined HC Features'][:,USE_FEAT], hc_leave, [4], hc_enter, hc_type, alldict['Combined HC File Index'], alldict['Combined HC Augmented'])\n",
    "    \n",
    "    mst_sd_ms = split_by_leavemode(alldict['Combined MST Windows'][:,:,USE_CHAN], alldict['Combined MST Features'][:,USE_FEAT], mst_leave, [4], mst_enter, mst_type, alldict['Combined MST File Index'], alldict['Combined MST Augmented'])\n",
    "    to_ms = split_by_leavemode(alldict['Combined TO Windows'][:,:,USE_CHAN], alldict['Combined TO Features'][:,USE_FEAT], to_leave, [2,4,5,6], to_enter, to_type, alldict['Combined TO File Index'], alldict['Combined TO Augmented'])\n",
    "    msw_sa_ms = split_by_leavemode(alldict['Combined MSW Windows'][:,:,USE_CHAN], alldict['Combined MSW Features'][:,USE_FEAT], msw_leave, [3], msw_enter, msw_type, alldict['Combined MSW File Index'], alldict['Combined MSW Augmented'])\n",
    "    shc_ms = split_by_leavemode(alldict['Combined SHC Windows'][:,:,USE_CHAN], alldict['Combined SHC Features'][:,USE_FEAT], shc_leave, [1], shc_enter, shc_type, alldict['Combined SHC File Index'], alldict['Combined SHC Augmented'])\n",
    "    sto_ms = split_by_leavemode(alldict['Combined STO Windows'][:,:,USE_CHAN], alldict['Combined STO Features'][:,USE_FEAT], sto_leave, [1], sto_enter, sto_type, alldict['Combined STO File Index'], alldict['Combined STO Augmented'])\n",
    "    \n",
    "    modespec_dict = {'HC_LW': hc_lw_ms, 'HC_RA': hc_ra_ms, 'HC_RD': hc_rd_ms, 'HC_SD': hc_sd_ms,\n",
    "                     'MST_SD': mst_sd_ms, 'TO': to_ms, 'MSW_SA': msw_sa_ms, 'SHC': shc_ms, 'STO': sto_ms}\n",
    "    \n",
    "    return modespec_dict    \n",
    "\n",
    "\n",
    "def lda_classify(train_in,train_targ,test_in,test_targ,pca_proportion):\n",
    "    lda = LinearDiscriminantAnalysis()\n",
    "    \n",
    "    train_in_scaler = preprocessing.StandardScaler().fit(train_in)\n",
    "    train_in_scaled = train_in_scaler.transform(train_in)\n",
    "    test_in_scaled = train_in_scaler.transform(test_in)\n",
    "\n",
    "    train_in_scaler_pca = PCA().fit(train_in_scaled)\n",
    "    train_in_scaled_pca_xfm = train_in_scaler_pca.transform(train_in_scaled)\n",
    "    test_in_scaled_pca_xfm = train_in_scaler_pca.transform(test_in_scaled)\n",
    "\n",
    "    pcaexplainedvar = np.cumsum(train_in_scaler_pca.explained_variance_ratio_)\n",
    "    if pca_proportion < 1:\n",
    "        pcanumcomps = min(min(np.where(pcaexplainedvar > pca_proportion))) + 1\n",
    "    else:\n",
    "        pcanumcomps = pca_proportion\n",
    "\n",
    "    unique_modes = np.unique(train_targ)\n",
    "    lda.set_params(priors = np.ones(len(unique_modes))/len(unique_modes))\n",
    "\n",
    "    test_pred = lda.fit(train_in_scaled_pca_xfm[:,0:pcanumcomps],train_targ).predict(test_in_scaled_pca_xfm[:,0:pcanumcomps])\n",
    "    test_pred_prob = lda.fit(train_in_scaled_pca_xfm[:,0:pcanumcomps],train_targ).predict_proba(test_in_scaled_pca_xfm[:,0:pcanumcomps])\n",
    "    test_gtruth = test_targ\n",
    "    \n",
    "    return test_pred, test_gtruth, test_pred_prob\n",
    "\n",
    "    \n",
    "def ms_classify_results(msdict,pcaprop,trainfiles,testfiles):\n",
    "    ohe = preprocessing.OneHotEncoder()\n",
    "    lda = LinearDiscriminantAnalysis()\n",
    "    kf = KFold(n_splits=10, shuffle=True)\n",
    "    loo = LeaveOneOut()\n",
    "    \n",
    "    # Define histogram labels\n",
    "    gaitmodes = ['St','LW','SA','SD','RA','RD']\n",
    "    \n",
    "    all_pred, all_gtruth, all_type, all_files = [], [], [], []\n",
    "    \n",
    "    for classifiers in list(allevents_ms.keys()):\n",
    "        print(classifiers)\n",
    "        \n",
    "        event_mode_windows = msdict[classifiers][0]\n",
    "        event_mode_features = msdict[classifiers][1]\n",
    "        event_mode_targets = msdict[classifiers][2]\n",
    "        event_mode_files = msdict[classifiers][3]\n",
    "        event_mode_augbin = msdict[classifiers][4]\n",
    "        event_mode_typebin = msdict[classifiers][5]\n",
    "    \n",
    "#         train_inds = [event_mode_files[fileind] in trainfiles for fileind in np.arange(len(event_mode_files))]\n",
    "#         test_inds = [event_mode_files[fileind] in testfiles and event_mode_augbin[fileind] == False for fileind in np.arange(len(event_mode_files))]\n",
    "        \n",
    "        print('Unique: {}'.format(np.unique(event_mode_targets)))\n",
    "        print(gaitmodes)\n",
    "        print(np.histogram(event_mode_targets,bins=[0.5, 1.5, 2.5, 3.5, 4.5, 5.5, 6.5])[0])  \n",
    "    \n",
    "        event_mode_pred = np.zeros((len(event_mode_targets),1))\n",
    "    \n",
    "        for train_inds, test_inds in loo.split(event_mode_features,event_mode_targets):\n",
    "#             if len(msdict[classifiers][0]) > 0 and len(np.unique(event_mode_targets[train_inds])) > 1:\n",
    "#             print(test_inds)\n",
    "        \n",
    "            pred, gtruth, pred_prob = lda_classify(event_mode_features[train_inds],event_mode_targets[train_inds],event_mode_features[test_inds],event_mode_targets[test_inds],pcaprop)           \n",
    "            event_mode_pred[test_inds] = pred\n",
    "#                 print('Unique: {}'.format(np.unique(event_mode_targets[train_inds])))\n",
    "#                 print(gaitmodes)\n",
    "#                 print(np.histogram(event_mode_targets[train_inds],bins=[0.5, 1.5, 2.5, 3.5, 4.5, 5.5, 6.5])[0])           \n",
    "#                 print('Training instances: {}'.format(len(event_mode_targets[train_inds])))\n",
    "#                 print('Testing instances: {}'.format(len(event_mode_targets[test_inds])))\n",
    "\n",
    "#                 try:\n",
    "#                     pred, gtruth, pred_prob = lda_classify(event_mode_features[train_inds],event_mode_targets[train_inds],event_mode_features[test_inds],event_mode_targets[test_inds],0.95)           \n",
    "#                     acc = np.sum(np.equal(pred,gtruth))/len(pred)\n",
    "\n",
    "#                     print('Testing Accuracy: {}'.format(acc))\n",
    "#                     print()\n",
    "#                 except:\n",
    "#                     print('...Skipping this event/mode...')          \n",
    "#                     print()\n",
    "#             else:\n",
    "#                 print('Unique: {}'.format(np.unique(event_mode_targets[train_inds])))\n",
    "#                 print(gaitmodes)\n",
    "#                 print(np.histogram(event_mode_targets[train_inds],bins=[0.5, 1.5, 2.5, 3.5, 4.5, 5.5, 6.5])[0])           \n",
    "#                 print('Training instances: {}'.format(len(event_mode_targets[train_inds])))\n",
    "#                 print('Testing instances: {}'.format(len(event_mode_targets[test_inds])))\n",
    "#                 print('...Skipping this event/mode...')          \n",
    "#                 print()\n",
    "        \n",
    "#         acc = np.sum(np.equal(event_mode_pred,event_mode_targets))/len(event_mode_pred)\n",
    "        print('LOO Accuracy: {}'.format(accuracy_score(event_mode_pred,event_mode_targets)))\n",
    "        print()\n",
    "        \n",
    "        all_pred.append(event_mode_pred)\n",
    "        all_gtruth.append(event_mode_targets)\n",
    "        all_type.append(event_mode_typebin)\n",
    "        all_files.append(event_mode_files)\n",
    "    \n",
    "    all_pred = np.concatenate(all_pred)\n",
    "    all_gtruth = np.concatenate(all_gtruth)\n",
    "    all_type = np.concatenate(all_type)\n",
    "    all_files = np.concatenate(all_files)\n",
    "    \n",
    "    return all_pred, all_gtruth, all_type, all_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def select_chan(USE_MECH_CHAN_NAME,allcolheaders,allfeatnames):\n",
    "    for featind, featname in enumerate(allfeatnames):\n",
    "        if '_' in featname:\n",
    "            allfeatnames[featind] = featname.replace('_',' ')\n",
    "    \n",
    "    for colind, colheader in enumerate(allcolheaders):\n",
    "        if '_' in colheader:\n",
    "            allcolheaders[colind] = colheader.replace('_',' ')\n",
    "    \n",
    "    if len(USE_MECH_CHAN_NAME) > 0:\n",
    "        USE_MECH_CHAN = []\n",
    "        USE_MECH_FEAT = []\n",
    "        for channameind, channame in enumerate(USE_MECH_CHAN_NAME): \n",
    "            MECH_CHAN_IND = [allcolheaders.index(colname) for colname in allcolheaders if channame in colname]\n",
    "            MECH_FEAT_IND = [allfeatnames.index(colname) for colname in allfeatnames if channame in colname]\n",
    "            USE_MECH_CHAN.append(MECH_CHAN_IND)\n",
    "            USE_MECH_FEAT.append(MECH_FEAT_IND)\n",
    "        \n",
    "        USE_MECH_CHAN = np.array(sorted(sum(USE_MECH_CHAN,[])),dtype=int)\n",
    "        USE_MECH_FEAT = np.array(sorted(sum(USE_MECH_FEAT,[])),dtype=int)\n",
    "\n",
    "        USE_CHAN = USE_MECH_CHAN\n",
    "        USE_FEAT = USE_MECH_FEAT\n",
    "    else:\n",
    "        USE_CHAN = np.arange(len(allcolheaders))\n",
    "        USE_FEAT = np.arange(len(allfeatnames))\n",
    "    \n",
    "    return np.unique(USE_CHAN), np.unique(USE_FEAT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and organize raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 129 processed data file(s) ...\n",
      "=== circuit_even_121917_data_001_py.mat ===\n",
      "=== circuit_even_121917_data_003_py.mat ===\n",
      "=== circuit_even_121917_data_004_py.mat ===\n",
      "=== circuit_even_121917_data_007_py.mat ===\n",
      "=== circuit_even_121917_data_009_py.mat ===\n",
      "=== circuit_even_121917_data_010_py.mat ===\n",
      "=== circuit_even_121917_data_012_py.mat ===\n",
      "=== circuit_even_121917_data_013_py.mat ===\n",
      "=== circuit_even_121917_data_015_py.mat ===\n",
      "=== circuit_even_121917_data_017_py.mat ===\n",
      "=== circuit_even_121917_data_020_py.mat ===\n",
      "=== circuit_even_121917_data_022_py.mat ===\n",
      "=== circuit_even_121917_data_023_py.mat ===\n",
      "=== circuit_even_121917_data_025_py.mat ===\n",
      "=== circuit_even_121917_data_026_py.mat ===\n",
      "=== circuit_even_121917_data_028_py.mat ===\n",
      "=== circuit_even_121917_data_030_py.mat ===\n",
      "=== circuit_even_121917_data_032_py.mat ===\n",
      "=== circuit_even_121917_data_033_py.mat ===\n",
      "=== circuit_even_121917_data_035_py.mat ===\n",
      "=== circuit_even_121917_data_036_py.mat ===\n",
      "=== circuit_even_121917_data_038_py.mat ===\n",
      "=== circuit_even_121917_data_039_py.mat ===\n",
      "=== circuit_even_121917_data_041_py.mat ===\n",
      "=== circuit_even_121917_data_042_py.mat ===\n",
      "=== circuit_even_121917_data_044_py.mat ===\n",
      "=== circuit_even_121917_data_045_py.mat ===\n",
      "=== circuit_even_121917_data_047_py.mat ===\n",
      "=== circuit_even_121917_data_048_py.mat ===\n",
      "=== circuit_even_121917_data_050_py.mat ===\n",
      "=== circuit_even_121917_data_051_py.mat ===\n",
      "=== circuit_even_121917_data_053_py.mat ===\n",
      "=== circuit_even_121917_data_054_py.mat ===\n",
      "=== circuit_even_121917_data_056_py.mat ===\n",
      "=== circuit_even_121917_data_057_py.mat ===\n",
      "=== circuit_even_121917_data_059_py.mat ===\n",
      "=== circuit_even_121917_data_060_py.mat ===\n",
      "=== circuit_even_121917_data_062_py.mat ===\n",
      "=== circuit_odd_121917_data_001_py.mat ===\n",
      "=== circuit_odd_121917_data_002_py.mat ===\n",
      "=== circuit_odd_121917_data_003_py.mat ===\n",
      "=== circuit_odd_121917_data_004_py.mat ===\n",
      "=== circuit_odd_121917_data_005_py.mat ===\n",
      "=== circuit_odd_121917_data_006_py.mat ===\n",
      "=== circuit_odd_121917_data_007_py.mat ===\n",
      "=== circuit_odd_121917_data_008_py.mat ===\n",
      "=== circuit_odd_121917_data_009_py.mat ===\n",
      "=== circuit_odd_121917_data_010_py.mat ===\n",
      "=== circuit_odd_121917_data_011_py.mat ===\n",
      "=== circuit_odd_121917_data_012_py.mat ===\n",
      "=== circuit_odd_121917_data_014_py.mat ===\n",
      "=== circuit_odd_121917_data_015_py.mat ===\n",
      "=== circuit_odd_121917_data_016_py.mat ===\n",
      "=== circuit_odd_121917_data_017_py.mat ===\n",
      "=== circuit_odd_121917_data_019_py.mat ===\n",
      "=== circuit_odd_121917_data_020_py.mat ===\n",
      "=== circuit_odd_121917_data_021_py.mat ===\n",
      "=== circuit_odd_121917_data_023_py.mat ===\n",
      "=== circuit_odd_121917_data_024_py.mat ===\n",
      "=== circuit_odd_121917_data_025_py.mat ===\n",
      "=== circuit_odd_121917_data_027_py.mat ===\n",
      "=== circuit_odd_121917_data_028_py.mat ===\n",
      "=== circuit_odd_121917_data_029_py.mat ===\n",
      "=== circuit_odd_121917_data_030_py.mat ===\n",
      "=== circuit_odd_121917_data_031_py.mat ===\n",
      "=== circuit_odd_RIC_121917_data_001_py.mat ===\n",
      "=== circuit_odd_RIC_121917_data_002_py.mat ===\n",
      "=== circuit_odd_RIC_121917_data_003_py.mat ===\n",
      "=== circuit_odd_RIC_121917_data_004_py.mat ===\n",
      "=== circuit_odd_RIC_121917_data_005_py.mat ===\n",
      "=== circuit_odd_RIC_121917_data_006_py.mat ===\n",
      "=== circuit_odd_RIC_121917_data_007_py.mat ===\n",
      "=== circuit_odd_RIC_121917_data_008_py.mat ===\n",
      "=== circuit_odd_RIC_121917_data_009_py.mat ===\n",
      "=== circuit_odd_RIC_121917_data_010_py.mat ===\n",
      "=== circuit_odd_RIC_121917_data_011_py.mat ===\n",
      "=== circuit_odd_RIC_121917_data_012_py.mat ===\n",
      "=== circuit_odd_RIC_121917_data_013_py.mat ===\n",
      "=== circuit_odd_RIC_121917_data_014_py.mat ===\n",
      "=== circuit_odd_RIC_121917_data_015_py.mat ===\n",
      "=== circuit_odd_RIC_121917_data_016_py.mat ===\n",
      "=== circuit_odd_RIC_121917_data_017_py.mat ===\n",
      "=== circuit_odd_RIC_121917_data_018_py.mat ===\n",
      "=== circuit_odd_RIC_121917_data_019_py.mat ===\n",
      "=== ramp_121917_data_001_py.mat ===\n",
      "=== ramp_121917_data_002_py.mat ===\n",
      "=== ramp_121917_data_003_py.mat ===\n",
      "=== ramp_121917_data_004_py.mat ===\n",
      "=== ramp_121917_data_005_py.mat ===\n",
      "=== ramp_121917_data_006_py.mat ===\n",
      "=== ramp_121917_data_008_py.mat ===\n",
      "=== ramp_121917_data_009_py.mat ===\n",
      "=== ramp_121917_data_010_py.mat ===\n",
      "=== stair_stand_121917_data_001_py.mat ===\n",
      "=== stair_stand_121917_data_002_py.mat ===\n",
      "=== stair_stand_121917_data_003_py.mat ===\n",
      "=== stair_stand_121917_data_004_py.mat ===\n",
      "=== stair_stand_121917_data_005_py.mat ===\n",
      "=== stair_stand_121917_data_006_py.mat ===\n",
      "=== stair_stand_121917_data_007_py.mat ===\n",
      "=== stair_stand_121917_data_008_py.mat ===\n",
      "=== stair_stand_121917_data_009_py.mat ===\n",
      "=== stair_stand_121917_data_010_py.mat ===\n",
      "=== stand_121917_data_001_py.mat ===\n",
      "=== stand_121917_data_002_py.mat ===\n",
      "=== stand_121917_data_003_py.mat ===\n",
      "=== stand_121917_data_004_py.mat ===\n",
      "=== stand_121917_data_005_py.mat ===\n",
      "=== stand_121917_data_006_py.mat ===\n",
      "=== stand_121917_data_007_py.mat ===\n",
      "=== stand_121917_data_008_py.mat ===\n",
      "=== stand_121917_data_009_py.mat ===\n",
      "=== stand_121917_data_010_py.mat ===\n",
      "=== walk_121917_data_001_py.mat ===\n",
      "=== walk_121917_data_002_py.mat ===\n",
      "=== walk_121917_data_003_py.mat ===\n",
      "=== walk_121917_data_004_py.mat ===\n",
      "=== walk_121917_data_005_py.mat ===\n",
      "=== walk_121917_data_006_py.mat ===\n",
      "=== walk_121917_data_007_py.mat ===\n",
      "=== walk_121917_data_008_py.mat ===\n",
      "=== walk_121917_data_009_py.mat ===\n",
      "=== walk_121917_data_010_py.mat ===\n",
      "=== walk_121917_data_011_py.mat ===\n",
      "=== walk_121917_data_012_py.mat ===\n",
      "=== walk_121917_data_013_py.mat ===\n",
      "=== walk_121917_data_014_py.mat ===\n",
      "=== walk_121917_data_015_py.mat ===\n",
      "Finished!\n"
     ]
    }
   ],
   "source": [
    "# Just get the filenames from the filedir\n",
    "# FILEDIR = u'C:\\\\Users\\\\bhu\\\\Desktop\\\\Contra Pilot\\\\PYTHON'\n",
    "FILEDIR = u'Z:\\\\Lab Member Folders\\\\Blair Hu\\\\Contralateral Prosthesis Control 2017\\\\Pilot Data\\\\PYTHON'\n",
    "NUM_FILES = [] # if empty list, load all files; otherwise [X] means load X files\n",
    "DELAYED = False\n",
    "filedict, allindtrigsdict, allcolheaders = load_vu_all_files(FILEDIR, NUM_FILES, DELAYED)\n",
    "filekeys = sorted(list(filedict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomly select train/test data based on testing data proportion (5%):\n",
      "No data augmentation...\n",
      "\n",
      "Preparing file 1: circuit_even_121917_data_001_py.mat ...\n",
      "Preparing file 2: circuit_even_121917_data_003_py.mat ...\n",
      "Preparing file 3: circuit_even_121917_data_004_py.mat ...\n",
      "Preparing file 4: circuit_even_121917_data_007_py.mat ...\n",
      "Preparing file 5: circuit_even_121917_data_009_py.mat ...\n",
      "Preparing file 6: circuit_even_121917_data_010_py.mat ...\n",
      "Preparing file 7: circuit_even_121917_data_012_py.mat ...\n",
      "Preparing file 8: circuit_even_121917_data_013_py.mat ...\n",
      "Preparing file 9: circuit_even_121917_data_015_py.mat ...\n",
      "Preparing file 10: circuit_even_121917_data_017_py.mat ...\n",
      "No SHC events...\n",
      "No STO events...\n",
      "Preparing file 11: circuit_even_121917_data_020_py.mat ...\n",
      "Preparing file 12: circuit_even_121917_data_022_py.mat ...\n",
      "Preparing file 13: circuit_even_121917_data_023_py.mat ...\n",
      "Preparing file 14: circuit_even_121917_data_025_py.mat ...\n",
      "Preparing file 15: circuit_even_121917_data_026_py.mat ...\n",
      "Preparing file 16: circuit_even_121917_data_028_py.mat ...\n",
      "Preparing file 17: circuit_even_121917_data_030_py.mat ...\n",
      "No SHC events...\n",
      "No STO events...\n",
      "Preparing file 18: circuit_even_121917_data_032_py.mat ...\n",
      "Preparing file 19: circuit_even_121917_data_033_py.mat ...\n",
      "Preparing file 20: circuit_even_121917_data_035_py.mat ...\n",
      "No STO events...\n",
      "Preparing file 21: circuit_even_121917_data_036_py.mat ...\n",
      "Preparing file 22: circuit_even_121917_data_038_py.mat ...\n",
      "No STO events...\n",
      "Preparing file 23: circuit_even_121917_data_039_py.mat ...\n",
      "Preparing file 24: circuit_even_121917_data_041_py.mat ...\n",
      "Preparing file 25: circuit_even_121917_data_042_py.mat ...\n",
      "Preparing file 26: circuit_even_121917_data_044_py.mat ...\n",
      "Preparing file 27: circuit_even_121917_data_045_py.mat ...\n",
      "Preparing file 28: circuit_even_121917_data_047_py.mat ...\n",
      "No STO events...\n",
      "Preparing file 29: circuit_even_121917_data_048_py.mat ...\n",
      "Preparing file 30: circuit_even_121917_data_050_py.mat ...\n",
      "Preparing file 31: circuit_even_121917_data_051_py.mat ...\n",
      "Preparing file 32: circuit_even_121917_data_053_py.mat ...\n",
      "Preparing file 33: circuit_even_121917_data_054_py.mat ...\n",
      "Preparing file 34: circuit_even_121917_data_056_py.mat ...\n",
      "No STO events...\n",
      "Preparing file 35: circuit_even_121917_data_057_py.mat ...\n",
      "Preparing file 36: circuit_even_121917_data_059_py.mat ...\n",
      "Preparing file 37: circuit_even_121917_data_060_py.mat ...\n",
      "No SHC events...\n",
      "Preparing file 38: circuit_even_121917_data_062_py.mat ...\n",
      "Preparing file 39: circuit_odd_121917_data_001_py.mat ...\n",
      "Preparing file 40: circuit_odd_121917_data_002_py.mat ...\n",
      "Preparing file 41: circuit_odd_121917_data_003_py.mat ...\n",
      "Preparing file 42: circuit_odd_121917_data_004_py.mat ...\n",
      "Preparing file 43: circuit_odd_121917_data_005_py.mat ...\n",
      "Preparing file 44: circuit_odd_121917_data_006_py.mat ...\n",
      "Preparing file 45: circuit_odd_121917_data_007_py.mat ...\n",
      "No STO events...\n",
      "Preparing file 46: circuit_odd_121917_data_008_py.mat ...\n",
      "Preparing file 47: circuit_odd_121917_data_009_py.mat ...\n",
      "No TO events...\n",
      "Preparing file 48: circuit_odd_121917_data_010_py.mat ...\n",
      "Preparing file 49: circuit_odd_121917_data_011_py.mat ...\n",
      "Preparing file 50: circuit_odd_121917_data_012_py.mat ...\n",
      "No STO events...\n",
      "Preparing file 51: circuit_odd_121917_data_014_py.mat ...\n",
      "Preparing file 52: circuit_odd_121917_data_015_py.mat ...\n",
      "Preparing file 53: circuit_odd_121917_data_016_py.mat ...\n",
      "Preparing file 54: circuit_odd_121917_data_017_py.mat ...\n",
      "Preparing file 55: circuit_odd_121917_data_019_py.mat ...\n",
      "Preparing file 56: circuit_odd_121917_data_020_py.mat ...\n",
      "Preparing file 57: circuit_odd_121917_data_021_py.mat ...\n",
      "Preparing file 58: circuit_odd_121917_data_023_py.mat ...\n",
      "Preparing file 59: circuit_odd_121917_data_024_py.mat ...\n",
      "Preparing file 60: circuit_odd_121917_data_025_py.mat ...\n",
      "Preparing file 61: circuit_odd_121917_data_027_py.mat ...\n",
      "Preparing file 62: circuit_odd_121917_data_028_py.mat ...\n",
      "Preparing file 63: circuit_odd_121917_data_029_py.mat ...\n",
      "Preparing file 64: circuit_odd_121917_data_030_py.mat ...\n",
      "Preparing file 65: circuit_odd_121917_data_031_py.mat ...\n",
      "Preparing file 66: circuit_odd_RIC_121917_data_001_py.mat ...\n",
      "Preparing file 67: circuit_odd_RIC_121917_data_002_py.mat ...\n",
      "Preparing file 68: circuit_odd_RIC_121917_data_003_py.mat ...\n",
      "Preparing file 69: circuit_odd_RIC_121917_data_004_py.mat ...\n",
      "Preparing file 70: circuit_odd_RIC_121917_data_005_py.mat ...\n",
      "Preparing file 71: circuit_odd_RIC_121917_data_006_py.mat ...\n",
      "Preparing file 72: circuit_odd_RIC_121917_data_007_py.mat ...\n",
      "Preparing file 73: circuit_odd_RIC_121917_data_008_py.mat ...\n",
      "Preparing file 74: circuit_odd_RIC_121917_data_009_py.mat ...\n",
      "No SHC events...\n",
      "Preparing file 75: circuit_odd_RIC_121917_data_010_py.mat ...\n",
      "Preparing file 76: circuit_odd_RIC_121917_data_011_py.mat ...\n",
      "Preparing file 77: circuit_odd_RIC_121917_data_012_py.mat ...\n",
      "No STO events...\n",
      "Preparing file 78: circuit_odd_RIC_121917_data_013_py.mat ...\n",
      "Preparing file 79: circuit_odd_RIC_121917_data_014_py.mat ...\n",
      "Preparing file 80: circuit_odd_RIC_121917_data_015_py.mat ...\n",
      "Preparing file 81: circuit_odd_RIC_121917_data_016_py.mat ...\n",
      "Preparing file 82: circuit_odd_RIC_121917_data_017_py.mat ...\n",
      "No SHC events...\n",
      "No STO events...\n",
      "Preparing file 83: circuit_odd_RIC_121917_data_018_py.mat ...\n",
      "Preparing file 84: circuit_odd_RIC_121917_data_019_py.mat ...\n",
      "Preparing file 85: ramp_121917_data_001_py.mat ...\n",
      "Preparing file 86: ramp_121917_data_002_py.mat ...\n",
      "Preparing file 87: ramp_121917_data_003_py.mat ...\n",
      "Preparing file 88: ramp_121917_data_004_py.mat ...\n",
      "Preparing file 89: ramp_121917_data_005_py.mat ...\n",
      "Preparing file 90: ramp_121917_data_006_py.mat ...\n",
      "Preparing file 91: ramp_121917_data_008_py.mat ...\n",
      "Preparing file 92: ramp_121917_data_009_py.mat ...\n",
      "Preparing file 93: ramp_121917_data_010_py.mat ...\n",
      "Preparing file 94: stair_stand_121917_data_001_py.mat ...\n",
      "Preparing file 95: stair_stand_121917_data_002_py.mat ...\n",
      "Preparing file 96: stair_stand_121917_data_003_py.mat ...\n",
      "No TO events...\n",
      "Preparing file 97: stair_stand_121917_data_004_py.mat ...\n",
      "Preparing file 98: stair_stand_121917_data_005_py.mat ...\n",
      "No TO events...\n",
      "Preparing file 99: stair_stand_121917_data_006_py.mat ...\n",
      "Preparing file 100: stair_stand_121917_data_007_py.mat ...\n",
      "No TO events...\n",
      "Preparing file 101: stair_stand_121917_data_008_py.mat ...\n",
      "Preparing file 102: stair_stand_121917_data_009_py.mat ...\n",
      "No TO events...\n",
      "Preparing file 103: stair_stand_121917_data_010_py.mat ...\n",
      "Preparing file 104: stand_121917_data_001_py.mat ...\n",
      "Preparing file 105: stand_121917_data_002_py.mat ...\n",
      "No MST events...\n",
      "Preparing file 106: stand_121917_data_003_py.mat ...\n",
      "Preparing file 107: stand_121917_data_004_py.mat ...\n",
      "No MSW events...\n",
      "No MST events...\n",
      "No TO events...\n",
      "No HC events...\n",
      "Preparing file 108: stand_121917_data_005_py.mat ...\n",
      "No MST events...\n",
      "Preparing file 109: stand_121917_data_006_py.mat ...\n",
      "No MST events...\n",
      "Preparing file 110: stand_121917_data_007_py.mat ...\n",
      "No MSW events...\n",
      "No MST events...\n",
      "No TO events...\n",
      "No HC events...\n",
      "Preparing file 111: stand_121917_data_008_py.mat ...\n",
      "No MSW events...\n",
      "No MST events...\n",
      "No TO events...\n",
      "No HC events...\n",
      "Preparing file 112: stand_121917_data_009_py.mat ...\n",
      "No MSW events...\n",
      "No MST events...\n",
      "No TO events...\n",
      "No HC events...\n",
      "Preparing file 113: stand_121917_data_010_py.mat ...\n",
      "No MSW events...\n",
      "No MST events...\n",
      "No TO events...\n",
      "No HC events...\n",
      "Preparing file 114: walk_121917_data_001_py.mat ...\n",
      "Preparing file 115: walk_121917_data_002_py.mat ...\n",
      "Preparing file 116: walk_121917_data_003_py.mat ...\n",
      "Preparing file 117: walk_121917_data_004_py.mat ...\n",
      "Preparing file 118: walk_121917_data_005_py.mat ...\n",
      "No STO events...\n",
      "Preparing file 119: walk_121917_data_006_py.mat ...\n",
      "Preparing file 120: walk_121917_data_007_py.mat ...\n",
      "Preparing file 121: walk_121917_data_008_py.mat ...\n",
      "No MST events...\n",
      "Preparing file 122: walk_121917_data_009_py.mat ...\n",
      "No MSW events...\n",
      "No MST events...\n",
      "No TO events...\n",
      "No HC events...\n",
      "Preparing file 123: walk_121917_data_010_py.mat ...\n",
      "Preparing file 124: walk_121917_data_011_py.mat ...\n",
      "Preparing file 125: walk_121917_data_012_py.mat ...\n",
      "Preparing file 126: walk_121917_data_013_py.mat ...\n",
      "Preparing file 127: walk_121917_data_014_py.mat ...\n",
      "Preparing file 128: walk_121917_data_015_py.mat ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Aggregated input dimensions: (114718, 30, 31)\n",
      "Aggregated feature dimensions: (114718, 186)\n",
      "Aggregated target dimensions: (114718, 1)\n",
      "\n",
      "Aggregated HC feature dimensions: (912, 186)\n",
      "Aggregated MST feature dimensions: (743, 186)\n",
      "Aggregated TO feature dimensions: (815, 186)\n",
      "Aggregated MSW feature dimensions: (1002, 186)\n",
      "Aggregated SHC feature dimensions: (804, 186)\n",
      "Aggregated STO feature dimensions: (738, 186)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "TRAIN_SIZE = 300 # Training windows are 300 time-steps long for sliding windows\n",
    "\n",
    "PREST_TRAIN_SIZE = 300 # Training examples are 400 time-steps long before HC (captures the swing phase)\n",
    "PREMST_TRAIN_SIZE = 300 # Training examples are 500 time-steps long before MST (captures the swing phase and early stance)\n",
    "PRESW_TRAIN_SIZE = 300 # Training examples are 600 time-steps long before TO (captures the stance phase)\n",
    "PREMSW_TRAIN_SIZE = 300 # Training examples are 500 time-steps long before MSW (captures the stance phase and early swing)\n",
    "PRESHC_TRAIN_SIZE = 300 # Training examples are 300 time-steps long before SHC\n",
    "PRESTO_TRAIN_SIZE = 300 # Training examples are 300 time-steps long before STO\n",
    "\n",
    "PRED_SIZE = 30 # Prediction is the 30th future time step\n",
    "STEP_SIZE = 30 # Window increment is one time-step\n",
    "DS_FACTOR = 10 # Down-sampling factor for NN inputs\n",
    "\n",
    "PRE_AUG = 30 # Start with window beginning 60 ms BEFORE the gait event\n",
    "POST_AUG = 30 # End with window beginning 60 ms AFTER the gait event\n",
    "NUM_AUG = 0 # Extract 4 augmented windows\n",
    "\n",
    "# FOR_TEST = ['circuit_even_data_020.mat','circuit_odd_data_019.mat'] # Filenames to use for testing\n",
    "FOR_TEST = [0.05]\n",
    "\n",
    "CHAN_MECH = np.arange(31)\n",
    "\n",
    "PRINT_OUT = False # Print dimensions of data for each file\n",
    "PRINT_SUMMARY = True # Print dimensions of aggregated data from all files\n",
    "\n",
    "arginput = [TRAIN_SIZE,PREST_TRAIN_SIZE,PREMST_TRAIN_SIZE,PRESW_TRAIN_SIZE,PREMSW_TRAIN_SIZE,PRESHC_TRAIN_SIZE,PRESTO_TRAIN_SIZE,PRED_SIZE,STEP_SIZE,DS_FACTOR,PRE_AUG,POST_AUG,NUM_AUG,FOR_TEST,CHAN_MECH,PRINT_OUT,PRINT_SUMMARY]\n",
    "\n",
    "alldict, allfeatnames, train_files, test_files = unpack_files(filedict,allindtrigsdict,filekeys,arginput,allcolheaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "orig_train_files = np.copy(train_files)\n",
    "orig_test_files = np.copy(test_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====Using the following 31 channels:=====\n",
      "Knee Angle\n",
      "Knee Vel\n",
      "Knee Current\n",
      "Ankle Angle\n",
      "Ankle Vel\n",
      "Ankle Current\n",
      "VU Ax\n",
      "VU Ay\n",
      "VU Az\n",
      "VU Gx\n",
      "VU Gz\n",
      "VU Gy\n",
      "Shank Angle\n",
      "Thigh Angle\n",
      "Knee Ref\n",
      "Ankle Ref\n",
      "Load\n",
      "Shank Ax\n",
      "Shank Ay\n",
      "Shank Az\n",
      "Shank Gy\n",
      "Shank Gz\n",
      "Shank Gx\n",
      "Thigh Ax\n",
      "Thigh Ay\n",
      "Thigh Az\n",
      "Thigh Gy\n",
      "Thigh Gz\n",
      "Thigh Gx\n",
      "Contra Shank\n",
      "Contra Thigh\n",
      "\n",
      "=====Using the following 186 features:=====\n",
      "Knee Angle Min\n",
      "Knee Vel Min\n",
      "Knee Current Min\n",
      "Ankle Angle Min\n",
      "Ankle Vel Min\n",
      "Ankle Current Min\n",
      "VU Ax Min\n",
      "VU Ay Min\n",
      "VU Az Min\n",
      "VU Gx Min\n",
      "VU Gz Min\n",
      "VU Gy Min\n",
      "Shank Angle Min\n",
      "Thigh Angle Min\n",
      "Knee Ref Min\n",
      "Ankle Ref Min\n",
      "Load Min\n",
      "Shank Ax Min\n",
      "Shank Ay Min\n",
      "Shank Az Min\n",
      "Shank Gy Min\n",
      "Shank Gz Min\n",
      "Shank Gx Min\n",
      "Thigh Ax Min\n",
      "Thigh Ay Min\n",
      "Thigh Az Min\n",
      "Thigh Gy Min\n",
      "Thigh Gz Min\n",
      "Thigh Gx Min\n",
      "Contra Shank Min\n",
      "Contra Thigh Min\n",
      "Knee Angle Max\n",
      "Knee Vel Max\n",
      "Knee Current Max\n",
      "Ankle Angle Max\n",
      "Ankle Vel Max\n",
      "Ankle Current Max\n",
      "VU Ax Max\n",
      "VU Ay Max\n",
      "VU Az Max\n",
      "VU Gx Max\n",
      "VU Gz Max\n",
      "VU Gy Max\n",
      "Shank Angle Max\n",
      "Thigh Angle Max\n",
      "Knee Ref Max\n",
      "Ankle Ref Max\n",
      "Load Max\n",
      "Shank Ax Max\n",
      "Shank Ay Max\n",
      "Shank Az Max\n",
      "Shank Gy Max\n",
      "Shank Gz Max\n",
      "Shank Gx Max\n",
      "Thigh Ax Max\n",
      "Thigh Ay Max\n",
      "Thigh Az Max\n",
      "Thigh Gy Max\n",
      "Thigh Gz Max\n",
      "Thigh Gx Max\n",
      "Contra Shank Max\n",
      "Contra Thigh Max\n",
      "Knee Angle Initial\n",
      "Knee Vel Initial\n",
      "Knee Current Initial\n",
      "Ankle Angle Initial\n",
      "Ankle Vel Initial\n",
      "Ankle Current Initial\n",
      "VU Ax Initial\n",
      "VU Ay Initial\n",
      "VU Az Initial\n",
      "VU Gx Initial\n",
      "VU Gz Initial\n",
      "VU Gy Initial\n",
      "Shank Angle Initial\n",
      "Thigh Angle Initial\n",
      "Knee Ref Initial\n",
      "Ankle Ref Initial\n",
      "Load Initial\n",
      "Shank Ax Initial\n",
      "Shank Ay Initial\n",
      "Shank Az Initial\n",
      "Shank Gy Initial\n",
      "Shank Gz Initial\n",
      "Shank Gx Initial\n",
      "Thigh Ax Initial\n",
      "Thigh Ay Initial\n",
      "Thigh Az Initial\n",
      "Thigh Gy Initial\n",
      "Thigh Gz Initial\n",
      "Thigh Gx Initial\n",
      "Contra Shank Initial\n",
      "Contra Thigh Initial\n",
      "Knee Angle Final\n",
      "Knee Vel Final\n",
      "Knee Current Final\n",
      "Ankle Angle Final\n",
      "Ankle Vel Final\n",
      "Ankle Current Final\n",
      "VU Ax Final\n",
      "VU Ay Final\n",
      "VU Az Final\n",
      "VU Gx Final\n",
      "VU Gz Final\n",
      "VU Gy Final\n",
      "Shank Angle Final\n",
      "Thigh Angle Final\n",
      "Knee Ref Final\n",
      "Ankle Ref Final\n",
      "Load Final\n",
      "Shank Ax Final\n",
      "Shank Ay Final\n",
      "Shank Az Final\n",
      "Shank Gy Final\n",
      "Shank Gz Final\n",
      "Shank Gx Final\n",
      "Thigh Ax Final\n",
      "Thigh Ay Final\n",
      "Thigh Az Final\n",
      "Thigh Gy Final\n",
      "Thigh Gz Final\n",
      "Thigh Gx Final\n",
      "Contra Shank Final\n",
      "Contra Thigh Final\n",
      "Knee Angle Mean\n",
      "Knee Vel Mean\n",
      "Knee Current Mean\n",
      "Ankle Angle Mean\n",
      "Ankle Vel Mean\n",
      "Ankle Current Mean\n",
      "VU Ax Mean\n",
      "VU Ay Mean\n",
      "VU Az Mean\n",
      "VU Gx Mean\n",
      "VU Gz Mean\n",
      "VU Gy Mean\n",
      "Shank Angle Mean\n",
      "Thigh Angle Mean\n",
      "Knee Ref Mean\n",
      "Ankle Ref Mean\n",
      "Load Mean\n",
      "Shank Ax Mean\n",
      "Shank Ay Mean\n",
      "Shank Az Mean\n",
      "Shank Gy Mean\n",
      "Shank Gz Mean\n",
      "Shank Gx Mean\n",
      "Thigh Ax Mean\n",
      "Thigh Ay Mean\n",
      "Thigh Az Mean\n",
      "Thigh Gy Mean\n",
      "Thigh Gz Mean\n",
      "Thigh Gx Mean\n",
      "Contra Shank Mean\n",
      "Contra Thigh Mean\n",
      "Knee Angle SD\n",
      "Knee Vel SD\n",
      "Knee Current SD\n",
      "Ankle Angle SD\n",
      "Ankle Vel SD\n",
      "Ankle Current SD\n",
      "VU Ax SD\n",
      "VU Ay SD\n",
      "VU Az SD\n",
      "VU Gx SD\n",
      "VU Gz SD\n",
      "VU Gy SD\n",
      "Shank Angle SD\n",
      "Thigh Angle SD\n",
      "Knee Ref SD\n",
      "Ankle Ref SD\n",
      "Load SD\n",
      "Shank Ax SD\n",
      "Shank Ay SD\n",
      "Shank Az SD\n",
      "Shank Gy SD\n",
      "Shank Gz SD\n",
      "Shank Gx SD\n",
      "Thigh Ax SD\n",
      "Thigh Ay SD\n",
      "Thigh Az SD\n",
      "Thigh Gy SD\n",
      "Thigh Gz SD\n",
      "Thigh Gx SD\n",
      "Contra Shank SD\n",
      "Contra Thigh SD\n"
     ]
    }
   ],
   "source": [
    "vu_only = list(range(17))\n",
    "contra_1 = list(range(23)) + [29]\n",
    "contra_2 = list(range(31))\n",
    "chanuse = contra_2\n",
    "# chanuse = list(range(14)) + [16]\n",
    "\n",
    "USE_MECH_CHAN_NAME = [allcolheaders[chanuse[i]] for i in range(len(chanuse))]\n",
    "# USE_MECH_CHAN_NAME = allcolheaders[:17]\n",
    "# USE_MECH_CHAN_NAME = []\n",
    "\n",
    "USE_CHAN, USE_FEAT = select_chan(USE_MECH_CHAN_NAME,allcolheaders,allfeatnames)\n",
    "print('=====Using the following {} channels:====='.format(len(USE_CHAN)))\n",
    "for chan in USE_CHAN:\n",
    "    print(allcolheaders[chan])\n",
    "print()\n",
    "print('=====Using the following {} features:====='.format(len(USE_FEAT)))\n",
    "for feat in USE_FEAT:\n",
    "    print(allfeatnames[feat])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Event Channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chan = 15\n",
    "event = 'HC'\n",
    "\n",
    "traineventinds = [fileind for fileind in np.arange(len(alldict['Combined ' + event + ' File Index'])) if alldict['Combined ' + event + ' File Index'][fileind] in train_files]\n",
    "testeventinds = [fileind for fileind in np.arange(len(alldict['Combined ' + event + ' File Index'])) if alldict['Combined ' + event + ' File Index'][fileind] in test_files]\n",
    "\n",
    "leave, enter, steptype = unpack_trig(alldict['Combined ' + event + ' Triggers']) # Get the leaving and entering modes from the triggers\n",
    "\n",
    "pcolor = {1: 'k', 2: 'b', 3:'c', 4:'m', 5:'g', 6:'r'}\n",
    "titles = {1:'Stand', 2:'LW', 3:'SA', 4:'SD', 5:'RA', 6:'RD'}\n",
    "\n",
    "fig, ax = plt.subplots(nrows = 3, ncols = 2)\n",
    "fig.tight_layout()\n",
    "\n",
    "for i in np.unique(leave):\n",
    "    plt.subplot(3,2,i)\n",
    "    plt.title(event + ': ' + titles[i])    \n",
    "    for j in np.unique(enter[np.where(leave == i)]):\n",
    "        print('Leave: {} / Enter: {}'.format(i,j))\n",
    "        ms_inds = [ind for ind in np.arange(len(leave)) if leave[ind] == i and enter[ind] == j]\n",
    "        for k in ms_inds:\n",
    "            if alldict['Combined ' + event + ' Augmented'][k] == False:\n",
    "                plt.plot(alldict['Combined ' + event + ' Windows'][k,:,chan],color=pcolor[j],alpha=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA Visualize Event Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "event = 'MSW'\n",
    "\n",
    "traineventinds = [fileind for fileind in np.arange(len(alldict['Combined ' + event + ' File Index'])) if alldict['Combined ' + event + ' File Index'][fileind] in train_files]\n",
    "testeventinds = [fileind for fileind in np.arange(len(alldict['Combined ' + event + ' File Index'])) if alldict['Combined ' + event + ' File Index'][fileind] in test_files]\n",
    "\n",
    "leave, enter, steptype = unpack_trig(alldict['Combined ' + event + ' Triggers']) # Get the leaving and entering modes from the triggers\n",
    "\n",
    "# Fit scaler with training files\n",
    "trainevent_scaler = preprocessing.StandardScaler().fit(alldict['Combined ' + event + ' Features'][traineventinds][:,USE_FEAT])\n",
    "trainevent_norm = trainevent_scaler.transform(alldict['Combined ' + event + ' Features'][traineventinds][:,USE_FEAT])\n",
    "testevent_norm = trainevent_scaler.transform(alldict['Combined ' + event + ' Features'][testeventinds][:,USE_FEAT])\n",
    "\n",
    "# Fit PCA with training files\n",
    "trainevent_pca = PCA().fit(trainevent_norm)\n",
    "trainevent_dimred = trainevent_pca.transform(trainevent_norm)\n",
    "testevent_dimred = trainevent_pca.transform(testevent_norm)   \n",
    "\n",
    "pcolor = {1: 'k', 2: 'b', 3:'c', 4:'m', 5:'g', 6:'r'}\n",
    "    \n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "for wind in np.arange(0,np.shape(trainevent_dimred)[0]):\n",
    "    ax.scatter(trainevent_dimred[wind,0],trainevent_dimred[wind,1],trainevent_dimred[wind,2],color=pcolor[enter[traineventinds][wind]],alpha=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Mode Specific Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Re-organize data for mode-specific classifiers for each gait event (downsampling optional)\n",
    "allevents_ms = make_modespec(alldict, USE_CHAN, USE_FEAT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Mode Specific Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pcaprop = 50\n",
    "print('MS classification:')\n",
    "all_pred, all_gtruth, all_type, all_files = ms_classify_results(allevents_ms,pcaprop,train_files,test_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files with errors:\n",
      "circuit_even_121917_data_015_py.mat\n",
      "circuit_even_121917_data_023_py.mat\n",
      "circuit_even_121917_data_030_py.mat\n",
      "circuit_even_121917_data_050_py.mat\n",
      "circuit_odd_121917_data_007_py.mat\n",
      "circuit_odd_121917_data_025_py.mat\n",
      "circuit_odd_121917_data_027_py.mat\n",
      "ramp_121917_data_001_py.mat\n",
      "ramp_121917_data_005_py.mat\n",
      "stand_121917_data_006_py.mat\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(all_pred,all_gtruth)\n",
    "class_totals = cm.sum(axis=1)\n",
    "norm_cm = np.zeros((6,6))\n",
    "for i in range(6):\n",
    "    norm_cm[i,:] = cm[i,:]/class_totals[i]\n",
    "\n",
    "print(pd.DataFrame(100*norm_cm))\n",
    "\n",
    "t_step = np.where(all_type == 0)[0]\n",
    "ss_step = np.where(all_type == 1)[0]\n",
    "\n",
    "print('SS Accuracy: {}'.format(accuracy_score(all_pred[ss_step],all_gtruth[ss_step])))\n",
    "print('Trans Accuracy: {}'.format(accuracy_score(all_pred[t_step],all_gtruth[t_step])))\n",
    "print('Overall Accuracy: {}'.format(accuracy_score(all_pred,all_gtruth)))\n",
    "\n",
    "print('SS steps: {}'.format(len(ss_step)))\n",
    "print('T steps: {}'.format(len(t_step)))\n",
    "\n",
    "print('Files with errors:')\n",
    "for fnum in np.unique([all_files[i] for i in range(len(all_files)) if all_pred[i] != all_gtruth[i]]):\n",
    "    print(filekeys[fnum])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-gpu",
   "language": "python",
   "name": "tensorflow-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
