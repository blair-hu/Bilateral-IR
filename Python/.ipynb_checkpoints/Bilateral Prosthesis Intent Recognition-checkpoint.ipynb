{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "from scipy import stats, signal\n",
    "import scipy.io as sio\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import KFold, LeaveOneOut\n",
    "from sklearn import preprocessing, model_selection, metrics\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_vu_all_files(filedirectory, numfiles, delaybool):\n",
    "# Get data from .mat files\n",
    "# Assumes input data are in the format of time (rows) x channels (columns)\n",
    "    if len(numfiles) == 0:\n",
    "        numfiles_start = 0\n",
    "        numfiles_end = len(os.listdir(filedirectory)) # Use all files in directory\n",
    "        print('Loading {} processed data file(s) ...'.format(numfiles_end - numfiles_start + 1))     \n",
    "    else:\n",
    "        numfiles_start = numfiles[0]\n",
    "        numfiles_end = numfiles[1]\n",
    "        print('Loading {} processed data file(s) ...'.format(numfiles_end - numfiles_start + 1))      \n",
    "\n",
    "    filedict = {}\n",
    "    allindtrigsdict = {}\n",
    "        \n",
    "    for fileindex, filename in enumerate(sorted(os.listdir(filedirectory))):\n",
    "        if numfiles_start <= fileindex < numfiles_end:\n",
    "            print('=== {} ==='.format(filename))\n",
    "            mat_contents = sio.loadmat(filedirectory + '\\\\' + filename)\n",
    "            daqdata = mat_contents['python_input']\n",
    "            \n",
    "            if fileindex == numfiles_start:\n",
    "                chanlabels = mat_contents['chanlabels'].flatten()\n",
    "                allcolheaders = []\n",
    "                for i in np.arange(len(chanlabels)):\n",
    "                    allcolheaders.append(chanlabels[i][0])\n",
    "                        \n",
    "            # Get post-processed channel data\n",
    "            chans = daqdata[:,:31]\n",
    "            locomode = daqdata[:,31]\n",
    "            \n",
    "            if delaybool == True: # 3 frame (~90 ms) delay\n",
    "                use_trig = daqdata[:,33]\n",
    "                # Define valid triggers (5-digit)\n",
    "                hc_sm_trigs = [24210, 28210, 54510, 64610, 34310, 44410, 24510, 54210, 24610, 64210, 24410, 44210]\n",
    "                mst_sm_trigs = [21220, 51520, 61620, 31110, 41420, 41260]\n",
    "                to_sm_trigs = [22230, 26230, 52530, 62630, 41430, 42430, 22530, 52230, 22630, 62230, 22330, 21330, 41230, 42230]\n",
    "                msw_sm_trigs = [23240, 53540, 63640, 33390, 43440, 39340, 33240, 39280]\n",
    "                shc_sm_trigs = [13110, 13410]\n",
    "                sto_sm_trigs = [11130, 11330]\n",
    "            else: # No delay\n",
    "                use_trig = daqdata[:,32]\n",
    "                # Define valid triggers (4-digit)\n",
    "                hc_sm_trigs = [2421, 2821, 5451, 6461, 3431, 4441, 2451, 5421, 2461, 6421, 2441, 4421]\n",
    "                mst_sm_trigs = [2122, 5152, 6162, 3111, 4142, 4126]\n",
    "                to_sm_trigs = [2223, 2623, 5253, 6263, 4143, 4243, 2253, 5223, 2263, 6223, 2233, 2133, 4123, 4223]\n",
    "                msw_sm_trigs = [2324, 5354, 6364, 3339, 4344, 3934, 3324, 3928]\n",
    "                shc_sm_trigs = [1311, 1341]\n",
    "                sto_sm_trigs = [1113, 1133]\n",
    "\n",
    "            trig_diff = np.diff(use_trig)\n",
    "            trig_ind = np.where(trig_diff > 0)[0] + 1\n",
    "            \n",
    "            hc_ind, mst_ind, to_ind, msw_ind, shc_ind, sto_ind = [], [], [], [], [], []\n",
    "            hc_trig, mst_trig, to_trig, msw_trig, shc_trig, sto_trig = [], [], [], [], [], []\n",
    "            \n",
    "            for i in np.arange(len(trig_ind)):\n",
    "                if use_trig[trig_ind[i]] in hc_sm_trigs:\n",
    "                    hc_ind.append(int(trig_ind[i]))\n",
    "                    hc_trig.append(int(use_trig[trig_ind[i]]))\n",
    "                elif use_trig[trig_ind[i]] in mst_sm_trigs:\n",
    "                    mst_ind.append(int(trig_ind[i]))\n",
    "                    mst_trig.append(int(use_trig[trig_ind[i]]))\n",
    "                elif use_trig[trig_ind[i]] in to_sm_trigs:\n",
    "                    to_ind.append(int(trig_ind[i]))\n",
    "                    to_trig.append(int(use_trig[trig_ind[i]]))\n",
    "                elif use_trig[trig_ind[i]] in msw_sm_trigs:\n",
    "                    msw_ind.append(int(trig_ind[i]))\n",
    "                    msw_trig.append(int(use_trig[trig_ind[i]]))\n",
    "                elif use_trig[trig_ind[i]] in shc_sm_trigs:\n",
    "                    shc_ind.append(int(trig_ind[i]))\n",
    "                    shc_trig.append(int(use_trig[trig_ind[i]]))\n",
    "                elif use_trig[trig_ind[i]] in sto_sm_trigs:\n",
    "                    sto_ind.append(int(trig_ind[i]))\n",
    "                    sto_trig.append(int(use_trig[trig_ind[i]]))\n",
    "            \n",
    "            hc_ind, hc_trig = np.array(hc_ind), np.array(hc_trig)\n",
    "            mst_ind, mst_trig = np.array(mst_ind), np.array(mst_trig)\n",
    "            to_ind, to_trig = np.array(to_ind), np.array(to_trig)\n",
    "            msw_ind, msw_trig = np.array(msw_ind), np.array(msw_trig)\n",
    "            shc_ind, shc_trig = np.array(shc_ind), np.array(shc_trig)\n",
    "            sto_ind, sto_trig = np.array(sto_ind), np.array(sto_trig)\n",
    "            \n",
    "            # Save the post-processed data and indices/triggers into dictionaries\n",
    "            filedict[filename] = [chans,locomode,daqdata[:,30],daqdata[:,31]]\n",
    "            allindtrigsdict[filename] = {'HC': np.vstack((hc_ind,hc_trig)).T, \n",
    "                                         'MST': np.vstack((mst_ind,mst_trig)).T, \n",
    "                                         'TO': np.vstack((to_ind,to_trig)).T, \n",
    "                                         'MSW': np.vstack((msw_ind,msw_trig)).T,\n",
    "                                         'SHC': np.vstack((shc_ind,shc_trig)).T,\n",
    "                                         'STO': np.vstack((sto_ind,sto_trig)).T}\n",
    "            \n",
    "    # filedict has the filename as the key and the 2D DAQ data and targets as entries\n",
    "    # allcolheaders has the channel names\n",
    "    print('Finished!')\n",
    "    \n",
    "    return filedict, allindtrigsdict, allcolheaders\n",
    "\n",
    "\n",
    "def unpack_files(filedict, allindtrigsdict, filekeys, arginput, allcolheaders):    \n",
    "    # Unpack the list of arguments (arginput)\n",
    "    # Windowing parameters\n",
    "    \n",
    "    TRAIN_SIZE = arginput[0] # Sliding window length\n",
    "    \n",
    "    # Train/test split parameters\n",
    "    FOR_TEST = arginput[1]\n",
    "    TOTAL_FILES = len(filekeys)\n",
    "    if len(FOR_TEST) > 1 or type(FOR_TEST[0]) is str:\n",
    "        print('Generating train/test data from {} specified file(s):'.format(len(FOR_TEST)))\n",
    "        TEST_FILE_INDS = []\n",
    "        for testfileind, testfile in enumerate(FOR_TEST):\n",
    "            TEST_FILE_INDS.append(filekeys.index(testfile))\n",
    "        TEST_FILE_INDS = np.array(TEST_FILE_INDS)\n",
    "        TRAIN_FILE_INDS = np.setdiff1d(np.arange(TOTAL_FILES),TEST_FILE_INDS)\n",
    "    else:\n",
    "        print('Randomly select train/test data based on testing data proportion ({}%):'.format(int(100*FOR_TEST[0])))\n",
    "        TOTAL_FOLDS = int(1/(FOR_TEST[0]))\n",
    "        SPLIT_FILES = np.array_split(np.arange(TOTAL_FILES),TOTAL_FOLDS)\n",
    "        TEST_FILE_INDS = SPLIT_FILES[np.random.randint(0,TOTAL_FOLDS-1)]\n",
    "        TRAIN_FILE_INDS = np.setdiff1d(np.arange(TOTAL_FILES),TEST_FILE_INDS)\n",
    "    print()\n",
    "    \n",
    "    # Channels to use\n",
    "    CHAN_EMG = arginput[2]\n",
    "    \n",
    "    # Printing parameters\n",
    "    PRINT_SUMMARY = arginput[3]\n",
    "    \n",
    "    alldict = {}\n",
    "    alldict['Combined File Index'] = []\n",
    "        \n",
    "    for gaitevent in ['HC', 'MST', 'TO', 'MSW', 'SHC', 'STO']:\n",
    "        alldict['Combined ' + gaitevent + ' Windows'] = []\n",
    "        alldict['Combined ' + gaitevent + ' Features'] = []\n",
    "        alldict['Combined ' + gaitevent + ' Triggers'] = [] \n",
    "        alldict['Combined ' + gaitevent + ' File Index'] = []\n",
    "        \n",
    "    for fkeyind, fkey in enumerate(filekeys):\n",
    "        print('Preparing file {}: {} ...'.format(fkeyind+1,fkey))\n",
    "\n",
    "        data = filedict[fkey][0]\n",
    "        locomode = filedict[fkey][1]\n",
    "        indtrigs = allindtrigsdict[fkey]\n",
    "\n",
    "        alleventdict, featnames = event_windows(indtrigs, data, TRAIN_SIZE, CHAN_MECH, allcolheaders)\n",
    "        \n",
    "        alleventfeats = {'HC': alleventdict['HC'][0], \n",
    "                         'MST': alleventdict['MST'][0], \n",
    "                         'TO': alleventdict['TO'][0], \n",
    "                         'MSW': alleventdict['MSW'][0],\n",
    "                         'SHC': alleventdict['SHC'][0],\n",
    "                         'STO': alleventdict['STO'][0]}\n",
    "\n",
    "        for gaitevent in ['HC', 'MST', 'TO', 'MSW', 'SHC', 'STO']:\n",
    "            # Save the features extracted from each gait event\n",
    "            alldict['Combined ' + gaitevent + ' Features'].append(alleventfeats[gaitevent])\n",
    "            # Save the trigger associated with each gait event\n",
    "            alldict['Combined ' + gaitevent + ' Triggers'].append(alleventdict[gaitevent][1])\n",
    "            # Save file index associated with each gait event\n",
    "            alldict['Combined ' + gaitevent + ' File Index'].append(np.tile(fkeyind,len(alleventdict[gaitevent][1])))\n",
    "    print()\n",
    "    \n",
    "    for gaitevent in ['HC', 'MST', 'TO', 'MSW', 'SHC', 'STO']:\n",
    "        # Remove empty lists\n",
    "        alldict['Combined ' + gaitevent + ' Features'] = [features for features in alldict['Combined ' + gaitevent + ' Features'] if len(features) > 0]\n",
    "        alldict['Combined ' + gaitevent + ' Triggers'] = [triggers for triggers in alldict['Combined ' + gaitevent + ' Triggers'] if len(triggers) > 0]\n",
    "        alldict['Combined ' + gaitevent + ' File Index'] = [fileinds for fileinds in alldict['Combined ' + gaitevent + ' File Index'] if len(fileinds) > 0]\n",
    "        \n",
    "        alldict['Combined ' + gaitevent + ' Features'] = np.concatenate(alldict['Combined ' + gaitevent + ' Features'])\n",
    "        alldict['Combined ' + gaitevent + ' Triggers'] = np.concatenate(alldict['Combined ' + gaitevent + ' Triggers'])\n",
    "        alldict['Combined ' + gaitevent + ' File Index'] = np.concatenate(alldict['Combined ' + gaitevent + ' File Index'])\n",
    "        \n",
    "    if PRINT_SUMMARY:\n",
    "        print('Aggregated HC feature dimensions: {}'.format(alldict['Combined HC Features'].shape))\n",
    "        print('Aggregated MST feature dimensions: {}'.format(alldict['Combined MST Features'].shape))\n",
    "        print('Aggregated TO feature dimensions: {}'.format(alldict['Combined TO Features'].shape))\n",
    "        print('Aggregated MSW feature dimensions: {}'.format(alldict['Combined MSW Features'].shape))\n",
    "        print('Aggregated SHC feature dimensions: {}'.format(alldict['Combined SHC Features'].shape))\n",
    "        print('Aggregated STO feature dimensions: {}'.format(alldict['Combined STO Features'].shape))\n",
    "        print()\n",
    "        \n",
    "    return alldict, featnames, TRAIN_FILE_INDS, TEST_FILE_INDS\n",
    "\n",
    "\n",
    "def event_windows(indtrigs, data, train_window, chanmech, allcolheaders):\n",
    "# Get data from windows near gait events specified by indtrigs\n",
    "# Try different sized windows (pre_stance before stance and pre_swing before swing)\n",
    "# Try data augmentation (get aug_windows_per_event extra windows beginning aug_pre before to aug_post after the gait event)\n",
    "# scale = normalize data (boolean)\n",
    "\n",
    "    mechheaders = [allcolheaders[i] for i in chanmech]\n",
    "    eventkeys = list(indtrigs.keys())\n",
    "        \n",
    "    alleventdict = {}\n",
    "    for eventkeyind, eventkey in enumerate(eventkeys):\n",
    "        allfeats, trig_list = [], []\n",
    "        \n",
    "        inds = indtrigs[eventkey][:,0]\n",
    "        trigs = indtrigs[eventkey][:,1]\n",
    "        \n",
    "        # Remove triggers occurring less than 300 ms into DAQ file\n",
    "        keepinds = [inds > train_window]\n",
    "        inds = inds[keepinds]\n",
    "        trigs = trigs[keepinds] \n",
    "        \n",
    "        if len(inds) > 0:\n",
    "            for i in range(inds.shape[0]): # Iterates over indices\n",
    "                x_i = data[(inds[i]-train_window):inds[i],:]\n",
    "                y_i = trigs[i]\n",
    "\n",
    "                feats, featslabels = feats_from_window(x_i[-300:],chanmech,mechheaders)\n",
    "\n",
    "                allfeats.append(feats)\n",
    "                trig_list.append(y_i)\n",
    "\n",
    "            # Convert from list of arrays to 3D array        \n",
    "            alleventdict[eventkey] = [np.array(allfeats)] # Extracted features\n",
    "            alleventdict[eventkey].append(np.array(trig_list)) # Target\n",
    "        else:\n",
    "            print('No ' + eventkey + ' events...')\n",
    "            alleventdict[eventkey] = [[],[]]\n",
    "            \n",
    "    # alleventdict has the gait event (HC, TO) as the key and contains the 3d array of the extracted windows (300 ms for feature extraction and pre_stance, pre_swing) and targets\n",
    "    return alleventdict, featslabels\n",
    "\n",
    "\n",
    "def feats_from_window(window,chanmech,mechheaders):\n",
    "    if len(chanmech) > 0:\n",
    "        mechfeats, mechfeatsnames = getmechfeats(window,chanmech,mechheaders)\n",
    "    else:\n",
    "        mechfeats = []\n",
    "\n",
    "    feats = mechfeats.T    \n",
    "    featslabels = mechfeatsnames\n",
    "    \n",
    "    return feats, featslabels\n",
    "\n",
    "\n",
    "def getmechfeats(X,chanmech,mechheaders):    \n",
    "    X_mech = X[:,chanmech]\n",
    "\n",
    "    X_min = np.min(X_mech,axis=0)\n",
    "    X_max = np.max(X_mech,axis=0)\n",
    "    X_mean = np.mean(X_mech,axis=0)\n",
    "    X_std = np.std(X_mech,axis=0)\n",
    "    X_init = X_mech[0]\n",
    "    X_final = X_mech[-1]\n",
    "\n",
    "    mechfeats = np.array([X_min,X_max,X_init,X_final,X_mean,X_std]).flatten()\n",
    "    mechfeatsnames = []\n",
    "    \n",
    "    featsnames = [' Min',' Max',' Initial',' Final',' Mean',' SD']\n",
    "    \n",
    "    for names in featsnames:\n",
    "        for mechchan in mechheaders:\n",
    "            mechfeatsnames.append(mechchan + names)\n",
    "\n",
    "    return mechfeats, mechfeatsnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unpack_trig(triggers):\n",
    "# Get the first and third digits of the four-digit trigger in order to make mode-specific classifiers\n",
    "\n",
    "    trigstr = triggers.astype(int).astype(str)\n",
    "    leavemode = np.array([int(trig[0]) for trig in trigstr])\n",
    "    entermode = np.array([int(trig[2]) for trig in trigstr])\n",
    "    steptype = np.array([int(leavemode[i] == entermode[i]) for i in range(len(leavemode))])\n",
    "    \n",
    "    return leavemode, entermode, steptype\n",
    "    \n",
    "    \n",
    "def split_by_leavemode(feat_data, leavemode, useleave, entermode, steptype, fileind):\n",
    "# Organize data into dictionary with two-layer keys: gait event (RHC, RTO, LHC, LTO) and mode (LW, RA, RD, SA, SD)\n",
    "# DS_FACTOR = down-sampling factor (may improve NN performance by reducing dimensionality of raw data)\n",
    "# fileind keeps track of which file each gait event came from (in order to do leave-one-circuit-out cross validation)\n",
    "    # 1 = St, 2 = LW, 3 = SA, 4 = SD, 5 = RA, 6 = RD\n",
    "    \n",
    "    lmode_inds = [ind for ind in range(len(leavemode)) if leavemode[ind] in useleave] \n",
    "    feat_data_inds = feat_data[lmode_inds]\n",
    "    target_inds = entermode[lmode_inds]\n",
    "    file_inds = fileind[lmode_inds]\n",
    "    type_bin_inds = steptype[lmode_inds]\n",
    "    \n",
    "    # Merge RA targets with LW targets\n",
    "    target_inds[target_inds == 5] = 2\n",
    "\n",
    "    data_ms = [feat_data_inds, target_inds, file_inds, type_bin_inds]\n",
    "    \n",
    "    return data_ms \n",
    " \n",
    "\n",
    "def make_modespec(alldict,USE_FEAT):\n",
    "# Make modespec_dict to organize all events/modes\n",
    "    \n",
    "    hc_leave, hc_enter, hc_type = unpack_trig(alldict['Combined HC Triggers'])\n",
    "    mst_leave, mst_enter, mst_type = unpack_trig(alldict['Combined MST Triggers'])\n",
    "    to_leave, to_enter, to_type = unpack_trig(alldict['Combined TO Triggers'])\n",
    "    msw_leave, msw_enter, msw_type = unpack_trig(alldict['Combined MSW Triggers'])\n",
    "    shc_leave, shc_enter, shc_type = unpack_trig(alldict['Combined SHC Triggers'])\n",
    "    sto_leave, sto_enter, sto_type = unpack_trig(alldict['Combined STO Triggers'])\n",
    "    \n",
    "    hc_lw_ms = split_by_leavemode(alldict['Combined HC Features'][:,USE_FEAT], hc_leave, [2,5], hc_enter, hc_type, alldict['Combined HC File Index'])\n",
    "    hc_rd_ms = split_by_leavemode(alldict['Combined HC Features'][:,USE_FEAT], hc_leave, [6], hc_enter, hc_type, alldict['Combined HC File Index'])\n",
    "    hc_sd_ms = split_by_leavemode(alldict['Combined HC Features'][:,USE_FEAT], hc_leave, [4], hc_enter, hc_type, alldict['Combined HC File Index'])\n",
    "    \n",
    "    mst_sd_ms = split_by_leavemode(alldict['Combined MST Features'][:,USE_FEAT], mst_leave, [4], mst_enter, mst_type, alldict['Combined MST File Index'])\n",
    "    to_ms = split_by_leavemode(alldict['Combined TO Features'][:,USE_FEAT], to_leave, [2,4,5,6], to_enter, to_type, alldict['Combined TO File Index'])\n",
    "    msw_sa_ms = split_by_leavemode(alldict['Combined MSW Features'][:,USE_FEAT], msw_leave, [3], msw_enter, msw_type, alldict['Combined MSW File Index'])\n",
    "    shc_ms = split_by_leavemode(alldict['Combined SHC Features'][:,USE_FEAT], shc_leave, [1], shc_enter, shc_type, alldict['Combined SHC File Index'])\n",
    "    sto_ms = split_by_leavemode(alldict['Combined STO Features'][:,USE_FEAT], sto_leave, [1], sto_enter, sto_type, alldict['Combined STO File Index'])\n",
    "    \n",
    "    modespec_dict = {'HC_LW': hc_lw_ms, 'HC_RD': hc_rd_ms, 'HC_SD': hc_sd_ms,\n",
    "                     'MST_SD': mst_sd_ms, 'TO': to_ms, 'MSW_SA': msw_sa_ms, 'SHC': shc_ms, 'STO': sto_ms}\n",
    "    \n",
    "    return modespec_dict    \n",
    "\n",
    "\n",
    "def lda_classify(train_in,train_targ,test_in,test_targ,pca_proportion):\n",
    "    lda = LinearDiscriminantAnalysis()\n",
    "    \n",
    "    train_in_scaler = preprocessing.StandardScaler().fit(train_in)\n",
    "    train_in_scaled = train_in_scaler.transform(train_in)\n",
    "    test_in_scaled = train_in_scaler.transform(test_in)\n",
    "\n",
    "    train_in_scaler_pca = PCA().fit(train_in_scaled)\n",
    "    train_in_scaled_pca_xfm = train_in_scaler_pca.transform(train_in_scaled)\n",
    "    test_in_scaled_pca_xfm = train_in_scaler_pca.transform(test_in_scaled)\n",
    "\n",
    "    pcaexplainedvar = np.cumsum(train_in_scaler_pca.explained_variance_ratio_)\n",
    "    if pca_proportion < 1:\n",
    "        pcanumcomps = min(min(np.where(pcaexplainedvar > pca_proportion))) + 1\n",
    "    else:\n",
    "        pcanumcomps = pca_proportion\n",
    "\n",
    "    unique_modes = np.unique(train_targ)\n",
    "    lda.set_params(priors = np.ones(len(unique_modes))/len(unique_modes))\n",
    "\n",
    "    test_pred = lda.fit(train_in_scaled_pca_xfm[:,0:pcanumcomps],train_targ).predict(test_in_scaled_pca_xfm[:,0:pcanumcomps])\n",
    "    test_pred_prob = lda.fit(train_in_scaled_pca_xfm[:,0:pcanumcomps],train_targ).predict_proba(test_in_scaled_pca_xfm[:,0:pcanumcomps])\n",
    "    test_gtruth = test_targ\n",
    "    \n",
    "    return test_pred, test_gtruth, test_pred_prob\n",
    "\n",
    "    \n",
    "def ms_classify_results(msdict,pcaprop,trainfiles,testfiles):\n",
    "    ohe = preprocessing.OneHotEncoder()\n",
    "    lda = LinearDiscriminantAnalysis()\n",
    "    kf = KFold(n_splits=10, shuffle=True)\n",
    "    loo = LeaveOneOut()\n",
    "    \n",
    "    # Define histogram labels\n",
    "    gaitmodes = ['St','LW','SA','SD','RA','RD']\n",
    "    \n",
    "    all_pred, all_gtruth, all_type, all_files = [], [], [], []\n",
    "    \n",
    "    for classifiers in list(allevents_ms.keys()):\n",
    "        print('Classifier: {}'.format(classifiers))\n",
    "        \n",
    "        event_mode_features = msdict[classifiers][0]\n",
    "        event_mode_targets = msdict[classifiers][1]\n",
    "        event_mode_files = msdict[classifiers][2]\n",
    "        event_mode_typebin = msdict[classifiers][3]\n",
    "    \n",
    "        print('Unique: {}'.format(np.unique(event_mode_targets)))\n",
    "        print(gaitmodes)\n",
    "        print(np.histogram(event_mode_targets,bins=[0.5, 1.5, 2.5, 3.5, 4.5, 5.5, 6.5])[0])  \n",
    "    \n",
    "        event_mode_pred = np.zeros((len(event_mode_targets),1))\n",
    "        \n",
    "        if len(trainfiles) > 0 and len(testfiles) > 0:\n",
    "            train_inds = [i for i in range(len(event_mode_files)) if event_mode_files[i] in trainfiles]\n",
    "            test_inds = [i for i in range(len(event_mode_files)) if event_mode_files[i] in testfiles]\n",
    "            pred, gtruth, pred_prob = lda_classify(event_mode_features[train_inds],event_mode_targets[train_inds],event_mode_features[test_inds],event_mode_targets[test_inds],pcaprop)           \n",
    "            event_mode_pred[test_inds] = pred\n",
    "        else:\n",
    "            for train_inds, test_inds in loo.split(event_mode_features,event_mode_targets):\n",
    "                pred, gtruth, pred_prob = lda_classify(event_mode_features[train_inds],event_mode_targets[train_inds],event_mode_features[test_inds],event_mode_targets[test_inds],pcaprop)           \n",
    "                event_mode_pred[test_inds] = pred\n",
    "\n",
    "        print('LOO Accuracy: {}'.format(accuracy_score(event_mode_pred,event_mode_targets)))\n",
    "        print()\n",
    "        \n",
    "        all_pred.append(event_mode_pred)\n",
    "        all_gtruth.append(event_mode_targets)\n",
    "        all_type.append(event_mode_typebin)\n",
    "        all_files.append(event_mode_files)\n",
    "    \n",
    "    all_pred = np.concatenate(all_pred)\n",
    "    all_gtruth = np.concatenate(all_gtruth)\n",
    "    all_type = np.concatenate(all_type)\n",
    "    all_files = np.concatenate(all_files)\n",
    "    \n",
    "    return all_pred, all_gtruth, all_type, all_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def select_chan(USE_MECH_CHAN_NAME,allcolheaders,allfeatnames):\n",
    "    for featind, featname in enumerate(allfeatnames):\n",
    "        if '_' in featname:\n",
    "            allfeatnames[featind] = featname.replace('_',' ')\n",
    "    \n",
    "    for colind, colheader in enumerate(allcolheaders):\n",
    "        if '_' in colheader:\n",
    "            allcolheaders[colind] = colheader.replace('_',' ')\n",
    "    \n",
    "    if len(USE_MECH_CHAN_NAME) > 0:\n",
    "        USE_MECH_CHAN = []\n",
    "        USE_MECH_FEAT = []\n",
    "        for channameind, channame in enumerate(USE_MECH_CHAN_NAME): \n",
    "            MECH_CHAN_IND = [allcolheaders.index(colname) for colname in allcolheaders if channame in colname]\n",
    "            MECH_FEAT_IND = [allfeatnames.index(colname) for colname in allfeatnames if channame in colname]\n",
    "            USE_MECH_CHAN.append(MECH_CHAN_IND)\n",
    "            USE_MECH_FEAT.append(MECH_FEAT_IND)\n",
    "        \n",
    "        USE_MECH_CHAN = np.array(sorted(sum(USE_MECH_CHAN,[])),dtype=int)\n",
    "        USE_MECH_FEAT = np.array(sorted(sum(USE_MECH_FEAT,[])),dtype=int)\n",
    "\n",
    "        USE_CHAN = USE_MECH_CHAN\n",
    "        USE_FEAT = USE_MECH_FEAT\n",
    "    else:\n",
    "        USE_CHAN = np.arange(len(allcolheaders))\n",
    "        USE_FEAT = np.arange(len(allfeatnames))\n",
    "    \n",
    "    return np.unique(USE_CHAN), np.unique(USE_FEAT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and organize raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 129 processed data file(s) ...\n",
      "=== circuit_even_121917_data_001_py.mat ===\n",
      "=== circuit_even_121917_data_003_py.mat ===\n",
      "=== circuit_even_121917_data_004_py.mat ===\n",
      "=== circuit_even_121917_data_007_py.mat ===\n",
      "=== circuit_even_121917_data_009_py.mat ===\n",
      "=== circuit_even_121917_data_010_py.mat ===\n",
      "=== circuit_even_121917_data_012_py.mat ===\n",
      "=== circuit_even_121917_data_013_py.mat ===\n",
      "=== circuit_even_121917_data_015_py.mat ===\n",
      "=== circuit_even_121917_data_017_py.mat ===\n",
      "=== circuit_even_121917_data_020_py.mat ===\n",
      "=== circuit_even_121917_data_022_py.mat ===\n",
      "=== circuit_even_121917_data_023_py.mat ===\n",
      "=== circuit_even_121917_data_025_py.mat ===\n",
      "=== circuit_even_121917_data_026_py.mat ===\n",
      "=== circuit_even_121917_data_028_py.mat ===\n",
      "=== circuit_even_121917_data_030_py.mat ===\n",
      "=== circuit_even_121917_data_032_py.mat ===\n",
      "=== circuit_even_121917_data_033_py.mat ===\n",
      "=== circuit_even_121917_data_035_py.mat ===\n",
      "=== circuit_even_121917_data_036_py.mat ===\n",
      "=== circuit_even_121917_data_038_py.mat ===\n",
      "=== circuit_even_121917_data_039_py.mat ===\n",
      "=== circuit_even_121917_data_041_py.mat ===\n",
      "=== circuit_even_121917_data_042_py.mat ===\n",
      "=== circuit_even_121917_data_044_py.mat ===\n",
      "=== circuit_even_121917_data_045_py.mat ===\n",
      "=== circuit_even_121917_data_047_py.mat ===\n",
      "=== circuit_even_121917_data_048_py.mat ===\n",
      "=== circuit_even_121917_data_050_py.mat ===\n",
      "=== circuit_even_121917_data_051_py.mat ===\n",
      "=== circuit_even_121917_data_053_py.mat ===\n",
      "=== circuit_even_121917_data_054_py.mat ===\n",
      "=== circuit_even_121917_data_056_py.mat ===\n",
      "=== circuit_even_121917_data_057_py.mat ===\n",
      "=== circuit_even_121917_data_059_py.mat ===\n",
      "=== circuit_even_121917_data_060_py.mat ===\n",
      "=== circuit_even_121917_data_062_py.mat ===\n",
      "=== circuit_odd_121917_data_001_py.mat ===\n",
      "=== circuit_odd_121917_data_002_py.mat ===\n",
      "=== circuit_odd_121917_data_003_py.mat ===\n",
      "=== circuit_odd_121917_data_004_py.mat ===\n",
      "=== circuit_odd_121917_data_005_py.mat ===\n",
      "=== circuit_odd_121917_data_006_py.mat ===\n",
      "=== circuit_odd_121917_data_007_py.mat ===\n",
      "=== circuit_odd_121917_data_008_py.mat ===\n",
      "=== circuit_odd_121917_data_009_py.mat ===\n",
      "=== circuit_odd_121917_data_010_py.mat ===\n",
      "=== circuit_odd_121917_data_011_py.mat ===\n",
      "=== circuit_odd_121917_data_012_py.mat ===\n",
      "=== circuit_odd_121917_data_014_py.mat ===\n",
      "=== circuit_odd_121917_data_015_py.mat ===\n",
      "=== circuit_odd_121917_data_016_py.mat ===\n",
      "=== circuit_odd_121917_data_017_py.mat ===\n",
      "=== circuit_odd_121917_data_019_py.mat ===\n",
      "=== circuit_odd_121917_data_020_py.mat ===\n",
      "=== circuit_odd_121917_data_021_py.mat ===\n",
      "=== circuit_odd_121917_data_023_py.mat ===\n",
      "=== circuit_odd_121917_data_024_py.mat ===\n",
      "=== circuit_odd_121917_data_025_py.mat ===\n",
      "=== circuit_odd_121917_data_027_py.mat ===\n",
      "=== circuit_odd_121917_data_028_py.mat ===\n",
      "=== circuit_odd_121917_data_029_py.mat ===\n",
      "=== circuit_odd_121917_data_030_py.mat ===\n",
      "=== circuit_odd_121917_data_031_py.mat ===\n",
      "=== circuit_odd_RIC_121917_data_001_py.mat ===\n",
      "=== circuit_odd_RIC_121917_data_002_py.mat ===\n",
      "=== circuit_odd_RIC_121917_data_003_py.mat ===\n",
      "=== circuit_odd_RIC_121917_data_004_py.mat ===\n",
      "=== circuit_odd_RIC_121917_data_005_py.mat ===\n",
      "=== circuit_odd_RIC_121917_data_006_py.mat ===\n",
      "=== circuit_odd_RIC_121917_data_007_py.mat ===\n",
      "=== circuit_odd_RIC_121917_data_008_py.mat ===\n",
      "=== circuit_odd_RIC_121917_data_009_py.mat ===\n",
      "=== circuit_odd_RIC_121917_data_010_py.mat ===\n",
      "=== circuit_odd_RIC_121917_data_011_py.mat ===\n",
      "=== circuit_odd_RIC_121917_data_012_py.mat ===\n",
      "=== circuit_odd_RIC_121917_data_013_py.mat ===\n",
      "=== circuit_odd_RIC_121917_data_014_py.mat ===\n",
      "=== circuit_odd_RIC_121917_data_015_py.mat ===\n",
      "=== circuit_odd_RIC_121917_data_016_py.mat ===\n",
      "=== circuit_odd_RIC_121917_data_017_py.mat ===\n",
      "=== circuit_odd_RIC_121917_data_018_py.mat ===\n",
      "=== circuit_odd_RIC_121917_data_019_py.mat ===\n",
      "=== ramp_121917_data_001_py.mat ===\n",
      "=== ramp_121917_data_002_py.mat ===\n",
      "=== ramp_121917_data_003_py.mat ===\n",
      "=== ramp_121917_data_004_py.mat ===\n",
      "=== ramp_121917_data_005_py.mat ===\n",
      "=== ramp_121917_data_006_py.mat ===\n",
      "=== ramp_121917_data_008_py.mat ===\n",
      "=== ramp_121917_data_009_py.mat ===\n",
      "=== ramp_121917_data_010_py.mat ===\n",
      "=== stair_stand_121917_data_001_py.mat ===\n",
      "=== stair_stand_121917_data_002_py.mat ===\n",
      "=== stair_stand_121917_data_003_py.mat ===\n",
      "=== stair_stand_121917_data_004_py.mat ===\n",
      "=== stair_stand_121917_data_005_py.mat ===\n",
      "=== stair_stand_121917_data_006_py.mat ===\n",
      "=== stair_stand_121917_data_007_py.mat ===\n",
      "=== stair_stand_121917_data_008_py.mat ===\n",
      "=== stair_stand_121917_data_009_py.mat ===\n",
      "=== stair_stand_121917_data_010_py.mat ===\n",
      "=== stand_121917_data_001_py.mat ===\n",
      "=== stand_121917_data_002_py.mat ===\n",
      "=== stand_121917_data_003_py.mat ===\n",
      "=== stand_121917_data_004_py.mat ===\n",
      "=== stand_121917_data_005_py.mat ===\n",
      "=== stand_121917_data_006_py.mat ===\n",
      "=== stand_121917_data_007_py.mat ===\n",
      "=== stand_121917_data_008_py.mat ===\n",
      "=== stand_121917_data_009_py.mat ===\n",
      "=== stand_121917_data_010_py.mat ===\n",
      "=== walk_121917_data_001_py.mat ===\n",
      "=== walk_121917_data_002_py.mat ===\n",
      "=== walk_121917_data_003_py.mat ===\n",
      "=== walk_121917_data_004_py.mat ===\n",
      "=== walk_121917_data_005_py.mat ===\n",
      "=== walk_121917_data_006_py.mat ===\n",
      "=== walk_121917_data_007_py.mat ===\n",
      "=== walk_121917_data_008_py.mat ===\n",
      "=== walk_121917_data_009_py.mat ===\n",
      "=== walk_121917_data_010_py.mat ===\n",
      "=== walk_121917_data_011_py.mat ===\n",
      "=== walk_121917_data_012_py.mat ===\n",
      "=== walk_121917_data_013_py.mat ===\n",
      "=== walk_121917_data_014_py.mat ===\n",
      "=== walk_121917_data_015_py.mat ===\n",
      "Finished!\n"
     ]
    }
   ],
   "source": [
    "# Just get the filenames from the filedir\n",
    "FILEDIR = u'C:\\\\Users\\\\bhu\\\\Git\\\\Bilateral_Intent_Recognition\\\\Data\\\\Prosthesis'\n",
    "# FILEDIR = u'Z:\\\\Lab Member Folders\\\\Blair Hu\\\\Contralateral Prosthesis Control 2017\\\\Pilot Data\\\\PYTHON'\n",
    "NUM_FILES = [] # if empty list, load all files; otherwise [X] means load X files\n",
    "DELAYED = False\n",
    "filedict, allindtrigsdict, allcolheaders = load_vu_all_files(FILEDIR, NUM_FILES, DELAYED)\n",
    "filekeys = sorted(list(filedict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomly select train/test data based on testing data proportion (5%):\n",
      "\n",
      "Preparing file 1: circuit_even_121917_data_001_py.mat ...\n",
      "Preparing file 2: circuit_even_121917_data_003_py.mat ...\n",
      "Preparing file 3: circuit_even_121917_data_004_py.mat ...\n",
      "Preparing file 4: circuit_even_121917_data_007_py.mat ...\n",
      "Preparing file 5: circuit_even_121917_data_009_py.mat ...\n",
      "Preparing file 6: circuit_even_121917_data_010_py.mat ...\n",
      "Preparing file 7: circuit_even_121917_data_012_py.mat ...\n",
      "Preparing file 8: circuit_even_121917_data_013_py.mat ...\n",
      "Preparing file 9: circuit_even_121917_data_015_py.mat ...\n",
      "Preparing file 10: circuit_even_121917_data_017_py.mat ...\n",
      "No STO events...\n",
      "No SHC events...\n",
      "Preparing file 11: circuit_even_121917_data_020_py.mat ...\n",
      "Preparing file 12: circuit_even_121917_data_022_py.mat ...\n",
      "Preparing file 13: circuit_even_121917_data_023_py.mat ...\n",
      "Preparing file 14: circuit_even_121917_data_025_py.mat ...\n",
      "Preparing file 15: circuit_even_121917_data_026_py.mat ...\n",
      "Preparing file 16: circuit_even_121917_data_028_py.mat ...\n",
      "Preparing file 17: circuit_even_121917_data_030_py.mat ...\n",
      "No STO events...\n",
      "No SHC events...\n",
      "Preparing file 18: circuit_even_121917_data_032_py.mat ...\n",
      "Preparing file 19: circuit_even_121917_data_033_py.mat ...\n",
      "Preparing file 20: circuit_even_121917_data_035_py.mat ...\n",
      "No STO events...\n",
      "Preparing file 21: circuit_even_121917_data_036_py.mat ...\n",
      "Preparing file 22: circuit_even_121917_data_038_py.mat ...\n",
      "No STO events...\n",
      "Preparing file 23: circuit_even_121917_data_039_py.mat ...\n",
      "Preparing file 24: circuit_even_121917_data_041_py.mat ...\n",
      "Preparing file 25: circuit_even_121917_data_042_py.mat ...\n",
      "Preparing file 26: circuit_even_121917_data_044_py.mat ...\n",
      "Preparing file 27: circuit_even_121917_data_045_py.mat ...\n",
      "Preparing file 28: circuit_even_121917_data_047_py.mat ...\n",
      "No STO events...\n",
      "Preparing file 29: circuit_even_121917_data_048_py.mat ...\n",
      "Preparing file 30: circuit_even_121917_data_050_py.mat ...\n",
      "Preparing file 31: circuit_even_121917_data_051_py.mat ...\n",
      "Preparing file 32: circuit_even_121917_data_053_py.mat ...\n",
      "Preparing file 33: circuit_even_121917_data_054_py.mat ...\n",
      "Preparing file 34: circuit_even_121917_data_056_py.mat ...\n",
      "No STO events...\n",
      "Preparing file 35: circuit_even_121917_data_057_py.mat ...\n",
      "Preparing file 36: circuit_even_121917_data_059_py.mat ...\n",
      "Preparing file 37: circuit_even_121917_data_060_py.mat ...\n",
      "No SHC events...\n",
      "Preparing file 38: circuit_even_121917_data_062_py.mat ...\n",
      "Preparing file 39: circuit_odd_121917_data_001_py.mat ...\n",
      "Preparing file 40: circuit_odd_121917_data_002_py.mat ...\n",
      "Preparing file 41: circuit_odd_121917_data_003_py.mat ...\n",
      "Preparing file 42: circuit_odd_121917_data_004_py.mat ...\n",
      "Preparing file 43: circuit_odd_121917_data_005_py.mat ...\n",
      "Preparing file 44: circuit_odd_121917_data_006_py.mat ...\n",
      "Preparing file 45: circuit_odd_121917_data_007_py.mat ...\n",
      "No STO events...\n",
      "Preparing file 46: circuit_odd_121917_data_008_py.mat ...\n",
      "Preparing file 47: circuit_odd_121917_data_009_py.mat ...\n",
      "No TO events...\n",
      "Preparing file 48: circuit_odd_121917_data_010_py.mat ...\n",
      "Preparing file 49: circuit_odd_121917_data_011_py.mat ...\n",
      "Preparing file 50: circuit_odd_121917_data_012_py.mat ...\n",
      "No STO events...\n",
      "Preparing file 51: circuit_odd_121917_data_014_py.mat ...\n",
      "Preparing file 52: circuit_odd_121917_data_015_py.mat ...\n",
      "Preparing file 53: circuit_odd_121917_data_016_py.mat ...\n",
      "Preparing file 54: circuit_odd_121917_data_017_py.mat ...\n",
      "Preparing file 55: circuit_odd_121917_data_019_py.mat ...\n",
      "Preparing file 56: circuit_odd_121917_data_020_py.mat ...\n",
      "Preparing file 57: circuit_odd_121917_data_021_py.mat ...\n",
      "Preparing file 58: circuit_odd_121917_data_023_py.mat ...\n",
      "Preparing file 59: circuit_odd_121917_data_024_py.mat ...\n",
      "Preparing file 60: circuit_odd_121917_data_025_py.mat ...\n",
      "Preparing file 61: circuit_odd_121917_data_027_py.mat ...\n",
      "Preparing file 62: circuit_odd_121917_data_028_py.mat ...\n",
      "Preparing file 63: circuit_odd_121917_data_029_py.mat ...\n",
      "Preparing file 64: circuit_odd_121917_data_030_py.mat ...\n",
      "Preparing file 65: circuit_odd_121917_data_031_py.mat ...\n",
      "Preparing file 66: circuit_odd_RIC_121917_data_001_py.mat ...\n",
      "Preparing file 67: circuit_odd_RIC_121917_data_002_py.mat ...\n",
      "Preparing file 68: circuit_odd_RIC_121917_data_003_py.mat ...\n",
      "Preparing file 69: circuit_odd_RIC_121917_data_004_py.mat ...\n",
      "Preparing file 70: circuit_odd_RIC_121917_data_005_py.mat ...\n",
      "Preparing file 71: circuit_odd_RIC_121917_data_006_py.mat ...\n",
      "Preparing file 72: circuit_odd_RIC_121917_data_007_py.mat ...\n",
      "Preparing file 73: circuit_odd_RIC_121917_data_008_py.mat ...\n",
      "Preparing file 74: circuit_odd_RIC_121917_data_009_py.mat ...\n",
      "No SHC events...\n",
      "Preparing file 75: circuit_odd_RIC_121917_data_010_py.mat ...\n",
      "Preparing file 76: circuit_odd_RIC_121917_data_011_py.mat ...\n",
      "Preparing file 77: circuit_odd_RIC_121917_data_012_py.mat ...\n",
      "No STO events...\n",
      "Preparing file 78: circuit_odd_RIC_121917_data_013_py.mat ...\n",
      "Preparing file 79: circuit_odd_RIC_121917_data_014_py.mat ...\n",
      "Preparing file 80: circuit_odd_RIC_121917_data_015_py.mat ...\n",
      "Preparing file 81: circuit_odd_RIC_121917_data_016_py.mat ...\n",
      "Preparing file 82: circuit_odd_RIC_121917_data_017_py.mat ...\n",
      "No STO events...\n",
      "No SHC events...\n",
      "Preparing file 83: circuit_odd_RIC_121917_data_018_py.mat ...\n",
      "Preparing file 84: circuit_odd_RIC_121917_data_019_py.mat ...\n",
      "Preparing file 85: ramp_121917_data_001_py.mat ...\n",
      "Preparing file 86: ramp_121917_data_002_py.mat ...\n",
      "Preparing file 87: ramp_121917_data_003_py.mat ...\n",
      "Preparing file 88: ramp_121917_data_004_py.mat ...\n",
      "Preparing file 89: ramp_121917_data_005_py.mat ...\n",
      "Preparing file 90: ramp_121917_data_006_py.mat ...\n",
      "Preparing file 91: ramp_121917_data_008_py.mat ...\n",
      "Preparing file 92: ramp_121917_data_009_py.mat ...\n",
      "Preparing file 93: ramp_121917_data_010_py.mat ...\n",
      "Preparing file 94: stair_stand_121917_data_001_py.mat ...\n",
      "Preparing file 95: stair_stand_121917_data_002_py.mat ...\n",
      "Preparing file 96: stair_stand_121917_data_003_py.mat ...\n",
      "No TO events...\n",
      "Preparing file 97: stair_stand_121917_data_004_py.mat ...\n",
      "Preparing file 98: stair_stand_121917_data_005_py.mat ...\n",
      "No TO events...\n",
      "Preparing file 99: stair_stand_121917_data_006_py.mat ...\n",
      "Preparing file 100: stair_stand_121917_data_007_py.mat ...\n",
      "No TO events...\n",
      "Preparing file 101: stair_stand_121917_data_008_py.mat ...\n",
      "Preparing file 102: stair_stand_121917_data_009_py.mat ...\n",
      "No TO events...\n",
      "Preparing file 103: stair_stand_121917_data_010_py.mat ...\n",
      "Preparing file 104: stand_121917_data_001_py.mat ...\n",
      "Preparing file 105: stand_121917_data_002_py.mat ...\n",
      "No MST events...\n",
      "Preparing file 106: stand_121917_data_003_py.mat ...\n",
      "Preparing file 107: stand_121917_data_004_py.mat ...\n",
      "No TO events...\n",
      "No MST events...\n",
      "No MSW events...\n",
      "No HC events...\n",
      "Preparing file 108: stand_121917_data_005_py.mat ...\n",
      "No MST events...\n",
      "Preparing file 109: stand_121917_data_006_py.mat ...\n",
      "No MST events...\n",
      "Preparing file 110: stand_121917_data_007_py.mat ...\n",
      "No TO events...\n",
      "No MST events...\n",
      "No MSW events...\n",
      "No HC events...\n",
      "Preparing file 111: stand_121917_data_008_py.mat ...\n",
      "No TO events...\n",
      "No MST events...\n",
      "No MSW events...\n",
      "No HC events...\n",
      "Preparing file 112: stand_121917_data_009_py.mat ...\n",
      "No TO events...\n",
      "No MST events...\n",
      "No MSW events...\n",
      "No HC events...\n",
      "Preparing file 113: stand_121917_data_010_py.mat ...\n",
      "No TO events...\n",
      "No MST events...\n",
      "No MSW events...\n",
      "No HC events...\n",
      "Preparing file 114: walk_121917_data_001_py.mat ...\n",
      "Preparing file 115: walk_121917_data_002_py.mat ...\n",
      "Preparing file 116: walk_121917_data_003_py.mat ...\n",
      "Preparing file 117: walk_121917_data_004_py.mat ...\n",
      "Preparing file 118: walk_121917_data_005_py.mat ...\n",
      "No STO events...\n",
      "Preparing file 119: walk_121917_data_006_py.mat ...\n",
      "Preparing file 120: walk_121917_data_007_py.mat ...\n",
      "Preparing file 121: walk_121917_data_008_py.mat ...\n",
      "No MST events...\n",
      "Preparing file 122: walk_121917_data_009_py.mat ...\n",
      "No TO events...\n",
      "No MST events...\n",
      "No MSW events...\n",
      "No HC events...\n",
      "Preparing file 123: walk_121917_data_010_py.mat ...\n",
      "Preparing file 124: walk_121917_data_011_py.mat ...\n",
      "Preparing file 125: walk_121917_data_012_py.mat ...\n",
      "Preparing file 126: walk_121917_data_013_py.mat ...\n",
      "Preparing file 127: walk_121917_data_014_py.mat ...\n",
      "Preparing file 128: walk_121917_data_015_py.mat ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Aggregated HC feature dimensions: (912, 186)\n",
      "Aggregated MST feature dimensions: (743, 186)\n",
      "Aggregated TO feature dimensions: (821, 186)\n",
      "Aggregated MSW feature dimensions: (1002, 186)\n",
      "Aggregated SHC feature dimensions: (804, 186)\n",
      "Aggregated STO feature dimensions: (738, 186)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "TRAIN_SIZE = 300 # Training windows are 300 time-steps long for sliding windows\n",
    "# FOR_TEST = ['circuit_even_data_020.mat','circuit_odd_data_019.mat'] # Filenames to use for testing\n",
    "FOR_TEST = [0.05]\n",
    "CHAN_MECH = np.arange(31)\n",
    "PRINT_SUMMARY = True # Print dimensions of aggregated data from all files\n",
    "\n",
    "arginput = [TRAIN_SIZE,FOR_TEST,CHAN_MECH,PRINT_SUMMARY]\n",
    "\n",
    "alldict, allfeatnames, _, _ = unpack_files(filedict,allindtrigsdict,filekeys,arginput,allcolheaders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====Using the following 17 channels:=====\n",
      "Knee Angle\n",
      "Knee Vel\n",
      "Knee Current\n",
      "Ankle Angle\n",
      "Ankle Vel\n",
      "Ankle Current\n",
      "VU Ax\n",
      "VU Ay\n",
      "VU Az\n",
      "VU Gx\n",
      "VU Gz\n",
      "VU Gy\n",
      "Shank Angle\n",
      "Thigh Angle\n",
      "Knee Ref\n",
      "Ankle Ref\n",
      "Load\n",
      "\n",
      "=====Using the following 102 features:=====\n",
      "Knee Angle Min\n",
      "Knee Vel Min\n",
      "Knee Current Min\n",
      "Ankle Angle Min\n",
      "Ankle Vel Min\n",
      "Ankle Current Min\n",
      "VU Ax Min\n",
      "VU Ay Min\n",
      "VU Az Min\n",
      "VU Gx Min\n",
      "VU Gz Min\n",
      "VU Gy Min\n",
      "Shank Angle Min\n",
      "Thigh Angle Min\n",
      "Knee Ref Min\n",
      "Ankle Ref Min\n",
      "Load Min\n",
      "Knee Angle Max\n",
      "Knee Vel Max\n",
      "Knee Current Max\n",
      "Ankle Angle Max\n",
      "Ankle Vel Max\n",
      "Ankle Current Max\n",
      "VU Ax Max\n",
      "VU Ay Max\n",
      "VU Az Max\n",
      "VU Gx Max\n",
      "VU Gz Max\n",
      "VU Gy Max\n",
      "Shank Angle Max\n",
      "Thigh Angle Max\n",
      "Knee Ref Max\n",
      "Ankle Ref Max\n",
      "Load Max\n",
      "Knee Angle Initial\n",
      "Knee Vel Initial\n",
      "Knee Current Initial\n",
      "Ankle Angle Initial\n",
      "Ankle Vel Initial\n",
      "Ankle Current Initial\n",
      "VU Ax Initial\n",
      "VU Ay Initial\n",
      "VU Az Initial\n",
      "VU Gx Initial\n",
      "VU Gz Initial\n",
      "VU Gy Initial\n",
      "Shank Angle Initial\n",
      "Thigh Angle Initial\n",
      "Knee Ref Initial\n",
      "Ankle Ref Initial\n",
      "Load Initial\n",
      "Knee Angle Final\n",
      "Knee Vel Final\n",
      "Knee Current Final\n",
      "Ankle Angle Final\n",
      "Ankle Vel Final\n",
      "Ankle Current Final\n",
      "VU Ax Final\n",
      "VU Ay Final\n",
      "VU Az Final\n",
      "VU Gx Final\n",
      "VU Gz Final\n",
      "VU Gy Final\n",
      "Shank Angle Final\n",
      "Thigh Angle Final\n",
      "Knee Ref Final\n",
      "Ankle Ref Final\n",
      "Load Final\n",
      "Knee Angle Mean\n",
      "Knee Vel Mean\n",
      "Knee Current Mean\n",
      "Ankle Angle Mean\n",
      "Ankle Vel Mean\n",
      "Ankle Current Mean\n",
      "VU Ax Mean\n",
      "VU Ay Mean\n",
      "VU Az Mean\n",
      "VU Gx Mean\n",
      "VU Gz Mean\n",
      "VU Gy Mean\n",
      "Shank Angle Mean\n",
      "Thigh Angle Mean\n",
      "Knee Ref Mean\n",
      "Ankle Ref Mean\n",
      "Load Mean\n",
      "Knee Angle SD\n",
      "Knee Vel SD\n",
      "Knee Current SD\n",
      "Ankle Angle SD\n",
      "Ankle Vel SD\n",
      "Ankle Current SD\n",
      "VU Ax SD\n",
      "VU Ay SD\n",
      "VU Az SD\n",
      "VU Gx SD\n",
      "VU Gz SD\n",
      "VU Gy SD\n",
      "Shank Angle SD\n",
      "Thigh Angle SD\n",
      "Knee Ref SD\n",
      "Ankle Ref SD\n",
      "Load SD\n"
     ]
    }
   ],
   "source": [
    "vu_only = list(range(17))\n",
    "contra_1 = list(range(23)) + [29]\n",
    "contra_2 = list(range(31))\n",
    "chanuse = vu_only\n",
    "\n",
    "USE_MECH_CHAN_NAME = [allcolheaders[chanuse[i]] for i in range(len(chanuse))]\n",
    "\n",
    "USE_CHAN, USE_FEAT = select_chan(USE_MECH_CHAN_NAME,allcolheaders,allfeatnames)\n",
    "print('=====Using the following {} channels:====='.format(len(USE_CHAN)))\n",
    "for chan in USE_CHAN:\n",
    "    print(allcolheaders[chan])\n",
    "print()\n",
    "print('=====Using the following {} features:====='.format(len(USE_FEAT)))\n",
    "for feat in USE_FEAT:\n",
    "    print(allfeatnames[feat])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use PCA to visualize event-specific feature space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event = 'MSW'\n",
    "\n",
    "trainfiles = []\n",
    "testfiles = []\n",
    "traineventinds = [fileind for fileind in np.arange(len(alldict['Combined ' + event + ' File Index'])) if alldict['Combined ' + event + ' File Index'][fileind] in train_files]\n",
    "testeventinds = [fileind for fileind in np.arange(len(alldict['Combined ' + event + ' File Index'])) if alldict['Combined ' + event + ' File Index'][fileind] in test_files]\n",
    "\n",
    "leave, enter, steptype = unpack_trig(alldict['Combined ' + event + ' Triggers']) # Get the leaving and entering modes from the triggers\n",
    "\n",
    "# Fit scaler with training files\n",
    "trainevent_scaler = preprocessing.StandardScaler().fit(alldict['Combined ' + event + ' Features'][traineventinds][:,USE_FEAT])\n",
    "trainevent_norm = trainevent_scaler.transform(alldict['Combined ' + event + ' Features'][traineventinds][:,USE_FEAT])\n",
    "testevent_norm = trainevent_scaler.transform(alldict['Combined ' + event + ' Features'][testeventinds][:,USE_FEAT])\n",
    "\n",
    "# Fit PCA with training files\n",
    "trainevent_pca = PCA().fit(trainevent_norm)\n",
    "trainevent_dimred = trainevent_pca.transform(trainevent_norm)\n",
    "testevent_dimred = trainevent_pca.transform(testevent_norm)   \n",
    "\n",
    "pcolor = {1: 'k', 2: 'b', 3:'c', 4:'m', 5:'g', 6:'r'}\n",
    "    \n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "for wind in np.arange(0,np.shape(trainevent_dimred)[0]):\n",
    "    ax.scatter(trainevent_dimred[wind,0],trainevent_dimred[wind,1],trainevent_dimred[wind,2],color=pcolor[enter[traineventinds][wind]],alpha=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Mode Specific Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-organize data for mode-specific classifiers for each gait event (downsampling optional)\n",
    "allevents_ms = make_modespec(alldict, USE_FEAT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Mode Specific Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MS classification:\n",
      "TO\n",
      "Unique: [2 3 4 6]\n",
      "['St', 'LW', 'SA', 'SD', 'RA', 'RD']\n",
      "[  0 641   7  64   0 109]\n",
      "LOO Accuracy: 0.9914738124238733\n",
      "\n",
      "MSW_SA\n",
      "Unique: [2 3]\n",
      "['St', 'LW', 'SA', 'SD', 'RA', 'RD']\n",
      "[  0  12 172   0   0   0]\n",
      "LOO Accuracy: 0.9619565217391305\n",
      "\n",
      "HC_LW\n",
      "Unique: [2 4 6]\n",
      "['St', 'LW', 'SA', 'SD', 'RA', 'RD']\n",
      "[  0 626   0  10   0  23]\n",
      "LOO Accuracy: 0.9484066767830045\n",
      "\n",
      "HC_SD\n",
      "Unique: [2 4]\n",
      "['St', 'LW', 'SA', 'SD', 'RA', 'RD']\n",
      "[ 0 22  0 42  0  0]\n",
      "LOO Accuracy: 0.984375\n",
      "\n",
      "STO\n",
      "Unique: [1 3]\n",
      "['St', 'LW', 'SA', 'SD', 'RA', 'RD']\n",
      "[653   0  85   0   0   0]\n",
      "LOO Accuracy: 0.978319783197832\n",
      "\n",
      "HC_RD\n",
      "Unique: [2 6]\n",
      "['St', 'LW', 'SA', 'SD', 'RA', 'RD']\n",
      "[ 0 24  0  0  0 85]\n",
      "LOO Accuracy: 0.8623853211009175\n",
      "\n",
      "MST_SD\n",
      "Unique: [2 4]\n",
      "['St', 'LW', 'SA', 'SD', 'RA', 'RD']\n",
      "[ 0 12  0 64  0  0]\n",
      "LOO Accuracy: 1.0\n",
      "\n",
      "SHC\n",
      "Unique: [1 4]\n",
      "['St', 'LW', 'SA', 'SD', 'RA', 'RD']\n",
      "[781   0   0  23   0   0]\n",
      "LOO Accuracy: 0.9987562189054726\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pcaprop = 50\n",
    "print('MS classification:')\n",
    "all_pred, all_gtruth, all_type, all_files = ms_classify_results(allevents_ms,pcaprop,[],[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           0          1          2          3          4\n",
      "0  99.649615   0.000000   0.280308   0.070077   0.000000\n",
      "1   0.000000  97.965335   0.678222   0.000000   1.356443\n",
      "2   4.511278   1.127820  94.360902   0.000000   0.000000\n",
      "3   0.000000   0.980392   0.000000  99.019608   0.000000\n",
      "4   0.000000  13.852814   0.000000   0.000000  86.147186\n",
      "\n",
      "SS Accuracy: 0.9806552262090483\n",
      "Trans Accuracy: 0.924\n",
      "Overall Accuracy: 0.9765557163531114\n",
      "\n",
      "SS steps: 3205\n",
      "T steps: 250\n",
      "\n",
      "Files with errors:\n",
      "circuit_even_121917_data_001_py.mat\n",
      "circuit_even_121917_data_007_py.mat\n",
      "circuit_even_121917_data_009_py.mat\n",
      "circuit_even_121917_data_015_py.mat\n",
      "circuit_even_121917_data_020_py.mat\n",
      "circuit_even_121917_data_022_py.mat\n",
      "circuit_even_121917_data_023_py.mat\n",
      "circuit_even_121917_data_025_py.mat\n",
      "circuit_even_121917_data_028_py.mat\n",
      "circuit_even_121917_data_033_py.mat\n",
      "circuit_even_121917_data_038_py.mat\n",
      "circuit_even_121917_data_039_py.mat\n",
      "circuit_even_121917_data_041_py.mat\n",
      "circuit_even_121917_data_050_py.mat\n",
      "circuit_even_121917_data_051_py.mat\n",
      "circuit_even_121917_data_053_py.mat\n",
      "circuit_even_121917_data_059_py.mat\n",
      "circuit_odd_121917_data_001_py.mat\n",
      "circuit_odd_121917_data_006_py.mat\n",
      "circuit_odd_121917_data_007_py.mat\n",
      "circuit_odd_121917_data_009_py.mat\n",
      "circuit_odd_121917_data_010_py.mat\n",
      "circuit_odd_121917_data_016_py.mat\n",
      "circuit_odd_121917_data_017_py.mat\n",
      "circuit_odd_121917_data_020_py.mat\n",
      "circuit_odd_121917_data_024_py.mat\n",
      "circuit_odd_121917_data_027_py.mat\n",
      "circuit_odd_121917_data_030_py.mat\n",
      "circuit_odd_121917_data_031_py.mat\n",
      "circuit_odd_RIC_121917_data_001_py.mat\n",
      "circuit_odd_RIC_121917_data_002_py.mat\n",
      "circuit_odd_RIC_121917_data_009_py.mat\n",
      "circuit_odd_RIC_121917_data_013_py.mat\n",
      "circuit_odd_RIC_121917_data_018_py.mat\n",
      "ramp_121917_data_003_py.mat\n",
      "ramp_121917_data_004_py.mat\n",
      "ramp_121917_data_005_py.mat\n",
      "ramp_121917_data_006_py.mat\n",
      "ramp_121917_data_008_py.mat\n",
      "ramp_121917_data_009_py.mat\n",
      "ramp_121917_data_010_py.mat\n",
      "stair_stand_121917_data_007_py.mat\n",
      "stand_121917_data_003_py.mat\n",
      "stand_121917_data_006_py.mat\n",
      "stand_121917_data_007_py.mat\n",
      "stand_121917_data_009_py.mat\n",
      "walk_121917_data_004_py.mat\n",
      "walk_121917_data_005_py.mat\n",
      "walk_121917_data_007_py.mat\n",
      "walk_121917_data_009_py.mat\n",
      "walk_121917_data_011_py.mat\n",
      "walk_121917_data_013_py.mat\n",
      "walk_121917_data_014_py.mat\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(all_pred,all_gtruth)\n",
    "class_totals = cm.sum(axis=1)\n",
    "norm_cm = np.zeros((5,5))\n",
    "for i in range(5):\n",
    "    norm_cm[i,:] = cm[i,:]/class_totals[i]\n",
    "\n",
    "print(pd.DataFrame(100*norm_cm))\n",
    "\n",
    "t_step = np.where(all_type == 0)[0]\n",
    "ss_step = np.where(all_type == 1)[0]\n",
    "\n",
    "print('\\nSS Accuracy: {}'.format(accuracy_score(all_pred[ss_step],all_gtruth[ss_step])))\n",
    "print('Trans Accuracy: {}'.format(accuracy_score(all_pred[t_step],all_gtruth[t_step])))\n",
    "print('Overall Accuracy: {}'.format(accuracy_score(all_pred,all_gtruth)))\n",
    "\n",
    "print('\\nSS steps: {}'.format(len(ss_step)))\n",
    "print('T steps: {}'.format(len(t_step)))\n",
    "\n",
    "print('\\nFiles with errors:')\n",
    "for fnum in np.unique([all_files[i] for i in range(len(all_files)) if all_pred[i] != all_gtruth[i]]):\n",
    "    print(filekeys[fnum])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-gpu",
   "language": "python",
   "name": "tensorflow-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
