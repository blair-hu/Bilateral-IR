{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "from scipy import stats, signal\n",
    "import scipy.io as sio\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import KFold, LeaveOneOut\n",
    "from sklearn import preprocessing, model_selection, metrics\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_vu_all_files(filedirectory, numfiles, delaybool):\n",
    "# Get data from .mat files\n",
    "# Assumes input data are in the format of time (rows) x channels (columns)\n",
    "    if len(numfiles) == 0:\n",
    "        numfiles_start = 0\n",
    "        numfiles_end = len(os.listdir(filedirectory)) # Use all files in directory\n",
    "        print('Loading {} processed data file(s) ...'.format(numfiles_end - numfiles_start + 1))     \n",
    "    else:\n",
    "        numfiles_start = numfiles[0]\n",
    "        numfiles_end = numfiles[1]\n",
    "        print('Loading {} processed data file(s) ...'.format(numfiles_end - numfiles_start + 1))      \n",
    "\n",
    "    filedict = {}\n",
    "    allindtrigsdict = {}\n",
    "        \n",
    "    for fileindex, filename in enumerate(sorted(os.listdir(filedirectory))):\n",
    "        if numfiles_start <= fileindex < numfiles_end:\n",
    "            print('=== {} ==='.format(filename))\n",
    "            mat_contents = sio.loadmat(filedirectory + '\\\\' + filename)\n",
    "            daqdata = mat_contents['python_input']\n",
    "            \n",
    "            if fileindex == numfiles_start:\n",
    "                chanlabels = mat_contents['chanlabels'].flatten()\n",
    "                allcolheaders = []\n",
    "                for i in np.arange(len(chanlabels)):\n",
    "                    allcolheaders.append(chanlabels[i][0])\n",
    "                        \n",
    "            # Get post-processed channel data\n",
    "            chans = daqdata[:,:31]\n",
    "            locomode = daqdata[:,31]\n",
    "            \n",
    "            if delaybool == True: # 3 frame (~90 ms) delay\n",
    "                use_trig = daqdata[:,33]\n",
    "                # Define valid triggers (5-digit)\n",
    "#                 hc_sm_trigs = [24210, 24510, 54510, 13410, 21410, 54210, 24610, 24410, 64610, 64210, 44410, 44210]\n",
    "                hc_sm_trigs = [24210, 24510, 54510, 21410, 54210, 24610, 24410, 64610, 64210, 44410, 44210]\n",
    "                mst_sm_trigs = [41410, 41420, 41260]\n",
    "#                 to_sm_trigs = [22230, 21130, 52530, 62630, 41430, 42430, 21330, 22370, 11330, 22330]\n",
    "                to_sm_trigs = [22230, 21130, 52530, 62630, 41430, 42430, 21330, 22370, 22330]\n",
    "                msw_sm_trigs = [39340, 39280]\n",
    "#                 shc_sm_trigs = [13110, 24410, 13410, 21410]\n",
    "                shc_sm_trigs = [13110, 13410]\n",
    "#                 sto_sm_trigs = [11130, 21130, 21330, 11330]\n",
    "                sto_sm_trigs = [11130, 11330]\n",
    "            else: # No delay\n",
    "                use_trig = daqdata[:,32]\n",
    "                # Define valid triggers (4-digit)\n",
    "#                 hc_sm_trigs = [2421, 2451, 5451, 1341, 2141, 5421, 2461, 2441, 6461, 6421, 4441, 4421]\n",
    "                hc_sm_trigs = [2421, 2451, 5451, 2141, 5421, 2461, 2441, 6461, 6421, 4441, 4421]\n",
    "                mst_sm_trigs = [4141, 4142, 4126]\n",
    "#                 to_sm_trigs = [2223, 2113, 5253, 6263, 4143, 4243, 2133, 2237, 1133, 2233]\n",
    "                to_sm_trigs = [2223, 2113, 5253, 6263, 4143, 4243, 2133, 2237, 2233]\n",
    "                msw_sm_trigs = [3934, 3928]\n",
    "#                 shc_sm_trigs = [1311, 2441, 1341, 2141]\n",
    "                shc_sm_trigs = [1311, 1341]\n",
    "#                 sto_sm_trigs = [1113, 2113, 2133, 1133]\n",
    "                sto_sm_trigs = [1113, 1133]\n",
    "\n",
    "            trig_diff = np.diff(use_trig)\n",
    "            trig_ind = np.where(trig_diff > 0)[0] + 1\n",
    "            \n",
    "            hc_ind, mst_ind, to_ind, msw_ind, shc_ind, sto_ind = [], [], [], [], [], []\n",
    "            hc_trig, mst_trig, to_trig, msw_trig, shc_trig, sto_trig = [], [], [], [], [], []\n",
    "            \n",
    "            for i in np.arange(len(trig_ind)):\n",
    "                if use_trig[trig_ind[i]] in hc_sm_trigs:\n",
    "                    hc_ind.append(int(trig_ind[i]))\n",
    "                    hc_trig.append(int(use_trig[trig_ind[i]]))\n",
    "                if use_trig[trig_ind[i]] in mst_sm_trigs:\n",
    "                    mst_ind.append(int(trig_ind[i]))\n",
    "                    mst_trig.append(int(use_trig[trig_ind[i]]))\n",
    "                if use_trig[trig_ind[i]] in to_sm_trigs:\n",
    "                    to_ind.append(int(trig_ind[i]))\n",
    "                    to_trig.append(int(use_trig[trig_ind[i]]))\n",
    "                if use_trig[trig_ind[i]] in msw_sm_trigs:\n",
    "                    msw_ind.append(int(trig_ind[i]))\n",
    "                    msw_trig.append(int(use_trig[trig_ind[i]]))\n",
    "                if use_trig[trig_ind[i]] in shc_sm_trigs:\n",
    "                    shc_ind.append(int(trig_ind[i]))\n",
    "                    shc_trig.append(int(use_trig[trig_ind[i]]))\n",
    "                if use_trig[trig_ind[i]] in sto_sm_trigs:\n",
    "                    sto_ind.append(int(trig_ind[i]))\n",
    "                    sto_trig.append(int(use_trig[trig_ind[i]]))\n",
    "            \n",
    "            hc_ind, hc_trig = np.array(hc_ind), np.array(hc_trig)\n",
    "            mst_ind, mst_trig = np.array(mst_ind), np.array(mst_trig)\n",
    "            to_ind, to_trig = np.array(to_ind), np.array(to_trig)\n",
    "            msw_ind, msw_trig = np.array(msw_ind), np.array(msw_trig)\n",
    "            shc_ind, shc_trig = np.array(shc_ind), np.array(shc_trig)\n",
    "            sto_ind, sto_trig = np.array(sto_ind), np.array(sto_trig)\n",
    "            \n",
    "            # Save the post-processed data and indices/triggers into dictionaries\n",
    "            filedict[filename] = [chans,locomode,daqdata[:,30],daqdata[:,31]]\n",
    "            allindtrigsdict[filename] = {'HC': np.vstack((hc_ind,hc_trig)).T, \n",
    "                                         'MST': np.vstack((mst_ind,mst_trig)).T, \n",
    "                                         'TO': np.vstack((to_ind,to_trig)).T, \n",
    "                                         'MSW': np.vstack((msw_ind,msw_trig)).T,\n",
    "                                         'SHC': np.vstack((shc_ind,shc_trig)).T,\n",
    "                                         'STO': np.vstack((sto_ind,sto_trig)).T}\n",
    "            \n",
    "    # filedict has the filename as the key and the 2D DAQ data and targets as entries\n",
    "    # allcolheaders has the channel names\n",
    "    print('Finished!')\n",
    "    \n",
    "    return filedict, allindtrigsdict, allcolheaders\n",
    "\n",
    "\n",
    "def unpack_files(filedict, allindtrigsdict, filekeys, arginput, allcolheaders):    \n",
    "    # Unpack the list of arguments (arginput)\n",
    "    # Windowing parameters\n",
    "    \n",
    "    TRAIN_SIZE = arginput[0] # Sliding window length\n",
    "    \n",
    "    # Train/test split parameters\n",
    "    FOR_TEST = arginput[1]\n",
    "    TOTAL_FILES = len(filekeys)\n",
    "    if len(FOR_TEST) > 1 or type(FOR_TEST[0]) is str:\n",
    "        print('Generating train/test data from {} specified file(s):'.format(len(FOR_TEST)))\n",
    "        TEST_FILE_INDS = []\n",
    "        for testfileind, testfile in enumerate(FOR_TEST):\n",
    "            TEST_FILE_INDS.append(filekeys.index(testfile))\n",
    "        TEST_FILE_INDS = np.array(TEST_FILE_INDS)\n",
    "        TRAIN_FILE_INDS = np.setdiff1d(np.arange(TOTAL_FILES),TEST_FILE_INDS)\n",
    "    else:\n",
    "        print('Randomly select train/test data based on testing data proportion ({}%):'.format(int(100*FOR_TEST[0])))\n",
    "        TOTAL_FOLDS = int(1/(FOR_TEST[0]))\n",
    "        SPLIT_FILES = np.array_split(np.arange(TOTAL_FILES),TOTAL_FOLDS)\n",
    "        TEST_FILE_INDS = SPLIT_FILES[np.random.randint(0,TOTAL_FOLDS-1)]\n",
    "        TRAIN_FILE_INDS = np.setdiff1d(np.arange(TOTAL_FILES),TEST_FILE_INDS)\n",
    "    print()\n",
    "    \n",
    "    # Channels to use\n",
    "    CHAN_MECH = arginput[2]\n",
    "    \n",
    "    # Printing parameters\n",
    "    PRINT_SUMMARY = arginput[3]\n",
    "    \n",
    "    alldict = {}\n",
    "    alldict['Combined File Index'] = []\n",
    "        \n",
    "    for gaitevent in ['HC', 'MST', 'TO', 'MSW', 'SHC', 'STO']:\n",
    "        alldict['Combined ' + gaitevent + ' Windows'] = []\n",
    "        alldict['Combined ' + gaitevent + ' Features'] = []\n",
    "        alldict['Combined ' + gaitevent + ' Triggers'] = [] \n",
    "        alldict['Combined ' + gaitevent + ' File Index'] = []\n",
    "        \n",
    "    for fkeyind, fkey in enumerate(filekeys):\n",
    "        print('Preparing file {}: {} ...'.format(fkeyind+1,fkey))\n",
    "\n",
    "        data = filedict[fkey][0]\n",
    "        locomode = filedict[fkey][1]\n",
    "        indtrigs = allindtrigsdict[fkey]\n",
    "\n",
    "        alleventdict, featnames = event_windows(indtrigs, data, TRAIN_SIZE, CHAN_MECH, allcolheaders)\n",
    "        \n",
    "        alleventfeats = {'HC': alleventdict['HC'][0], \n",
    "                         'MST': alleventdict['MST'][0], \n",
    "                         'TO': alleventdict['TO'][0], \n",
    "                         'MSW': alleventdict['MSW'][0],\n",
    "                         'SHC': alleventdict['SHC'][0],\n",
    "                         'STO': alleventdict['STO'][0]}\n",
    "\n",
    "        for gaitevent in ['HC', 'MST', 'TO', 'MSW', 'SHC', 'STO']:\n",
    "            # Save the features extracted from each gait event\n",
    "            alldict['Combined ' + gaitevent + ' Features'].append(alleventfeats[gaitevent])\n",
    "            # Save the trigger associated with each gait event\n",
    "            alldict['Combined ' + gaitevent + ' Triggers'].append(alleventdict[gaitevent][1])\n",
    "            # Save file index associated with each gait event\n",
    "            alldict['Combined ' + gaitevent + ' File Index'].append(np.tile(fkeyind,len(alleventdict[gaitevent][1])))\n",
    "    print()\n",
    "    \n",
    "    for gaitevent in ['HC', 'MST', 'TO', 'MSW', 'SHC', 'STO']:\n",
    "        # Remove empty lists\n",
    "        alldict['Combined ' + gaitevent + ' Features'] = [features for features in alldict['Combined ' + gaitevent + ' Features'] if len(features) > 0]\n",
    "        alldict['Combined ' + gaitevent + ' Triggers'] = [triggers for triggers in alldict['Combined ' + gaitevent + ' Triggers'] if len(triggers) > 0]\n",
    "        alldict['Combined ' + gaitevent + ' File Index'] = [fileinds for fileinds in alldict['Combined ' + gaitevent + ' File Index'] if len(fileinds) > 0]\n",
    "        \n",
    "        alldict['Combined ' + gaitevent + ' Features'] = np.concatenate(alldict['Combined ' + gaitevent + ' Features'])\n",
    "        alldict['Combined ' + gaitevent + ' Triggers'] = np.concatenate(alldict['Combined ' + gaitevent + ' Triggers'])\n",
    "        alldict['Combined ' + gaitevent + ' File Index'] = np.concatenate(alldict['Combined ' + gaitevent + ' File Index'])\n",
    "        \n",
    "    if PRINT_SUMMARY:\n",
    "        print('Aggregated HC feature dimensions: {}'.format(alldict['Combined HC Features'].shape))\n",
    "        print('Aggregated MST feature dimensions: {}'.format(alldict['Combined MST Features'].shape))\n",
    "        print('Aggregated TO feature dimensions: {}'.format(alldict['Combined TO Features'].shape))\n",
    "        print('Aggregated MSW feature dimensions: {}'.format(alldict['Combined MSW Features'].shape))\n",
    "        print('Aggregated SHC feature dimensions: {}'.format(alldict['Combined SHC Features'].shape))\n",
    "        print('Aggregated STO feature dimensions: {}'.format(alldict['Combined STO Features'].shape))\n",
    "        print()\n",
    "        \n",
    "    return alldict, featnames, TRAIN_FILE_INDS, TEST_FILE_INDS\n",
    "\n",
    "\n",
    "def event_windows(indtrigs, data, train_window, chanmech, allcolheaders):\n",
    "# Get data from windows near gait events specified by indtrigs\n",
    "# Try different sized windows (pre_stance before stance and pre_swing before swing)\n",
    "# Try data augmentation (get aug_windows_per_event extra windows beginning aug_pre before to aug_post after the gait event)\n",
    "# scale = normalize data (boolean)\n",
    "\n",
    "    mechheaders = [allcolheaders[i] for i in chanmech]\n",
    "    eventkeys = list(indtrigs.keys())\n",
    "        \n",
    "    alleventdict = {}\n",
    "    for eventkeyind, eventkey in enumerate(eventkeys):\n",
    "        allfeats, trig_list = [], []\n",
    "        \n",
    "        inds = indtrigs[eventkey][:,0]\n",
    "        trigs = indtrigs[eventkey][:,1]\n",
    "        \n",
    "        # Remove triggers occurring less than 300 ms into DAQ file\n",
    "        keepinds = [inds > train_window]\n",
    "        inds = inds[keepinds]\n",
    "        trigs = trigs[keepinds] \n",
    "        \n",
    "        if len(inds) > 0:\n",
    "            for i in range(inds.shape[0]): # Iterates over indices\n",
    "                x_i = data[(inds[i]-train_window):inds[i],:]\n",
    "                y_i = trigs[i]\n",
    "\n",
    "                feats, featslabels = feats_from_window(x_i[-300:],chanmech,mechheaders)\n",
    "\n",
    "                allfeats.append(feats)\n",
    "                trig_list.append(y_i)\n",
    "\n",
    "            # Convert from list of arrays to 3D array        \n",
    "            alleventdict[eventkey] = [np.array(allfeats)] # Extracted features\n",
    "            alleventdict[eventkey].append(np.array(trig_list)) # Target\n",
    "        else:\n",
    "            print('No ' + eventkey + ' events...')\n",
    "            alleventdict[eventkey] = [[],[]]\n",
    "            \n",
    "    # alleventdict has the gait event (HC, TO) as the key and contains the 3d array of the extracted windows (300 ms for feature extraction and pre_stance, pre_swing) and targets\n",
    "    return alleventdict, featslabels\n",
    "\n",
    "\n",
    "def feats_from_window(window,chanmech,mechheaders):\n",
    "    if len(chanmech) > 0:\n",
    "        mechfeats, mechfeatsnames = getmechfeats(window,chanmech,mechheaders)\n",
    "    else:\n",
    "        mechfeats = []\n",
    "\n",
    "    feats = mechfeats.T    \n",
    "    featslabels = mechfeatsnames\n",
    "    \n",
    "    return feats, featslabels\n",
    "\n",
    "\n",
    "def getmechfeats(X,chanmech,mechheaders):    \n",
    "    X_mech = X[:,chanmech]\n",
    "\n",
    "    X_min = np.min(X_mech,axis=0)\n",
    "    X_max = np.max(X_mech,axis=0)\n",
    "    X_mean = np.mean(X_mech,axis=0)\n",
    "    X_std = np.std(X_mech,axis=0)\n",
    "    X_init = X_mech[0]\n",
    "    X_final = X_mech[-1]\n",
    "\n",
    "    mechfeats = np.array([X_min,X_max,X_init,X_final,X_mean,X_std]).flatten()\n",
    "    mechfeatsnames = []\n",
    "    \n",
    "    featsnames = [' Min',' Max',' Initial',' Final',' Mean',' SD']\n",
    "    \n",
    "    for names in featsnames:\n",
    "        for mechchan in mechheaders:\n",
    "            mechfeatsnames.append(mechchan + names)\n",
    "\n",
    "    return mechfeats, mechfeatsnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unpack_trig(triggers):\n",
    "# Get the first and third digits of the four-digit trigger in order to make mode-specific classifiers\n",
    "\n",
    "    trigstr = triggers.astype(int).astype(str)\n",
    "    leavemode = np.array([int(trig[0]) for trig in trigstr])\n",
    "    entermode = np.array([int(trig[2]) for trig in trigstr])\n",
    "    steptype = np.array([int(leavemode[i] == entermode[i]) for i in range(len(leavemode))])\n",
    "    \n",
    "    return leavemode, entermode, steptype\n",
    "    \n",
    "    \n",
    "def split_by_leavemode(feat_data, leavemode, useleave, entermode, steptype, fileind):\n",
    "# Organize data into dictionary with two-layer keys: gait event (RHC, RTO, LHC, LTO) and mode (LW, RA, RD, SA, SD)\n",
    "# DS_FACTOR = down-sampling factor (may improve NN performance by reducing dimensionality of raw data)\n",
    "# fileind keeps track of which file each gait event came from (in order to do leave-one-circuit-out cross validation)\n",
    "    # 1 = St, 2 = LW, 3 = SA, 4 = SD, 5 = RA, 6 = RD\n",
    "    \n",
    "    lmode_inds = [ind for ind in range(len(leavemode)) if leavemode[ind] in useleave] \n",
    "    feat_data_inds = feat_data[lmode_inds]\n",
    "    target_inds = entermode[lmode_inds]\n",
    "    file_inds = fileind[lmode_inds]\n",
    "    type_bin_inds = steptype[lmode_inds]\n",
    "    \n",
    "    # Merge RA targets with LW targets\n",
    "    target_inds[target_inds == 5] = 2\n",
    "\n",
    "    data_ms = [feat_data_inds, target_inds, file_inds, type_bin_inds]\n",
    "    \n",
    "    return data_ms \n",
    " \n",
    "\n",
    "def make_modespec(alldict,USE_FEAT):\n",
    "# Make modespec_dict to organize all events/modes\n",
    "    \n",
    "    hc_leave, hc_enter, hc_type = unpack_trig(alldict['Combined HC Triggers'])\n",
    "    mst_leave, mst_enter, mst_type = unpack_trig(alldict['Combined MST Triggers'])\n",
    "    to_leave, to_enter, to_type = unpack_trig(alldict['Combined TO Triggers'])\n",
    "    msw_leave, msw_enter, msw_type = unpack_trig(alldict['Combined MSW Triggers'])\n",
    "    shc_leave, shc_enter, shc_type = unpack_trig(alldict['Combined SHC Triggers'])\n",
    "    sto_leave, sto_enter, sto_type = unpack_trig(alldict['Combined STO Triggers'])\n",
    "    \n",
    "    hc_lw_ms = split_by_leavemode(alldict['Combined HC Features'][:,USE_FEAT], hc_leave, [2,5], hc_enter, hc_type, alldict['Combined HC File Index'])\n",
    "    hc_rd_ms = split_by_leavemode(alldict['Combined HC Features'][:,USE_FEAT], hc_leave, [6], hc_enter, hc_type, alldict['Combined HC File Index'])\n",
    "    hc_sd_ms = split_by_leavemode(alldict['Combined HC Features'][:,USE_FEAT], hc_leave, [4], hc_enter, hc_type, alldict['Combined HC File Index'])\n",
    "    \n",
    "    mst_sd_ms = split_by_leavemode(alldict['Combined MST Features'][:,USE_FEAT], mst_leave, [4], mst_enter, mst_type, alldict['Combined MST File Index'])\n",
    "#     to_ms = split_by_leavemode(alldict['Combined TO Features'][:,USE_FEAT], to_leave, [1,2,4,5,6], to_enter, to_type, alldict['Combined TO File Index'])\n",
    "    to_ms = split_by_leavemode(alldict['Combined TO Features'][:,USE_FEAT], to_leave, [2,4,5,6], to_enter, to_type, alldict['Combined TO File Index'])\n",
    "    msw_sa_ms = split_by_leavemode(alldict['Combined MSW Features'][:,USE_FEAT], msw_leave, [3], msw_enter, msw_type, alldict['Combined MSW File Index'])\n",
    "#     shc_ms = split_by_leavemode(alldict['Combined SHC Features'][:,USE_FEAT], shc_leave, [1,2], shc_enter, shc_type, alldict['Combined SHC File Index'])\n",
    "#     sto_ms = split_by_leavemode(alldict['Combined STO Features'][:,USE_FEAT], sto_leave, [1,2], sto_enter, sto_type, alldict['Combined STO File Index'])\n",
    "    shc_ms = split_by_leavemode(alldict['Combined SHC Features'][:,USE_FEAT], shc_leave, [1], shc_enter, shc_type, alldict['Combined SHC File Index'])\n",
    "    sto_ms = split_by_leavemode(alldict['Combined STO Features'][:,USE_FEAT], sto_leave, [1], sto_enter, sto_type, alldict['Combined STO File Index'])\n",
    "    \n",
    "    modespec_dict = {'HC_LW': hc_lw_ms, 'HC_RD': hc_rd_ms, 'HC_SD': hc_sd_ms,\n",
    "                     'MST_SD': mst_sd_ms, 'TO': to_ms, 'MSW_SA': msw_sa_ms, 'SHC': shc_ms, 'STO': sto_ms}\n",
    "    \n",
    "    return modespec_dict    \n",
    "\n",
    "\n",
    "def lda_classify(train_in,train_targ,test_in,test_targ,pca_proportion):\n",
    "    lda = LinearDiscriminantAnalysis()\n",
    "    \n",
    "    train_in_scaler = preprocessing.StandardScaler().fit(train_in)\n",
    "    train_in_scaled = train_in_scaler.transform(train_in)\n",
    "    test_in_scaled = train_in_scaler.transform(test_in)\n",
    "\n",
    "    train_in_scaler_pca = PCA().fit(train_in_scaled)\n",
    "    train_in_scaled_pca_xfm = train_in_scaler_pca.transform(train_in_scaled)\n",
    "    test_in_scaled_pca_xfm = train_in_scaler_pca.transform(test_in_scaled)\n",
    "\n",
    "    pcaexplainedvar = np.cumsum(train_in_scaler_pca.explained_variance_ratio_)\n",
    "    if pca_proportion < 1:\n",
    "        pcanumcomps = min(min(np.where(pcaexplainedvar > pca_proportion))) + 1\n",
    "    else:\n",
    "        pcanumcomps = pca_proportion\n",
    "\n",
    "    unique_modes = np.unique(train_targ)\n",
    "    lda.set_params(priors = np.ones(len(unique_modes))/len(unique_modes))\n",
    "\n",
    "    test_pred = lda.fit(train_in_scaled_pca_xfm[:,0:pcanumcomps],train_targ).predict(test_in_scaled_pca_xfm[:,0:pcanumcomps])\n",
    "    test_pred_prob = lda.fit(train_in_scaled_pca_xfm[:,0:pcanumcomps],train_targ).predict_proba(test_in_scaled_pca_xfm[:,0:pcanumcomps])\n",
    "    test_gtruth = test_targ\n",
    "    \n",
    "    return test_pred, test_gtruth, test_pred_prob\n",
    "\n",
    "    \n",
    "def ms_classify_results(msdict,pcaprop,trainfiles,testfiles):\n",
    "    ohe = preprocessing.OneHotEncoder()\n",
    "    lda = LinearDiscriminantAnalysis()\n",
    "    kf = KFold(n_splits=10, shuffle=True)\n",
    "    loo = LeaveOneOut()\n",
    "    \n",
    "    # Define histogram labels\n",
    "    gaitmodes = ['St','LW','SA','SD','RA','RD']\n",
    "    \n",
    "    all_pred, all_gtruth, all_type, all_files = [], [], [], []\n",
    "    \n",
    "    for classifiers in list(allevents_ms.keys()):\n",
    "        print('Classifier: {}'.format(classifiers))\n",
    "        \n",
    "        event_mode_features = msdict[classifiers][0]\n",
    "        event_mode_targets = msdict[classifiers][1]\n",
    "        event_mode_files = msdict[classifiers][2]\n",
    "        event_mode_typebin = msdict[classifiers][3]\n",
    "    \n",
    "        print('Unique: {}'.format(np.unique(event_mode_targets)))\n",
    "        print(gaitmodes)\n",
    "        print(np.histogram(event_mode_targets,bins=[0.5, 1.5, 2.5, 3.5, 4.5, 5.5, 6.5])[0])  \n",
    "        \n",
    "        if len(np.unique(event_mode_targets)) > 1:\n",
    "            event_mode_pred = np.zeros((len(event_mode_targets),1))\n",
    "            if len(trainfiles) > 0 and len(testfiles) > 0:\n",
    "                train_inds = [i for i in range(len(event_mode_files)) if event_mode_files[i] in trainfiles]\n",
    "                test_inds = [i for i in range(len(event_mode_files)) if event_mode_files[i] in testfiles]\n",
    "                pred, gtruth, pred_prob = lda_classify(event_mode_features[train_inds],event_mode_targets[train_inds],event_mode_features[test_inds],event_mode_targets[test_inds],pcaprop)           \n",
    "                event_mode_pred[test_inds] = pred\n",
    "            else:\n",
    "                for train_inds, test_inds in loo.split(event_mode_features,event_mode_targets):\n",
    "                    pred, gtruth, pred_prob = lda_classify(event_mode_features[train_inds],event_mode_targets[train_inds],event_mode_features[test_inds],event_mode_targets[test_inds],pcaprop)           \n",
    "                    event_mode_pred[test_inds] = pred\n",
    "\n",
    "            print('LOO Accuracy: {}'.format(accuracy_score(event_mode_pred,event_mode_targets)))\n",
    "            print()\n",
    "            \n",
    "            all_pred.append(event_mode_pred)\n",
    "            all_gtruth.append(event_mode_targets)\n",
    "            all_type.append(event_mode_typebin)\n",
    "            all_files.append(event_mode_files) \n",
    "        else:\n",
    "            print('Skipping {} because only one class'.format(classifiers))\n",
    "            print()\n",
    "    \n",
    "    all_pred = np.concatenate(all_pred)\n",
    "    all_gtruth = np.concatenate(all_gtruth)\n",
    "    all_type = np.concatenate(all_type)\n",
    "    all_files = np.concatenate(all_files)\n",
    "    \n",
    "    return all_pred, all_gtruth, all_type, all_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def select_chan(USE_MECH_CHAN_NAME,allcolheaders,allfeatnames):\n",
    "    for featind, featname in enumerate(allfeatnames):\n",
    "        if '_' in featname:\n",
    "            allfeatnames[featind] = featname.replace('_',' ')\n",
    "    \n",
    "    for colind, colheader in enumerate(allcolheaders):\n",
    "        if '_' in colheader:\n",
    "            allcolheaders[colind] = colheader.replace('_',' ')\n",
    "    \n",
    "    if len(USE_MECH_CHAN_NAME) > 0:\n",
    "        USE_MECH_CHAN = []\n",
    "        USE_MECH_FEAT = []\n",
    "        for channameind, channame in enumerate(USE_MECH_CHAN_NAME): \n",
    "            MECH_CHAN_IND = [allcolheaders.index(colname) for colname in allcolheaders if channame in colname]\n",
    "            MECH_FEAT_IND = [allfeatnames.index(colname) for colname in allfeatnames if channame in colname]\n",
    "            USE_MECH_CHAN.append(MECH_CHAN_IND)\n",
    "            USE_MECH_FEAT.append(MECH_FEAT_IND)\n",
    "        \n",
    "        USE_MECH_CHAN = np.array(sorted(sum(USE_MECH_CHAN,[])),dtype=int)\n",
    "        USE_MECH_FEAT = np.array(sorted(sum(USE_MECH_FEAT,[])),dtype=int)\n",
    "\n",
    "        USE_CHAN = USE_MECH_CHAN\n",
    "        USE_FEAT = USE_MECH_FEAT\n",
    "    else:\n",
    "        USE_CHAN = np.arange(len(allcolheaders))\n",
    "        USE_FEAT = np.arange(len(allfeatnames))\n",
    "    \n",
    "    return np.unique(USE_CHAN), np.unique(USE_FEAT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and organize raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 127 processed data file(s) ...\n",
      "=== circuit_even_121917_data_001_pydaq.mat ===\n",
      "=== circuit_even_121917_data_002_pydaq.mat ===\n",
      "=== circuit_even_121917_data_003_pydaq.mat ===\n",
      "=== circuit_even_121917_data_004_pydaq.mat ===\n",
      "=== circuit_even_121917_data_007_pydaq.mat ===\n",
      "=== circuit_even_121917_data_009_pydaq.mat ===\n",
      "=== circuit_even_121917_data_010_pydaq.mat ===\n",
      "=== circuit_even_121917_data_012_pydaq.mat ===\n",
      "=== circuit_even_121917_data_013_pydaq.mat ===\n",
      "=== circuit_even_121917_data_014_pydaq.mat ===\n",
      "=== circuit_even_121917_data_015_pydaq.mat ===\n",
      "=== circuit_even_121917_data_017_pydaq.mat ===\n",
      "=== circuit_even_121917_data_018_pydaq.mat ===\n",
      "=== circuit_even_121917_data_020_pydaq.mat ===\n",
      "=== circuit_even_121917_data_022_pydaq.mat ===\n",
      "=== circuit_even_121917_data_023_pydaq.mat ===\n",
      "=== circuit_even_121917_data_025_pydaq.mat ===\n",
      "=== circuit_even_121917_data_026_pydaq.mat ===\n",
      "=== circuit_even_121917_data_028_pydaq.mat ===\n",
      "=== circuit_even_121917_data_030_pydaq.mat ===\n",
      "=== circuit_even_121917_data_032_pydaq.mat ===\n",
      "=== circuit_even_121917_data_033_pydaq.mat ===\n",
      "=== circuit_even_121917_data_035_pydaq.mat ===\n",
      "=== circuit_even_121917_data_036_pydaq.mat ===\n",
      "=== circuit_even_121917_data_038_pydaq.mat ===\n",
      "=== circuit_even_121917_data_039_pydaq.mat ===\n",
      "=== circuit_even_121917_data_041_pydaq.mat ===\n",
      "=== circuit_even_121917_data_042_pydaq.mat ===\n",
      "=== circuit_even_121917_data_044_pydaq.mat ===\n",
      "=== circuit_even_121917_data_045_pydaq.mat ===\n",
      "=== circuit_even_121917_data_047_pydaq.mat ===\n",
      "=== circuit_even_121917_data_048_pydaq.mat ===\n",
      "=== circuit_even_121917_data_050_pydaq.mat ===\n",
      "=== circuit_even_121917_data_051_pydaq.mat ===\n",
      "=== circuit_even_121917_data_053_pydaq.mat ===\n",
      "=== circuit_even_121917_data_054_pydaq.mat ===\n",
      "=== circuit_even_121917_data_056_pydaq.mat ===\n",
      "=== circuit_even_121917_data_057_pydaq.mat ===\n",
      "=== circuit_even_121917_data_059_pydaq.mat ===\n",
      "=== circuit_even_121917_data_060_pydaq.mat ===\n",
      "=== circuit_even_121917_data_062_pydaq.mat ===\n",
      "=== circuit_odd_121917_data_001_pydaq.mat ===\n",
      "=== circuit_odd_121917_data_002_pydaq.mat ===\n",
      "=== circuit_odd_121917_data_003_pydaq.mat ===\n",
      "=== circuit_odd_121917_data_004_pydaq.mat ===\n",
      "=== circuit_odd_121917_data_005_pydaq.mat ===\n",
      "=== circuit_odd_121917_data_006_pydaq.mat ===\n",
      "=== circuit_odd_121917_data_007_pydaq.mat ===\n",
      "=== circuit_odd_121917_data_008_pydaq.mat ===\n",
      "=== circuit_odd_121917_data_009_pydaq.mat ===\n",
      "=== circuit_odd_121917_data_010_pydaq.mat ===\n",
      "=== circuit_odd_121917_data_011_pydaq.mat ===\n",
      "=== circuit_odd_121917_data_012_pydaq.mat ===\n",
      "=== circuit_odd_121917_data_014_pydaq.mat ===\n",
      "=== circuit_odd_121917_data_015_pydaq.mat ===\n",
      "=== circuit_odd_121917_data_016_pydaq.mat ===\n",
      "=== circuit_odd_121917_data_017_pydaq.mat ===\n",
      "=== circuit_odd_121917_data_019_pydaq.mat ===\n",
      "=== circuit_odd_121917_data_020_pydaq.mat ===\n",
      "=== circuit_odd_121917_data_021_pydaq.mat ===\n",
      "=== circuit_odd_121917_data_023_pydaq.mat ===\n",
      "=== circuit_odd_121917_data_024_pydaq.mat ===\n",
      "=== circuit_odd_121917_data_025_pydaq.mat ===\n",
      "=== circuit_odd_121917_data_027_pydaq.mat ===\n",
      "=== circuit_odd_121917_data_028_pydaq.mat ===\n",
      "=== circuit_odd_121917_data_029_pydaq.mat ===\n",
      "=== circuit_odd_121917_data_030_pydaq.mat ===\n",
      "=== circuit_odd_121917_data_031_pydaq.mat ===\n",
      "=== circuit_odd_RIC_121917_data_001_pydaq.mat ===\n",
      "=== circuit_odd_RIC_121917_data_002_pydaq.mat ===\n",
      "=== circuit_odd_RIC_121917_data_003_pydaq.mat ===\n",
      "=== circuit_odd_RIC_121917_data_004_pydaq.mat ===\n",
      "=== circuit_odd_RIC_121917_data_005_pydaq.mat ===\n",
      "=== circuit_odd_RIC_121917_data_006_pydaq.mat ===\n",
      "=== circuit_odd_RIC_121917_data_007_pydaq.mat ===\n",
      "=== circuit_odd_RIC_121917_data_009_pydaq.mat ===\n",
      "=== circuit_odd_RIC_121917_data_010_pydaq.mat ===\n",
      "=== circuit_odd_RIC_121917_data_011_pydaq.mat ===\n",
      "=== circuit_odd_RIC_121917_data_012_pydaq.mat ===\n",
      "=== circuit_odd_RIC_121917_data_013_pydaq.mat ===\n",
      "=== circuit_odd_RIC_121917_data_014_pydaq.mat ===\n",
      "=== circuit_odd_RIC_121917_data_015_pydaq.mat ===\n",
      "=== circuit_odd_RIC_121917_data_016_pydaq.mat ===\n",
      "=== circuit_odd_RIC_121917_data_017_pydaq.mat ===\n",
      "=== circuit_odd_RIC_121917_data_019_pydaq.mat ===\n",
      "=== ramp_121917_data_002_pydaq.mat ===\n",
      "=== ramp_121917_data_003_pydaq.mat ===\n",
      "=== ramp_121917_data_004_pydaq.mat ===\n",
      "=== ramp_121917_data_005_pydaq.mat ===\n",
      "=== ramp_121917_data_006_pydaq.mat ===\n",
      "=== ramp_121917_data_008_pydaq.mat ===\n",
      "=== ramp_121917_data_009_pydaq.mat ===\n",
      "=== ramp_121917_data_010_pydaq.mat ===\n",
      "=== stair_stand_121917_data_001_pydaq.mat ===\n",
      "=== stair_stand_121917_data_002_pydaq.mat ===\n",
      "=== stair_stand_121917_data_004_pydaq.mat ===\n",
      "=== stair_stand_121917_data_005_pydaq.mat ===\n",
      "=== stair_stand_121917_data_006_pydaq.mat ===\n",
      "=== stair_stand_121917_data_007_pydaq.mat ===\n",
      "=== stair_stand_121917_data_008_pydaq.mat ===\n",
      "=== stair_stand_121917_data_009_pydaq.mat ===\n",
      "=== stand_121917_data_001_pydaq.mat ===\n",
      "=== stand_121917_data_002_pydaq.mat ===\n",
      "=== stand_121917_data_003_pydaq.mat ===\n",
      "=== stand_121917_data_004_pydaq.mat ===\n",
      "=== stand_121917_data_005_pydaq.mat ===\n",
      "=== stand_121917_data_006_pydaq.mat ===\n",
      "=== stand_121917_data_007_pydaq.mat ===\n",
      "=== stand_121917_data_008_pydaq.mat ===\n",
      "=== stand_121917_data_009_pydaq.mat ===\n",
      "=== stand_121917_data_010_pydaq.mat ===\n",
      "=== walk_121917_data_001_pydaq.mat ===\n",
      "=== walk_121917_data_002_pydaq.mat ===\n",
      "=== walk_121917_data_003_pydaq.mat ===\n",
      "=== walk_121917_data_004_pydaq.mat ===\n",
      "=== walk_121917_data_005_pydaq.mat ===\n",
      "=== walk_121917_data_006_pydaq.mat ===\n",
      "=== walk_121917_data_007_pydaq.mat ===\n",
      "=== walk_121917_data_008_pydaq.mat ===\n",
      "=== walk_121917_data_009_pydaq.mat ===\n",
      "=== walk_121917_data_010_pydaq.mat ===\n",
      "=== walk_121917_data_011_pydaq.mat ===\n",
      "=== walk_121917_data_012_pydaq.mat ===\n",
      "=== walk_121917_data_013_pydaq.mat ===\n",
      "=== walk_121917_data_014_pydaq.mat ===\n",
      "=== walk_121917_data_015_pydaq.mat ===\n",
      "Finished!\n"
     ]
    }
   ],
   "source": [
    "# Just get the filenames from the filedir\n",
    "# FILEDIR = u'C:\\\\Users\\\\bhu\\\\Git\\\\Bilateral_Intent_Recognition\\\\Data\\\\Prosthesis'\n",
    "FILEDIR = u'Z:\\\\Lab Member Folders\\\\Blair Hu\\\\Contralateral Prosthesis Control 2017\\\\TF01_Goldie04_121917_Dev\\\\DATA\\\\FORPY'\n",
    "NUM_FILES = [] # if empty list, load all files; otherwise [X] means load X files\n",
    "DELAYED = False\n",
    "filedict, allindtrigsdict, allcolheaders = load_vu_all_files(FILEDIR, NUM_FILES, DELAYED)\n",
    "filekeys = sorted(list(filedict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomly select train/test data based on testing data proportion (5%):\n",
      "\n",
      "Preparing file 1: circuit_even_121917_data_001_pydaq.mat ...\n",
      "No MST events...\n",
      "Preparing file 2: circuit_even_121917_data_002_pydaq.mat ...\n",
      "No MSW events...\n",
      "No MST events...\n",
      "No STO events...\n",
      "Preparing file 3: circuit_even_121917_data_003_pydaq.mat ...\n",
      "No MSW events...\n",
      "No MST events...\n",
      "Preparing file 4: circuit_even_121917_data_004_pydaq.mat ...\n",
      "No MSW events...\n",
      "No MST events...\n",
      "Preparing file 5: circuit_even_121917_data_007_pydaq.mat ...\n",
      "No MST events...\n",
      "Preparing file 6: circuit_even_121917_data_009_pydaq.mat ...\n",
      "No MSW events...\n",
      "No MST events...\n",
      "Preparing file 7: circuit_even_121917_data_010_pydaq.mat ...\n",
      "No MSW events...\n",
      "No MST events...\n",
      "Preparing file 8: circuit_even_121917_data_012_pydaq.mat ...\n",
      "No MSW events...\n",
      "Preparing file 9: circuit_even_121917_data_013_pydaq.mat ...\n",
      "No MST events...\n",
      "Preparing file 10: circuit_even_121917_data_014_pydaq.mat ...\n",
      "No MSW events...\n",
      "No SHC events...\n",
      "No MST events...\n",
      "No STO events...\n",
      "Preparing file 11: circuit_even_121917_data_015_pydaq.mat ...\n",
      "No MSW events...\n",
      "No MST events...\n",
      "Preparing file 12: circuit_even_121917_data_017_pydaq.mat ...\n",
      "No MSW events...\n",
      "No SHC events...\n",
      "No MST events...\n",
      "No STO events...\n",
      "Preparing file 13: circuit_even_121917_data_018_pydaq.mat ...\n",
      "No MST events...\n",
      "Preparing file 14: circuit_even_121917_data_020_pydaq.mat ...\n",
      "No MST events...\n",
      "Preparing file 15: circuit_even_121917_data_022_pydaq.mat ...\n",
      "No MSW events...\n",
      "No MST events...\n",
      "Preparing file 16: circuit_even_121917_data_023_pydaq.mat ...\n",
      "No MSW events...\n",
      "No MST events...\n",
      "Preparing file 17: circuit_even_121917_data_025_pydaq.mat ...\n",
      "No MSW events...\n",
      "Preparing file 18: circuit_even_121917_data_026_pydaq.mat ...\n",
      "No MST events...\n",
      "Preparing file 19: circuit_even_121917_data_028_pydaq.mat ...\n",
      "No MSW events...\n",
      "No MST events...\n",
      "Preparing file 20: circuit_even_121917_data_030_pydaq.mat ...\n",
      "No MSW events...\n",
      "No SHC events...\n",
      "No MST events...\n",
      "No STO events...\n",
      "Preparing file 21: circuit_even_121917_data_032_pydaq.mat ...\n",
      "No MSW events...\n",
      "Preparing file 22: circuit_even_121917_data_033_pydaq.mat ...\n",
      "No MST events...\n",
      "Preparing file 23: circuit_even_121917_data_035_pydaq.mat ...\n",
      "No MSW events...\n",
      "No MST events...\n",
      "No STO events...\n",
      "Preparing file 24: circuit_even_121917_data_036_pydaq.mat ...\n",
      "No MSW events...\n",
      "No MST events...\n",
      "Preparing file 25: circuit_even_121917_data_038_pydaq.mat ...\n",
      "No MSW events...\n",
      "No STO events...\n",
      "Preparing file 26: circuit_even_121917_data_039_pydaq.mat ...\n",
      "No MST events...\n",
      "Preparing file 27: circuit_even_121917_data_041_pydaq.mat ...\n",
      "No MSW events...\n",
      "No MST events...\n",
      "Preparing file 28: circuit_even_121917_data_042_pydaq.mat ...\n",
      "No MSW events...\n",
      "No MST events...\n",
      "Preparing file 29: circuit_even_121917_data_044_pydaq.mat ...\n",
      "No MSW events...\n",
      "Preparing file 30: circuit_even_121917_data_045_pydaq.mat ...\n",
      "No MSW events...\n",
      "No MST events...\n",
      "Preparing file 31: circuit_even_121917_data_047_pydaq.mat ...\n",
      "No MSW events...\n",
      "No STO events...\n",
      "Preparing file 32: circuit_even_121917_data_048_pydaq.mat ...\n",
      "No MST events...\n",
      "Preparing file 33: circuit_even_121917_data_050_pydaq.mat ...\n",
      "No MSW events...\n",
      "No MST events...\n",
      "Preparing file 34: circuit_even_121917_data_051_pydaq.mat ...\n",
      "No MST events...\n",
      "Preparing file 35: circuit_even_121917_data_053_pydaq.mat ...\n",
      "No MSW events...\n",
      "No MST events...\n",
      "Preparing file 36: circuit_even_121917_data_054_pydaq.mat ...\n",
      "No MST events...\n",
      "Preparing file 37: circuit_even_121917_data_056_pydaq.mat ...\n",
      "No MSW events...\n",
      "No MST events...\n",
      "No STO events...\n",
      "Preparing file 38: circuit_even_121917_data_057_pydaq.mat ...\n",
      "No MST events...\n",
      "Preparing file 39: circuit_even_121917_data_059_pydaq.mat ...\n",
      "No MSW events...\n",
      "No MST events...\n",
      "Preparing file 40: circuit_even_121917_data_060_pydaq.mat ...\n",
      "No SHC events...\n",
      "No MST events...\n",
      "Preparing file 41: circuit_even_121917_data_062_pydaq.mat ...\n",
      "No MSW events...\n",
      "No MST events...\n",
      "Preparing file 42: circuit_odd_121917_data_001_pydaq.mat ...\n",
      "No MST events...\n",
      "Preparing file 43: circuit_odd_121917_data_002_pydaq.mat ...\n",
      "No MSW events...\n",
      "No MST events...\n",
      "Preparing file 44: circuit_odd_121917_data_003_pydaq.mat ...\n",
      "No MSW events...\n",
      "No MST events...\n",
      "Preparing file 45: circuit_odd_121917_data_004_pydaq.mat ...\n",
      "No MSW events...\n",
      "Preparing file 46: circuit_odd_121917_data_005_pydaq.mat ...\n",
      "No MST events...\n",
      "Preparing file 47: circuit_odd_121917_data_006_pydaq.mat ...\n",
      "No MSW events...\n",
      "No MST events...\n",
      "Preparing file 48: circuit_odd_121917_data_007_pydaq.mat ...\n",
      "No MSW events...\n",
      "No MST events...\n",
      "No STO events...\n",
      "Preparing file 49: circuit_odd_121917_data_008_pydaq.mat ...\n",
      "No MSW events...\n",
      "Preparing file 50: circuit_odd_121917_data_009_pydaq.mat ...\n",
      "No HC events...\n",
      "No MST events...\n",
      "Preparing file 51: circuit_odd_121917_data_010_pydaq.mat ...\n",
      "No MSW events...\n",
      "No MST events...\n",
      "Preparing file 52: circuit_odd_121917_data_011_pydaq.mat ...\n",
      "No MSW events...\n",
      "No MST events...\n",
      "Preparing file 53: circuit_odd_121917_data_012_pydaq.mat ...\n",
      "No MSW events...\n",
      "No STO events...\n",
      "Preparing file 54: circuit_odd_121917_data_014_pydaq.mat ...\n",
      "No MSW events...\n",
      "No MST events...\n",
      "Preparing file 55: circuit_odd_121917_data_015_pydaq.mat ...\n",
      "No MSW events...\n",
      "Preparing file 56: circuit_odd_121917_data_016_pydaq.mat ...\n",
      "No MST events...\n",
      "Preparing file 57: circuit_odd_121917_data_017_pydaq.mat ...\n",
      "No MSW events...\n",
      "No MST events...\n",
      "Preparing file 58: circuit_odd_121917_data_019_pydaq.mat ...\n",
      "No MSW events...\n",
      "Preparing file 59: circuit_odd_121917_data_020_pydaq.mat ...\n",
      "No MST events...\n",
      "Preparing file 60: circuit_odd_121917_data_021_pydaq.mat ...\n",
      "No MSW events...\n",
      "No MST events...\n",
      "Preparing file 61: circuit_odd_121917_data_023_pydaq.mat ...\n",
      "No MSW events...\n",
      "Preparing file 62: circuit_odd_121917_data_024_pydaq.mat ...\n",
      "No MST events...\n",
      "Preparing file 63: circuit_odd_121917_data_025_pydaq.mat ...\n",
      "No MSW events...\n",
      "No MST events...\n",
      "Preparing file 64: circuit_odd_121917_data_027_pydaq.mat ...\n",
      "No MSW events...\n",
      "Preparing file 65: circuit_odd_121917_data_028_pydaq.mat ...\n",
      "No MST events...\n",
      "Preparing file 66: circuit_odd_121917_data_029_pydaq.mat ...\n",
      "No MSW events...\n",
      "No MST events...\n",
      "Preparing file 67: circuit_odd_121917_data_030_pydaq.mat ...\n",
      "No MSW events...\n",
      "No MST events...\n",
      "Preparing file 68: circuit_odd_121917_data_031_pydaq.mat ...\n",
      "No MSW events...\n",
      "Preparing file 69: circuit_odd_RIC_121917_data_001_pydaq.mat ...\n",
      "No HC events...\n",
      "No MST events...\n",
      "Preparing file 70: circuit_odd_RIC_121917_data_002_pydaq.mat ...\n",
      "No MSW events...\n",
      "Preparing file 71: circuit_odd_RIC_121917_data_003_pydaq.mat ...\n",
      "No MST events...\n",
      "Preparing file 72: circuit_odd_RIC_121917_data_004_pydaq.mat ...\n",
      "No MSW events...\n",
      "Preparing file 73: circuit_odd_RIC_121917_data_005_pydaq.mat ...\n",
      "No MST events...\n",
      "Preparing file 74: circuit_odd_RIC_121917_data_006_pydaq.mat ...\n",
      "No MSW events...\n",
      "Preparing file 75: circuit_odd_RIC_121917_data_007_pydaq.mat ...\n",
      "No MST events...\n",
      "Preparing file 76: circuit_odd_RIC_121917_data_009_pydaq.mat ...\n",
      "No SHC events...\n",
      "No MST events...\n",
      "Preparing file 77: circuit_odd_RIC_121917_data_010_pydaq.mat ...\n",
      "No MSW events...\n",
      "Preparing file 78: circuit_odd_RIC_121917_data_011_pydaq.mat ...\n",
      "No MST events...\n",
      "Preparing file 79: circuit_odd_RIC_121917_data_012_pydaq.mat ...\n",
      "No MSW events...\n",
      "No STO events...\n",
      "Preparing file 80: circuit_odd_RIC_121917_data_013_pydaq.mat ...\n",
      "No MST events...\n",
      "Preparing file 81: circuit_odd_RIC_121917_data_014_pydaq.mat ...\n",
      "No MSW events...\n",
      "Preparing file 82: circuit_odd_RIC_121917_data_015_pydaq.mat ...\n",
      "Preparing file 83: circuit_odd_RIC_121917_data_016_pydaq.mat ...\n",
      "No MST events...\n",
      "Preparing file 84: circuit_odd_RIC_121917_data_017_pydaq.mat ...\n",
      "No MSW events...\n",
      "No SHC events...\n",
      "No STO events...\n",
      "Preparing file 85: circuit_odd_RIC_121917_data_019_pydaq.mat ...\n",
      "No MSW events...\n",
      "Preparing file 86: ramp_121917_data_002_pydaq.mat ...\n",
      "No MSW events...\n",
      "No MST events...\n",
      "Preparing file 87: ramp_121917_data_003_pydaq.mat ...\n",
      "No MSW events...\n",
      "No MST events...\n",
      "Preparing file 88: ramp_121917_data_004_pydaq.mat ...\n",
      "No MSW events...\n",
      "No MST events...\n",
      "Preparing file 89: ramp_121917_data_005_pydaq.mat ...\n",
      "No MSW events...\n",
      "No MST events...\n",
      "Preparing file 90: ramp_121917_data_006_pydaq.mat ...\n",
      "No MSW events...\n",
      "No MST events...\n",
      "Preparing file 91: ramp_121917_data_008_pydaq.mat ...\n",
      "No MSW events...\n",
      "No MST events...\n",
      "Preparing file 92: ramp_121917_data_009_pydaq.mat ...\n",
      "No MSW events...\n",
      "No MST events...\n",
      "Preparing file 93: ramp_121917_data_010_pydaq.mat ...\n",
      "No MSW events...\n",
      "No MST events...\n",
      "Preparing file 94: stair_stand_121917_data_001_pydaq.mat ...\n",
      "No MST events...\n",
      "Preparing file 95: stair_stand_121917_data_002_pydaq.mat ...\n",
      "No MSW events...\n",
      "Preparing file 96: stair_stand_121917_data_004_pydaq.mat ...\n",
      "No MSW events...\n",
      "Preparing file 97: stair_stand_121917_data_005_pydaq.mat ...\n",
      "No TO events...\n",
      "No HC events...\n",
      "No MST events...\n",
      "Preparing file 98: stair_stand_121917_data_006_pydaq.mat ...\n",
      "No MSW events...\n",
      "Preparing file 99: stair_stand_121917_data_007_pydaq.mat ...\n",
      "No TO events...\n",
      "No HC events...\n",
      "No MST events...\n",
      "Preparing file 100: stair_stand_121917_data_008_pydaq.mat ...\n",
      "No MSW events...\n",
      "Preparing file 101: stair_stand_121917_data_009_pydaq.mat ...\n",
      "No TO events...\n",
      "No HC events...\n",
      "No MST events...\n",
      "Preparing file 102: stand_121917_data_001_pydaq.mat ...\n",
      "No MSW events...\n",
      "No MST events...\n",
      "Preparing file 103: stand_121917_data_002_pydaq.mat ...\n",
      "No MSW events...\n",
      "No MST events...\n",
      "Preparing file 104: stand_121917_data_003_pydaq.mat ...\n",
      "No MSW events...\n",
      "No MST events...\n",
      "Preparing file 105: stand_121917_data_004_pydaq.mat ...\n",
      "No TO events...\n",
      "No HC events...\n",
      "No MSW events...\n",
      "No MST events...\n",
      "Preparing file 106: stand_121917_data_005_pydaq.mat ...\n",
      "No MSW events...\n",
      "No MST events...\n",
      "Preparing file 107: stand_121917_data_006_pydaq.mat ...\n",
      "No MSW events...\n",
      "No MST events...\n",
      "Preparing file 108: stand_121917_data_007_pydaq.mat ...\n",
      "No TO events...\n",
      "No HC events...\n",
      "No MSW events...\n",
      "No MST events...\n",
      "Preparing file 109: stand_121917_data_008_pydaq.mat ...\n",
      "No TO events...\n",
      "No HC events...\n",
      "No MSW events...\n",
      "No MST events...\n",
      "Preparing file 110: stand_121917_data_009_pydaq.mat ...\n",
      "No TO events...\n",
      "No HC events...\n",
      "No MSW events...\n",
      "No MST events...\n",
      "Preparing file 111: stand_121917_data_010_pydaq.mat ...\n",
      "No TO events...\n",
      "No HC events...\n",
      "No MSW events...\n",
      "No MST events...\n",
      "Preparing file 112: walk_121917_data_001_pydaq.mat ...\n",
      "No MSW events...\n",
      "No MST events...\n",
      "Preparing file 113: walk_121917_data_002_pydaq.mat ...\n",
      "No MSW events...\n",
      "No MST events...\n",
      "Preparing file 114: walk_121917_data_003_pydaq.mat ...\n",
      "No MSW events...\n",
      "No MST events...\n",
      "Preparing file 115: walk_121917_data_004_pydaq.mat ...\n",
      "No MSW events...\n",
      "No MST events...\n",
      "Preparing file 116: walk_121917_data_005_pydaq.mat ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No MSW events...\n",
      "No MST events...\n",
      "No STO events...\n",
      "Preparing file 117: walk_121917_data_006_pydaq.mat ...\n",
      "No MSW events...\n",
      "No MST events...\n",
      "Preparing file 118: walk_121917_data_007_pydaq.mat ...\n",
      "No MSW events...\n",
      "No MST events...\n",
      "Preparing file 119: walk_121917_data_008_pydaq.mat ...\n",
      "No MSW events...\n",
      "No MST events...\n",
      "Preparing file 120: walk_121917_data_009_pydaq.mat ...\n",
      "No TO events...\n",
      "No HC events...\n",
      "No MSW events...\n",
      "No MST events...\n",
      "Preparing file 121: walk_121917_data_010_pydaq.mat ...\n",
      "No MSW events...\n",
      "No MST events...\n",
      "Preparing file 122: walk_121917_data_011_pydaq.mat ...\n",
      "No MSW events...\n",
      "No MST events...\n",
      "Preparing file 123: walk_121917_data_012_pydaq.mat ...\n",
      "No MSW events...\n",
      "No MST events...\n",
      "Preparing file 124: walk_121917_data_013_pydaq.mat ...\n",
      "No MSW events...\n",
      "No MST events...\n",
      "Preparing file 125: walk_121917_data_014_pydaq.mat ...\n",
      "No MSW events...\n",
      "No MST events...\n",
      "Preparing file 126: walk_121917_data_015_pydaq.mat ...\n",
      "No MSW events...\n",
      "No MST events...\n",
      "\n",
      "Aggregated HC feature dimensions: (801, 186)\n",
      "Aggregated MST feature dimensions: (71, 186)\n",
      "Aggregated TO feature dimensions: (957, 186)\n",
      "Aggregated MSW feature dimensions: (89, 186)\n",
      "Aggregated SHC feature dimensions: (787, 186)\n",
      "Aggregated STO feature dimensions: (721, 186)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "TRAIN_SIZE = 300 # Training windows are 300 time-steps long for sliding windows\n",
    "# FOR_TEST = ['circuit_even_data_020.mat','circuit_odd_data_019.mat'] # Filenames to use for testing\n",
    "FOR_TEST = [0.05]\n",
    "CHAN_MECH = np.arange(31)\n",
    "PRINT_SUMMARY = True # Print dimensions of aggregated data from all files\n",
    "\n",
    "arginput = [TRAIN_SIZE,FOR_TEST,CHAN_MECH,PRINT_SUMMARY]\n",
    "\n",
    "alldict, allfeatnames, _, _ = unpack_files(filedict,allindtrigsdict,filekeys,arginput,allcolheaders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====Using the following 31 channels:=====\n",
      "Knee Angle\n",
      "Knee Vel\n",
      "Knee Current\n",
      "Ankle Angle\n",
      "Ankle Vel\n",
      "Ankle Current\n",
      "VU Ax\n",
      "VU Ay\n",
      "VU Az\n",
      "VU Gx\n",
      "VU Gz\n",
      "VU Gy\n",
      "Shank Angle\n",
      "Thigh Angle\n",
      "Knee Ref\n",
      "Ankle Ref\n",
      "Load\n",
      "Shank Ax\n",
      "Shank Ay\n",
      "Shank Az\n",
      "Shank Gy\n",
      "Shank Gz\n",
      "Shank Gx\n",
      "Thigh Ax\n",
      "Thigh Ay\n",
      "Thigh Az\n",
      "Thigh Gy\n",
      "Thigh Gz\n",
      "Thigh Gx\n",
      "Contra Shank\n",
      "Contra Thigh\n",
      "\n",
      "=====Using the following 186 features:=====\n",
      "Knee Angle Min\n",
      "Knee Vel Min\n",
      "Knee Current Min\n",
      "Ankle Angle Min\n",
      "Ankle Vel Min\n",
      "Ankle Current Min\n",
      "VU Ax Min\n",
      "VU Ay Min\n",
      "VU Az Min\n",
      "VU Gx Min\n",
      "VU Gz Min\n",
      "VU Gy Min\n",
      "Shank Angle Min\n",
      "Thigh Angle Min\n",
      "Knee Ref Min\n",
      "Ankle Ref Min\n",
      "Load Min\n",
      "Shank Ax Min\n",
      "Shank Ay Min\n",
      "Shank Az Min\n",
      "Shank Gy Min\n",
      "Shank Gz Min\n",
      "Shank Gx Min\n",
      "Thigh Ax Min\n",
      "Thigh Ay Min\n",
      "Thigh Az Min\n",
      "Thigh Gy Min\n",
      "Thigh Gz Min\n",
      "Thigh Gx Min\n",
      "Contra Shank Min\n",
      "Contra Thigh Min\n",
      "Knee Angle Max\n",
      "Knee Vel Max\n",
      "Knee Current Max\n",
      "Ankle Angle Max\n",
      "Ankle Vel Max\n",
      "Ankle Current Max\n",
      "VU Ax Max\n",
      "VU Ay Max\n",
      "VU Az Max\n",
      "VU Gx Max\n",
      "VU Gz Max\n",
      "VU Gy Max\n",
      "Shank Angle Max\n",
      "Thigh Angle Max\n",
      "Knee Ref Max\n",
      "Ankle Ref Max\n",
      "Load Max\n",
      "Shank Ax Max\n",
      "Shank Ay Max\n",
      "Shank Az Max\n",
      "Shank Gy Max\n",
      "Shank Gz Max\n",
      "Shank Gx Max\n",
      "Thigh Ax Max\n",
      "Thigh Ay Max\n",
      "Thigh Az Max\n",
      "Thigh Gy Max\n",
      "Thigh Gz Max\n",
      "Thigh Gx Max\n",
      "Contra Shank Max\n",
      "Contra Thigh Max\n",
      "Knee Angle Initial\n",
      "Knee Vel Initial\n",
      "Knee Current Initial\n",
      "Ankle Angle Initial\n",
      "Ankle Vel Initial\n",
      "Ankle Current Initial\n",
      "VU Ax Initial\n",
      "VU Ay Initial\n",
      "VU Az Initial\n",
      "VU Gx Initial\n",
      "VU Gz Initial\n",
      "VU Gy Initial\n",
      "Shank Angle Initial\n",
      "Thigh Angle Initial\n",
      "Knee Ref Initial\n",
      "Ankle Ref Initial\n",
      "Load Initial\n",
      "Shank Ax Initial\n",
      "Shank Ay Initial\n",
      "Shank Az Initial\n",
      "Shank Gy Initial\n",
      "Shank Gz Initial\n",
      "Shank Gx Initial\n",
      "Thigh Ax Initial\n",
      "Thigh Ay Initial\n",
      "Thigh Az Initial\n",
      "Thigh Gy Initial\n",
      "Thigh Gz Initial\n",
      "Thigh Gx Initial\n",
      "Contra Shank Initial\n",
      "Contra Thigh Initial\n",
      "Knee Angle Final\n",
      "Knee Vel Final\n",
      "Knee Current Final\n",
      "Ankle Angle Final\n",
      "Ankle Vel Final\n",
      "Ankle Current Final\n",
      "VU Ax Final\n",
      "VU Ay Final\n",
      "VU Az Final\n",
      "VU Gx Final\n",
      "VU Gz Final\n",
      "VU Gy Final\n",
      "Shank Angle Final\n",
      "Thigh Angle Final\n",
      "Knee Ref Final\n",
      "Ankle Ref Final\n",
      "Load Final\n",
      "Shank Ax Final\n",
      "Shank Ay Final\n",
      "Shank Az Final\n",
      "Shank Gy Final\n",
      "Shank Gz Final\n",
      "Shank Gx Final\n",
      "Thigh Ax Final\n",
      "Thigh Ay Final\n",
      "Thigh Az Final\n",
      "Thigh Gy Final\n",
      "Thigh Gz Final\n",
      "Thigh Gx Final\n",
      "Contra Shank Final\n",
      "Contra Thigh Final\n",
      "Knee Angle Mean\n",
      "Knee Vel Mean\n",
      "Knee Current Mean\n",
      "Ankle Angle Mean\n",
      "Ankle Vel Mean\n",
      "Ankle Current Mean\n",
      "VU Ax Mean\n",
      "VU Ay Mean\n",
      "VU Az Mean\n",
      "VU Gx Mean\n",
      "VU Gz Mean\n",
      "VU Gy Mean\n",
      "Shank Angle Mean\n",
      "Thigh Angle Mean\n",
      "Knee Ref Mean\n",
      "Ankle Ref Mean\n",
      "Load Mean\n",
      "Shank Ax Mean\n",
      "Shank Ay Mean\n",
      "Shank Az Mean\n",
      "Shank Gy Mean\n",
      "Shank Gz Mean\n",
      "Shank Gx Mean\n",
      "Thigh Ax Mean\n",
      "Thigh Ay Mean\n",
      "Thigh Az Mean\n",
      "Thigh Gy Mean\n",
      "Thigh Gz Mean\n",
      "Thigh Gx Mean\n",
      "Contra Shank Mean\n",
      "Contra Thigh Mean\n",
      "Knee Angle SD\n",
      "Knee Vel SD\n",
      "Knee Current SD\n",
      "Ankle Angle SD\n",
      "Ankle Vel SD\n",
      "Ankle Current SD\n",
      "VU Ax SD\n",
      "VU Ay SD\n",
      "VU Az SD\n",
      "VU Gx SD\n",
      "VU Gz SD\n",
      "VU Gy SD\n",
      "Shank Angle SD\n",
      "Thigh Angle SD\n",
      "Knee Ref SD\n",
      "Ankle Ref SD\n",
      "Load SD\n",
      "Shank Ax SD\n",
      "Shank Ay SD\n",
      "Shank Az SD\n",
      "Shank Gy SD\n",
      "Shank Gz SD\n",
      "Shank Gx SD\n",
      "Thigh Ax SD\n",
      "Thigh Ay SD\n",
      "Thigh Az SD\n",
      "Thigh Gy SD\n",
      "Thigh Gz SD\n",
      "Thigh Gx SD\n",
      "Contra Shank SD\n",
      "Contra Thigh SD\n"
     ]
    }
   ],
   "source": [
    "vu_only = list(range(17))\n",
    "contra_thigh = list(range(17)) + list(range(23,29)) + [30]\n",
    "contra_shank = list(range(23)) + [29]\n",
    "contra_both = list(range(31))\n",
    "chanuse = contra_both\n",
    "\n",
    "USE_MECH_CHAN_NAME = [allcolheaders[chanuse[i]] for i in range(len(chanuse))]\n",
    "\n",
    "USE_CHAN, USE_FEAT = select_chan(USE_MECH_CHAN_NAME,allcolheaders,allfeatnames)\n",
    "print('=====Using the following {} channels:====='.format(len(USE_CHAN)))\n",
    "for chan in USE_CHAN:\n",
    "    print(allcolheaders[chan])\n",
    "print()\n",
    "print('=====Using the following {} features:====='.format(len(USE_FEAT)))\n",
    "for feat in USE_FEAT:\n",
    "    print(allfeatnames[feat])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use PCA to visualize event-specific feature space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "event = 'MSW'\n",
    "\n",
    "trainfiles = []\n",
    "testfiles = []\n",
    "traineventinds = [fileind for fileind in np.arange(len(alldict['Combined ' + event + ' File Index'])) if alldict['Combined ' + event + ' File Index'][fileind] in train_files]\n",
    "testeventinds = [fileind for fileind in np.arange(len(alldict['Combined ' + event + ' File Index'])) if alldict['Combined ' + event + ' File Index'][fileind] in test_files]\n",
    "\n",
    "leave, enter, steptype = unpack_trig(alldict['Combined ' + event + ' Triggers']) # Get the leaving and entering modes from the triggers\n",
    "\n",
    "# Fit scaler with training files\n",
    "trainevent_scaler = preprocessing.StandardScaler().fit(alldict['Combined ' + event + ' Features'][traineventinds][:,USE_FEAT])\n",
    "trainevent_norm = trainevent_scaler.transform(alldict['Combined ' + event + ' Features'][traineventinds][:,USE_FEAT])\n",
    "testevent_norm = trainevent_scaler.transform(alldict['Combined ' + event + ' Features'][testeventinds][:,USE_FEAT])\n",
    "\n",
    "# Fit PCA with training files\n",
    "trainevent_pca = PCA().fit(trainevent_norm)\n",
    "trainevent_dimred = trainevent_pca.transform(trainevent_norm)\n",
    "testevent_dimred = trainevent_pca.transform(testevent_norm)   \n",
    "\n",
    "pcolor = {1: 'k', 2: 'b', 3:'c', 4:'m', 5:'g', 6:'r'}\n",
    "    \n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "for wind in np.arange(0,np.shape(trainevent_dimred)[0]):\n",
    "    ax.scatter(trainevent_dimred[wind,0],trainevent_dimred[wind,1],trainevent_dimred[wind,2],color=pcolor[enter[traineventinds][wind]],alpha=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Mode Specific Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-organize data for mode-specific classifiers for each gait event (downsampling optional)\n",
    "allevents_ms = make_modespec(alldict, USE_FEAT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Mode Specific Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MS classification:\n",
      "Classifier: HC_LW\n",
      "Unique: [2 4 6]\n",
      "['St', 'LW', 'SA', 'SD', 'RA', 'RD']\n",
      "[  0 599   0  10   0  23]\n",
      "LOO Accuracy: 0.9936708860759493\n",
      "\n",
      "Classifier: MST_SD\n",
      "Unique: [2 4]\n",
      "['St', 'LW', 'SA', 'SD', 'RA', 'RD']\n",
      "[ 0 11  0 60  0  0]\n",
      "LOO Accuracy: 1.0\n",
      "\n",
      "Classifier: STO\n",
      "Unique: [1 3]\n",
      "['St', 'LW', 'SA', 'SD', 'RA', 'RD']\n",
      "[639   0  82   0   0   0]\n",
      "LOO Accuracy: 1.0\n",
      "\n",
      "Classifier: MSW_SA\n",
      "Unique: [2 3]\n",
      "['St', 'LW', 'SA', 'SD', 'RA', 'RD']\n",
      "[ 0 13 76  0  0  0]\n",
      "LOO Accuracy: 0.9775280898876404\n",
      "\n",
      "Classifier: HC_RD\n",
      "Unique: [2 6]\n",
      "['St', 'LW', 'SA', 'SD', 'RA', 'RD']\n",
      "[ 0 24  0  0  0 85]\n",
      "LOO Accuracy: 0.981651376146789\n",
      "\n",
      "Classifier: SHC\n",
      "Unique: [1 4]\n",
      "['St', 'LW', 'SA', 'SD', 'RA', 'RD']\n",
      "[766   0   0  21   0   0]\n",
      "LOO Accuracy: 1.0\n",
      "\n",
      "Classifier: HC_SD\n",
      "Unique: [2 4]\n",
      "['St', 'LW', 'SA', 'SD', 'RA', 'RD']\n",
      "[ 0 20  0 40  0  0]\n",
      "LOO Accuracy: 0.9833333333333333\n",
      "\n",
      "Classifier: TO\n",
      "Unique: [1 2 3 4 6]\n",
      "['St', 'LW', 'SA', 'SD', 'RA', 'RD']\n",
      "[160 621   7  60   0 109]\n",
      "LOO Accuracy: 0.9738766980146291\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pcaprop = 50\n",
    "print('MS classification:')\n",
    "all_pred, all_gtruth, all_type, all_files = ms_classify_results(allevents_ms,pcaprop,[],[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           0          1          2          3          4\n",
      "0  98.551637   1.385390   0.000000   0.000000   0.062972\n",
      "1   0.000000  99.683544   0.079114   0.000000   0.237342\n",
      "2   0.000000   1.204819  98.795181   0.000000   0.000000\n",
      "3   0.000000   1.030928   0.000000  98.453608   0.515464\n",
      "4   0.000000   0.934579   0.000000   0.000000  99.065421\n",
      "\n",
      "SS Accuracy: 0.9910743801652893\n",
      "Trans Accuracy: 0.9825436408977556\n",
      "Overall Accuracy: 0.9900758902510216\n",
      "\n",
      "SS steps: 3025\n",
      "T steps: 401\n",
      "\n",
      "Files with errors:\n",
      "circuit_even_121917_data_001_pydaq.mat\n",
      "circuit_even_121917_data_002_pydaq.mat\n",
      "circuit_even_121917_data_009_pydaq.mat\n",
      "circuit_even_121917_data_013_pydaq.mat\n",
      "circuit_even_121917_data_015_pydaq.mat\n",
      "circuit_even_121917_data_018_pydaq.mat\n",
      "circuit_even_121917_data_022_pydaq.mat\n",
      "circuit_even_121917_data_026_pydaq.mat\n",
      "circuit_even_121917_data_035_pydaq.mat\n",
      "circuit_even_121917_data_048_pydaq.mat\n",
      "circuit_even_121917_data_050_pydaq.mat\n",
      "circuit_even_121917_data_051_pydaq.mat\n",
      "circuit_odd_121917_data_003_pydaq.mat\n",
      "circuit_odd_121917_data_005_pydaq.mat\n",
      "circuit_odd_121917_data_010_pydaq.mat\n",
      "circuit_odd_121917_data_017_pydaq.mat\n",
      "circuit_odd_121917_data_027_pydaq.mat\n",
      "circuit_odd_121917_data_030_pydaq.mat\n",
      "circuit_odd_RIC_121917_data_003_pydaq.mat\n",
      "circuit_odd_RIC_121917_data_004_pydaq.mat\n",
      "circuit_odd_RIC_121917_data_005_pydaq.mat\n",
      "circuit_odd_RIC_121917_data_015_pydaq.mat\n",
      "ramp_121917_data_006_pydaq.mat\n",
      "ramp_121917_data_010_pydaq.mat\n",
      "stand_121917_data_002_pydaq.mat\n",
      "walk_121917_data_007_pydaq.mat\n",
      "walk_121917_data_012_pydaq.mat\n",
      "walk_121917_data_013_pydaq.mat\n",
      "walk_121917_data_014_pydaq.mat\n",
      "walk_121917_data_015_pydaq.mat\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(all_pred,all_gtruth)\n",
    "class_totals = cm.sum(axis=1)\n",
    "norm_cm = np.zeros((5,5))\n",
    "for i in range(5):\n",
    "    norm_cm[i,:] = cm[i,:]/class_totals[i]\n",
    "\n",
    "print(pd.DataFrame(100*norm_cm))\n",
    "\n",
    "t_step = np.where(all_type == 0)[0]\n",
    "ss_step = np.where(all_type == 1)[0]\n",
    "\n",
    "print('\\nSS Accuracy: {}'.format(accuracy_score(all_pred[ss_step],all_gtruth[ss_step])))\n",
    "print('Trans Accuracy: {}'.format(accuracy_score(all_pred[t_step],all_gtruth[t_step])))\n",
    "print('Overall Accuracy: {}'.format(accuracy_score(all_pred,all_gtruth)))\n",
    "\n",
    "print('\\nSS steps: {}'.format(len(ss_step)))\n",
    "print('T steps: {}'.format(len(t_step)))\n",
    "\n",
    "print('\\nFiles with errors:')\n",
    "for fnum in np.unique([all_files[i] for i in range(len(all_files)) if all_pred[i] != all_gtruth[i]]):\n",
    "    print(filekeys[fnum])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-gpu",
   "language": "python",
   "name": "tensorflow-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
